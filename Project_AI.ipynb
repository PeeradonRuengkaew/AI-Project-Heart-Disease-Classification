{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCFH0I2L4Z3S"
   },
   "source": [
    "#Import Libary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "XIOAnETVtOIn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,roc_auc_score,classification_report,f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "x6tz7O22tkEP"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/peeradonruengkaew/Downloads/AI-Project-Heart-Disease-Classification-main/heart.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2mXugJE4hs6"
   },
   "source": [
    "#data mange for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_DN79Q8RtkmS"
   },
   "outputs": [],
   "source": [
    "binary_vars_list=['Sex','ExerciseAngina']\n",
    "def binary_map(x):\n",
    "    return x.map({'Y':1,'N':0,'M':1,'F':0})\n",
    "    \n",
    "\n",
    "#applying the function to the housing variables list\n",
    "data[binary_vars_list] = data[binary_vars_list].apply(binary_map)\n",
    "\n",
    "def dummies(x,df):\n",
    "    temp = pd.get_dummies(df[x], drop_first = False)\n",
    "    df = pd.concat([df, temp], axis = 1)\n",
    "    df.drop([x], axis = 1, inplace = True)\n",
    "    return df\n",
    "# Applying the function to the data\n",
    "\n",
    "data = dummies('ChestPainType',data)\n",
    "data = dummies('RestingECG',data)\n",
    "data = dummies('ST_Slope',data)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "numeric_vars = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']\n",
    "data[numeric_vars] = scaler.fit_transform(data[numeric_vars])\n",
    "\n",
    "data.drop(['TA'], axis = 1, inplace = True)\n",
    "data.drop(['LVH'], axis = 1, inplace = True)\n",
    "data.drop(['Normal'], axis = 1, inplace = True)\n",
    "data.drop(['ST'], axis = 1, inplace = True)\n",
    "data.drop(['RestingBP'], axis = 1, inplace = True)\n",
    "data.drop(['Down'], axis = 1, inplace = True)\n",
    "\n",
    "x = np.array(data.drop(['HeartDisease'],axis=1))\n",
    "#x\n",
    "y = np.array(data['HeartDisease'])\n",
    "#y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkaMSiz-4m5n"
   },
   "source": [
    "#MLP with l2 & drop out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v78RUYhLshGy",
    "outputId": "0a210e1d-6b0e-4040-cfc5-e7ecfd2a8751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dropout_15 (Dropout)        (None, 12)                0         \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 6)                 78        \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 6)                 0         \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 10)                70        \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 8)                 88        \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 2)                 18        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 254\n",
      "Trainable params: 254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "re=regularizers.L2(0.1)\n",
    "def create_model_1():\n",
    "    model_1 = Sequential()\n",
    "    model_1.add(Dropout(0.1, input_dim = 12))\n",
    "    model_1.add(Dense(6, activation = 'relu',kernel_regularizer=re))\n",
    "    model_1.add(Dropout(0.1))\n",
    "    model_1.add(Dense(10, activation = 'relu'))\n",
    "    model_1.add(Dropout(0.1))\n",
    "    model_1.add(Dense(8, activation = 'relu'))\n",
    "    model_1.add(Dense(2, activation = 'sigmoid'))\n",
    "    model_1.compile(optimizer = Adam(learning_rate=0.00036), loss = 'binary_crossentropy', metrics = ['accuracy']) \n",
    "    return model_1\n",
    "\n",
    "model_1 = create_model_1()\n",
    "\n",
    "print(model_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mr3eXCRducv4",
    "outputId": "1661d4b9-c6bf-458c-9fa4-8dd4447fd1dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 1.3335 - accuracy: 0.5732 - val_loss: 1.2322 - val_accuracy: 0.6159\n",
      "Epoch 2/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.1505 - accuracy: 0.6604 - val_loss: 1.0718 - val_accuracy: 0.7681\n",
      "Epoch 3/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.0008 - accuracy: 0.7586 - val_loss: 0.9319 - val_accuracy: 0.8080\n",
      "Epoch 4/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.8699 - accuracy: 0.7773 - val_loss: 0.8103 - val_accuracy: 0.7971\n",
      "Epoch 5/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7587 - accuracy: 0.8069 - val_loss: 0.7136 - val_accuracy: 0.8225\n",
      "Epoch 6/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6709 - accuracy: 0.8209 - val_loss: 0.6316 - val_accuracy: 0.8333\n",
      "Epoch 7/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6001 - accuracy: 0.8318 - val_loss: 0.5666 - val_accuracy: 0.8514\n",
      "Epoch 8/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5402 - accuracy: 0.8458 - val_loss: 0.5173 - val_accuracy: 0.8623\n",
      "Epoch 9/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4990 - accuracy: 0.8411 - val_loss: 0.4836 - val_accuracy: 0.8551\n",
      "Epoch 10/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4910 - accuracy: 0.8193 - val_loss: 0.4636 - val_accuracy: 0.8514\n",
      "Epoch 11/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4822 - accuracy: 0.8302 - val_loss: 0.4525 - val_accuracy: 0.8514\n",
      "Epoch 12/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4532 - accuracy: 0.8364 - val_loss: 0.4438 - val_accuracy: 0.8551\n",
      "Epoch 13/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4295 - accuracy: 0.8396 - val_loss: 0.4368 - val_accuracy: 0.8551\n",
      "Epoch 14/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4259 - accuracy: 0.8396 - val_loss: 0.4346 - val_accuracy: 0.8514\n",
      "Epoch 15/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4332 - accuracy: 0.8287 - val_loss: 0.4295 - val_accuracy: 0.8514\n",
      "Epoch 16/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4261 - accuracy: 0.8458 - val_loss: 0.4292 - val_accuracy: 0.8478\n",
      "Epoch 17/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4136 - accuracy: 0.8489 - val_loss: 0.4278 - val_accuracy: 0.8551\n",
      "Epoch 18/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4147 - accuracy: 0.8458 - val_loss: 0.4322 - val_accuracy: 0.8406\n",
      "Epoch 19/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4018 - accuracy: 0.8505 - val_loss: 0.4276 - val_accuracy: 0.8406\n",
      "Epoch 20/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4177 - accuracy: 0.8411 - val_loss: 0.4252 - val_accuracy: 0.8478\n",
      "Epoch 21/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4026 - accuracy: 0.8505 - val_loss: 0.4251 - val_accuracy: 0.8478\n",
      "Epoch 22/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4216 - accuracy: 0.8364 - val_loss: 0.4255 - val_accuracy: 0.8370\n",
      "Epoch 23/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3912 - accuracy: 0.8707 - val_loss: 0.4225 - val_accuracy: 0.8514\n",
      "Epoch 24/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4158 - accuracy: 0.8427 - val_loss: 0.4209 - val_accuracy: 0.8514\n",
      "Epoch 25/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4110 - accuracy: 0.8411 - val_loss: 0.4253 - val_accuracy: 0.8333\n",
      "Epoch 26/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3975 - accuracy: 0.8474 - val_loss: 0.4230 - val_accuracy: 0.8478\n",
      "Epoch 27/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4130 - accuracy: 0.8396 - val_loss: 0.4217 - val_accuracy: 0.8551\n",
      "Epoch 28/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3988 - accuracy: 0.8583 - val_loss: 0.4183 - val_accuracy: 0.8551\n",
      "Epoch 29/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3956 - accuracy: 0.8442 - val_loss: 0.4191 - val_accuracy: 0.8551\n",
      "Epoch 30/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3932 - accuracy: 0.8489 - val_loss: 0.4175 - val_accuracy: 0.8514\n",
      "Epoch 31/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3845 - accuracy: 0.8583 - val_loss: 0.4176 - val_accuracy: 0.8514\n",
      "Epoch 32/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3993 - accuracy: 0.8536 - val_loss: 0.4200 - val_accuracy: 0.8478\n",
      "Epoch 33/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3811 - accuracy: 0.8489 - val_loss: 0.4160 - val_accuracy: 0.8551\n",
      "Epoch 34/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4091 - accuracy: 0.8520 - val_loss: 0.4170 - val_accuracy: 0.8514\n",
      "Epoch 35/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3977 - accuracy: 0.8364 - val_loss: 0.4165 - val_accuracy: 0.8551\n",
      "Epoch 36/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3847 - accuracy: 0.8489 - val_loss: 0.4174 - val_accuracy: 0.8514\n",
      "Epoch 37/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3755 - accuracy: 0.8489 - val_loss: 0.4165 - val_accuracy: 0.8551\n",
      "Epoch 38/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4038 - accuracy: 0.8427 - val_loss: 0.4160 - val_accuracy: 0.8514\n",
      "Epoch 39/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4096 - accuracy: 0.8489 - val_loss: 0.4170 - val_accuracy: 0.8333\n",
      "Epoch 40/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3879 - accuracy: 0.8474 - val_loss: 0.4142 - val_accuracy: 0.8587\n",
      "Epoch 41/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3891 - accuracy: 0.8520 - val_loss: 0.4136 - val_accuracy: 0.8514\n",
      "Epoch 42/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3905 - accuracy: 0.8598 - val_loss: 0.4166 - val_accuracy: 0.8333\n",
      "Epoch 43/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3832 - accuracy: 0.8567 - val_loss: 0.4155 - val_accuracy: 0.8406\n",
      "Epoch 44/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3952 - accuracy: 0.8536 - val_loss: 0.4131 - val_accuracy: 0.8551\n",
      "Epoch 45/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4028 - accuracy: 0.8349 - val_loss: 0.4152 - val_accuracy: 0.8406\n",
      "Epoch 46/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4030 - accuracy: 0.8349 - val_loss: 0.4128 - val_accuracy: 0.8551\n",
      "Epoch 47/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3861 - accuracy: 0.8489 - val_loss: 0.4117 - val_accuracy: 0.8514\n",
      "Epoch 48/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3879 - accuracy: 0.8536 - val_loss: 0.4120 - val_accuracy: 0.8514\n",
      "Epoch 49/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3837 - accuracy: 0.8551 - val_loss: 0.4120 - val_accuracy: 0.8514\n",
      "Epoch 50/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3818 - accuracy: 0.8520 - val_loss: 0.4124 - val_accuracy: 0.8514\n",
      "Epoch 51/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3960 - accuracy: 0.8458 - val_loss: 0.4155 - val_accuracy: 0.8370\n",
      "Epoch 52/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3778 - accuracy: 0.8442 - val_loss: 0.4198 - val_accuracy: 0.8188\n",
      "Epoch 53/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3698 - accuracy: 0.8645 - val_loss: 0.4133 - val_accuracy: 0.8514\n",
      "Epoch 54/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3638 - accuracy: 0.8551 - val_loss: 0.4153 - val_accuracy: 0.8406\n",
      "Epoch 55/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3814 - accuracy: 0.8598 - val_loss: 0.4149 - val_accuracy: 0.8406\n",
      "Epoch 56/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3976 - accuracy: 0.8427 - val_loss: 0.4139 - val_accuracy: 0.8442\n",
      "Epoch 57/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3869 - accuracy: 0.8474 - val_loss: 0.4098 - val_accuracy: 0.8478\n",
      "Epoch 58/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3467 - accuracy: 0.8707 - val_loss: 0.4154 - val_accuracy: 0.8225\n",
      "Epoch 59/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4014 - accuracy: 0.8411 - val_loss: 0.4126 - val_accuracy: 0.8406\n",
      "Epoch 60/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3690 - accuracy: 0.8645 - val_loss: 0.4089 - val_accuracy: 0.8514\n",
      "Epoch 61/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3866 - accuracy: 0.8458 - val_loss: 0.4091 - val_accuracy: 0.8478\n",
      "Epoch 62/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3594 - accuracy: 0.8629 - val_loss: 0.4094 - val_accuracy: 0.8478\n",
      "Epoch 63/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3805 - accuracy: 0.8536 - val_loss: 0.4109 - val_accuracy: 0.8478\n",
      "Epoch 64/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4013 - accuracy: 0.8302 - val_loss: 0.4108 - val_accuracy: 0.8406\n",
      "Epoch 65/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3698 - accuracy: 0.8551 - val_loss: 0.4111 - val_accuracy: 0.8406\n",
      "Epoch 66/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3735 - accuracy: 0.8567 - val_loss: 0.4073 - val_accuracy: 0.8551\n",
      "Epoch 67/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3608 - accuracy: 0.8551 - val_loss: 0.4130 - val_accuracy: 0.8333\n",
      "Epoch 68/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4098 - accuracy: 0.8458 - val_loss: 0.4073 - val_accuracy: 0.8551\n",
      "Epoch 69/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3708 - accuracy: 0.8489 - val_loss: 0.4101 - val_accuracy: 0.8406\n",
      "Epoch 70/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3576 - accuracy: 0.8567 - val_loss: 0.4123 - val_accuracy: 0.8406\n",
      "Epoch 71/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3736 - accuracy: 0.8505 - val_loss: 0.4081 - val_accuracy: 0.8478\n",
      "Epoch 72/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3937 - accuracy: 0.8427 - val_loss: 0.4086 - val_accuracy: 0.8514\n",
      "Epoch 73/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3935 - accuracy: 0.8427 - val_loss: 0.4127 - val_accuracy: 0.8442\n",
      "Epoch 74/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3921 - accuracy: 0.8427 - val_loss: 0.4080 - val_accuracy: 0.8478\n",
      "Epoch 75/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3675 - accuracy: 0.8598 - val_loss: 0.4080 - val_accuracy: 0.8478\n",
      "Epoch 76/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3876 - accuracy: 0.8551 - val_loss: 0.4074 - val_accuracy: 0.8478\n",
      "Epoch 77/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3750 - accuracy: 0.8474 - val_loss: 0.4100 - val_accuracy: 0.8478\n",
      "Epoch 78/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3678 - accuracy: 0.8629 - val_loss: 0.4124 - val_accuracy: 0.8333\n",
      "Epoch 79/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3824 - accuracy: 0.8629 - val_loss: 0.4129 - val_accuracy: 0.8406\n",
      "Epoch 80/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3687 - accuracy: 0.8676 - val_loss: 0.4087 - val_accuracy: 0.8514\n",
      "Epoch 1/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3854 - accuracy: 0.8536 - val_loss: 0.3431 - val_accuracy: 0.8841\n",
      "Epoch 2/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3872 - accuracy: 0.8380 - val_loss: 0.3472 - val_accuracy: 0.8804\n",
      "Epoch 3/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3972 - accuracy: 0.8458 - val_loss: 0.3461 - val_accuracy: 0.8804\n",
      "Epoch 4/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4125 - accuracy: 0.8458 - val_loss: 0.3441 - val_accuracy: 0.8841\n",
      "Epoch 5/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3965 - accuracy: 0.8380 - val_loss: 0.3433 - val_accuracy: 0.8804\n",
      "Epoch 6/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3948 - accuracy: 0.8474 - val_loss: 0.3459 - val_accuracy: 0.8732\n",
      "Epoch 7/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4007 - accuracy: 0.8458 - val_loss: 0.3453 - val_accuracy: 0.8841\n",
      "Epoch 8/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3924 - accuracy: 0.8333 - val_loss: 0.3467 - val_accuracy: 0.8804\n",
      "Epoch 9/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4170 - accuracy: 0.8411 - val_loss: 0.3471 - val_accuracy: 0.8804\n",
      "Epoch 10/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4192 - accuracy: 0.8271 - val_loss: 0.3495 - val_accuracy: 0.8804\n",
      "Epoch 11/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3819 - accuracy: 0.8692 - val_loss: 0.3478 - val_accuracy: 0.8841\n",
      "Epoch 12/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3983 - accuracy: 0.8442 - val_loss: 0.3464 - val_accuracy: 0.8804\n",
      "Epoch 13/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3994 - accuracy: 0.8364 - val_loss: 0.3506 - val_accuracy: 0.8732\n",
      "Epoch 14/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3806 - accuracy: 0.8489 - val_loss: 0.3449 - val_accuracy: 0.8804\n",
      "Epoch 15/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3835 - accuracy: 0.8505 - val_loss: 0.3455 - val_accuracy: 0.8804\n",
      "Epoch 16/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3884 - accuracy: 0.8442 - val_loss: 0.3437 - val_accuracy: 0.8804\n",
      "Epoch 17/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4080 - accuracy: 0.8411 - val_loss: 0.3456 - val_accuracy: 0.8804\n",
      "Epoch 18/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4072 - accuracy: 0.8349 - val_loss: 0.3429 - val_accuracy: 0.8841\n",
      "Epoch 19/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3935 - accuracy: 0.8458 - val_loss: 0.3449 - val_accuracy: 0.8804\n",
      "Epoch 20/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4079 - accuracy: 0.8255 - val_loss: 0.3442 - val_accuracy: 0.8841\n",
      "Epoch 21/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3970 - accuracy: 0.8380 - val_loss: 0.3479 - val_accuracy: 0.8804\n",
      "Epoch 22/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3888 - accuracy: 0.8598 - val_loss: 0.3459 - val_accuracy: 0.8804\n",
      "Epoch 23/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3920 - accuracy: 0.8505 - val_loss: 0.3481 - val_accuracy: 0.8732\n",
      "Epoch 24/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3946 - accuracy: 0.8427 - val_loss: 0.3450 - val_accuracy: 0.8841\n",
      "Epoch 25/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3903 - accuracy: 0.8411 - val_loss: 0.3461 - val_accuracy: 0.8841\n",
      "Epoch 26/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4120 - accuracy: 0.8287 - val_loss: 0.3490 - val_accuracy: 0.8804\n",
      "Epoch 27/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4003 - accuracy: 0.8411 - val_loss: 0.3443 - val_accuracy: 0.8804\n",
      "Epoch 28/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3967 - accuracy: 0.8489 - val_loss: 0.3450 - val_accuracy: 0.8804\n",
      "Epoch 29/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4046 - accuracy: 0.8411 - val_loss: 0.3475 - val_accuracy: 0.8768\n",
      "Epoch 30/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3604 - accuracy: 0.8583 - val_loss: 0.3480 - val_accuracy: 0.8732\n",
      "Epoch 31/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4126 - accuracy: 0.8271 - val_loss: 0.3457 - val_accuracy: 0.8804\n",
      "Epoch 32/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3973 - accuracy: 0.8427 - val_loss: 0.3469 - val_accuracy: 0.8732\n",
      "Epoch 33/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3840 - accuracy: 0.8442 - val_loss: 0.3435 - val_accuracy: 0.8804\n",
      "Epoch 34/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4079 - accuracy: 0.8364 - val_loss: 0.3449 - val_accuracy: 0.8804\n",
      "Epoch 35/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3948 - accuracy: 0.8396 - val_loss: 0.3462 - val_accuracy: 0.8841\n",
      "Epoch 36/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3888 - accuracy: 0.8551 - val_loss: 0.3488 - val_accuracy: 0.8768\n",
      "Epoch 37/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3624 - accuracy: 0.8583 - val_loss: 0.3480 - val_accuracy: 0.8732\n",
      "Epoch 38/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3978 - accuracy: 0.8364 - val_loss: 0.3473 - val_accuracy: 0.8768\n",
      "Epoch 39/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3944 - accuracy: 0.8411 - val_loss: 0.3513 - val_accuracy: 0.8768\n",
      "Epoch 40/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4131 - accuracy: 0.8240 - val_loss: 0.3499 - val_accuracy: 0.8804\n",
      "Epoch 41/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3982 - accuracy: 0.8364 - val_loss: 0.3477 - val_accuracy: 0.8804\n",
      "Epoch 42/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3966 - accuracy: 0.8427 - val_loss: 0.3458 - val_accuracy: 0.8841\n",
      "Epoch 43/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3887 - accuracy: 0.8333 - val_loss: 0.3466 - val_accuracy: 0.8841\n",
      "Epoch 44/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3835 - accuracy: 0.8349 - val_loss: 0.3508 - val_accuracy: 0.8768\n",
      "Epoch 45/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3854 - accuracy: 0.8411 - val_loss: 0.3465 - val_accuracy: 0.8841\n",
      "Epoch 46/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3905 - accuracy: 0.8442 - val_loss: 0.3513 - val_accuracy: 0.8841\n",
      "Epoch 47/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3924 - accuracy: 0.8458 - val_loss: 0.3508 - val_accuracy: 0.8804\n",
      "Epoch 48/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3817 - accuracy: 0.8505 - val_loss: 0.3505 - val_accuracy: 0.8768\n",
      "Epoch 49/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3943 - accuracy: 0.8505 - val_loss: 0.3488 - val_accuracy: 0.8804\n",
      "Epoch 50/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3888 - accuracy: 0.8442 - val_loss: 0.3492 - val_accuracy: 0.8804\n",
      "Epoch 51/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3957 - accuracy: 0.8427 - val_loss: 0.3471 - val_accuracy: 0.8804\n",
      "Epoch 52/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3897 - accuracy: 0.8318 - val_loss: 0.3489 - val_accuracy: 0.8804\n",
      "Epoch 53/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3758 - accuracy: 0.8474 - val_loss: 0.3463 - val_accuracy: 0.8768\n",
      "Epoch 54/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4017 - accuracy: 0.8427 - val_loss: 0.3440 - val_accuracy: 0.8804\n",
      "Epoch 55/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3762 - accuracy: 0.8474 - val_loss: 0.3467 - val_accuracy: 0.8841\n",
      "Epoch 56/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3871 - accuracy: 0.8364 - val_loss: 0.3489 - val_accuracy: 0.8804\n",
      "Epoch 57/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4070 - accuracy: 0.8349 - val_loss: 0.3545 - val_accuracy: 0.8768\n",
      "Epoch 58/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3875 - accuracy: 0.8489 - val_loss: 0.3536 - val_accuracy: 0.8841\n",
      "Epoch 59/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3924 - accuracy: 0.8333 - val_loss: 0.3524 - val_accuracy: 0.8804\n",
      "Epoch 60/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3795 - accuracy: 0.8458 - val_loss: 0.3543 - val_accuracy: 0.8768\n",
      "Epoch 61/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3784 - accuracy: 0.8458 - val_loss: 0.3502 - val_accuracy: 0.8804\n",
      "Epoch 62/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3954 - accuracy: 0.8287 - val_loss: 0.3526 - val_accuracy: 0.8841\n",
      "Epoch 63/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3989 - accuracy: 0.8209 - val_loss: 0.3523 - val_accuracy: 0.8768\n",
      "Epoch 64/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4048 - accuracy: 0.8536 - val_loss: 0.3529 - val_accuracy: 0.8768\n",
      "Epoch 65/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3795 - accuracy: 0.8442 - val_loss: 0.3551 - val_accuracy: 0.8768\n",
      "Epoch 66/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3780 - accuracy: 0.8333 - val_loss: 0.3485 - val_accuracy: 0.8768\n",
      "Epoch 67/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3770 - accuracy: 0.8380 - val_loss: 0.3492 - val_accuracy: 0.8804\n",
      "Epoch 68/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4068 - accuracy: 0.8318 - val_loss: 0.3530 - val_accuracy: 0.8768\n",
      "Epoch 69/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3970 - accuracy: 0.8442 - val_loss: 0.3498 - val_accuracy: 0.8804\n",
      "Epoch 70/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4002 - accuracy: 0.8287 - val_loss: 0.3510 - val_accuracy: 0.8804\n",
      "Epoch 71/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3788 - accuracy: 0.8505 - val_loss: 0.3518 - val_accuracy: 0.8804\n",
      "Epoch 72/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3790 - accuracy: 0.8427 - val_loss: 0.3493 - val_accuracy: 0.8804\n",
      "Epoch 73/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3800 - accuracy: 0.8442 - val_loss: 0.3552 - val_accuracy: 0.8696\n",
      "Epoch 74/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3987 - accuracy: 0.8427 - val_loss: 0.3563 - val_accuracy: 0.8841\n",
      "Epoch 75/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3950 - accuracy: 0.8427 - val_loss: 0.3539 - val_accuracy: 0.8768\n",
      "Epoch 76/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4037 - accuracy: 0.8364 - val_loss: 0.3542 - val_accuracy: 0.8768\n",
      "Epoch 77/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3857 - accuracy: 0.8505 - val_loss: 0.3495 - val_accuracy: 0.8768\n",
      "Epoch 78/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3731 - accuracy: 0.8583 - val_loss: 0.3525 - val_accuracy: 0.8732\n",
      "Epoch 79/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3971 - accuracy: 0.8255 - val_loss: 0.3556 - val_accuracy: 0.8768\n",
      "Epoch 80/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3837 - accuracy: 0.8474 - val_loss: 0.3521 - val_accuracy: 0.8768\n",
      "Epoch 1/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4118 - accuracy: 0.8224 - val_loss: 0.3291 - val_accuracy: 0.8768\n",
      "Epoch 2/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3858 - accuracy: 0.8505 - val_loss: 0.3266 - val_accuracy: 0.8696\n",
      "Epoch 3/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4016 - accuracy: 0.8427 - val_loss: 0.3308 - val_accuracy: 0.8696\n",
      "Epoch 4/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3803 - accuracy: 0.8411 - val_loss: 0.3249 - val_accuracy: 0.8804\n",
      "Epoch 5/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3954 - accuracy: 0.8411 - val_loss: 0.3227 - val_accuracy: 0.8804\n",
      "Epoch 6/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3923 - accuracy: 0.8489 - val_loss: 0.3282 - val_accuracy: 0.8732\n",
      "Epoch 7/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3990 - accuracy: 0.8318 - val_loss: 0.3371 - val_accuracy: 0.8732\n",
      "Epoch 8/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4004 - accuracy: 0.8474 - val_loss: 0.3294 - val_accuracy: 0.8732\n",
      "Epoch 9/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3880 - accuracy: 0.8551 - val_loss: 0.3288 - val_accuracy: 0.8732\n",
      "Epoch 10/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3945 - accuracy: 0.8396 - val_loss: 0.3355 - val_accuracy: 0.8696\n",
      "Epoch 11/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3941 - accuracy: 0.8427 - val_loss: 0.3256 - val_accuracy: 0.8768\n",
      "Epoch 12/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3697 - accuracy: 0.8333 - val_loss: 0.3290 - val_accuracy: 0.8732\n",
      "Epoch 13/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3931 - accuracy: 0.8411 - val_loss: 0.3373 - val_accuracy: 0.8696\n",
      "Epoch 14/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3767 - accuracy: 0.8583 - val_loss: 0.3227 - val_accuracy: 0.8696\n",
      "Epoch 15/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4055 - accuracy: 0.8318 - val_loss: 0.3362 - val_accuracy: 0.8696\n",
      "Epoch 16/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4015 - accuracy: 0.8474 - val_loss: 0.3317 - val_accuracy: 0.8732\n",
      "Epoch 17/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4002 - accuracy: 0.8442 - val_loss: 0.3209 - val_accuracy: 0.8768\n",
      "Epoch 18/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4068 - accuracy: 0.8349 - val_loss: 0.3336 - val_accuracy: 0.8732\n",
      "Epoch 19/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3855 - accuracy: 0.8676 - val_loss: 0.3336 - val_accuracy: 0.8732\n",
      "Epoch 20/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4033 - accuracy: 0.8520 - val_loss: 0.3364 - val_accuracy: 0.8659\n",
      "Epoch 21/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3970 - accuracy: 0.8442 - val_loss: 0.3327 - val_accuracy: 0.8696\n",
      "Epoch 22/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3986 - accuracy: 0.8505 - val_loss: 0.3250 - val_accuracy: 0.8768\n",
      "Epoch 23/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3859 - accuracy: 0.8474 - val_loss: 0.3312 - val_accuracy: 0.8732\n",
      "Epoch 24/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3961 - accuracy: 0.8505 - val_loss: 0.3258 - val_accuracy: 0.8768\n",
      "Epoch 25/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4057 - accuracy: 0.8349 - val_loss: 0.3257 - val_accuracy: 0.8732\n",
      "Epoch 26/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3907 - accuracy: 0.8458 - val_loss: 0.3251 - val_accuracy: 0.8768\n",
      "Epoch 27/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3973 - accuracy: 0.8411 - val_loss: 0.3214 - val_accuracy: 0.8841\n",
      "Epoch 28/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4036 - accuracy: 0.8364 - val_loss: 0.3321 - val_accuracy: 0.8732\n",
      "Epoch 29/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3871 - accuracy: 0.8551 - val_loss: 0.3219 - val_accuracy: 0.8804\n",
      "Epoch 30/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3964 - accuracy: 0.8411 - val_loss: 0.3225 - val_accuracy: 0.8804\n",
      "Epoch 31/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4065 - accuracy: 0.8302 - val_loss: 0.3343 - val_accuracy: 0.8732\n",
      "Epoch 32/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4033 - accuracy: 0.8520 - val_loss: 0.3254 - val_accuracy: 0.8768\n",
      "Epoch 33/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4136 - accuracy: 0.8349 - val_loss: 0.3282 - val_accuracy: 0.8732\n",
      "Epoch 34/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3896 - accuracy: 0.8442 - val_loss: 0.3224 - val_accuracy: 0.8804\n",
      "Epoch 35/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3861 - accuracy: 0.8520 - val_loss: 0.3238 - val_accuracy: 0.8768\n",
      "Epoch 36/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3951 - accuracy: 0.8380 - val_loss: 0.3283 - val_accuracy: 0.8768\n",
      "Epoch 37/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3790 - accuracy: 0.8442 - val_loss: 0.3302 - val_accuracy: 0.8732\n",
      "Epoch 38/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3951 - accuracy: 0.8489 - val_loss: 0.3232 - val_accuracy: 0.8732\n",
      "Epoch 39/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3796 - accuracy: 0.8520 - val_loss: 0.3228 - val_accuracy: 0.8768\n",
      "Epoch 40/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.4020 - accuracy: 0.8287 - val_loss: 0.3245 - val_accuracy: 0.8804\n",
      "Epoch 41/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.4015 - accuracy: 0.8380 - val_loss: 0.3259 - val_accuracy: 0.8732\n",
      "Epoch 42/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.4128 - accuracy: 0.8380 - val_loss: 0.3321 - val_accuracy: 0.8732\n",
      "Epoch 43/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.4045 - accuracy: 0.8396 - val_loss: 0.3288 - val_accuracy: 0.8732\n",
      "Epoch 44/80\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.3948 - accuracy: 0.8380 - val_loss: 0.3238 - val_accuracy: 0.8804\n",
      "Epoch 45/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3899 - accuracy: 0.8489 - val_loss: 0.3285 - val_accuracy: 0.8768\n",
      "Epoch 46/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4005 - accuracy: 0.8396 - val_loss: 0.3230 - val_accuracy: 0.8804\n",
      "Epoch 47/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3889 - accuracy: 0.8567 - val_loss: 0.3265 - val_accuracy: 0.8732\n",
      "Epoch 48/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3851 - accuracy: 0.8474 - val_loss: 0.3276 - val_accuracy: 0.8768\n",
      "Epoch 49/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4002 - accuracy: 0.8380 - val_loss: 0.3297 - val_accuracy: 0.8804\n",
      "Epoch 50/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3850 - accuracy: 0.8489 - val_loss: 0.3312 - val_accuracy: 0.8804\n",
      "Epoch 51/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3948 - accuracy: 0.8505 - val_loss: 0.3250 - val_accuracy: 0.8768\n",
      "Epoch 52/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4025 - accuracy: 0.8271 - val_loss: 0.3312 - val_accuracy: 0.8696\n",
      "Epoch 53/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3886 - accuracy: 0.8411 - val_loss: 0.3232 - val_accuracy: 0.8768\n",
      "Epoch 54/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3970 - accuracy: 0.8364 - val_loss: 0.3319 - val_accuracy: 0.8732\n",
      "Epoch 55/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3787 - accuracy: 0.8598 - val_loss: 0.3344 - val_accuracy: 0.8732\n",
      "Epoch 56/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3890 - accuracy: 0.8536 - val_loss: 0.3298 - val_accuracy: 0.8696\n",
      "Epoch 57/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3922 - accuracy: 0.8505 - val_loss: 0.3264 - val_accuracy: 0.8696\n",
      "Epoch 58/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3855 - accuracy: 0.8489 - val_loss: 0.3235 - val_accuracy: 0.8696\n",
      "Epoch 59/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3856 - accuracy: 0.8536 - val_loss: 0.3251 - val_accuracy: 0.8877\n",
      "Epoch 60/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4016 - accuracy: 0.8474 - val_loss: 0.3262 - val_accuracy: 0.8696\n",
      "Epoch 61/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4062 - accuracy: 0.8240 - val_loss: 0.3272 - val_accuracy: 0.8696\n",
      "Epoch 62/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3888 - accuracy: 0.8396 - val_loss: 0.3315 - val_accuracy: 0.8696\n",
      "Epoch 63/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3882 - accuracy: 0.8520 - val_loss: 0.3246 - val_accuracy: 0.8768\n",
      "Epoch 64/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4221 - accuracy: 0.8427 - val_loss: 0.3258 - val_accuracy: 0.8732\n",
      "Epoch 65/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3933 - accuracy: 0.8411 - val_loss: 0.3219 - val_accuracy: 0.8804\n",
      "Epoch 66/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3991 - accuracy: 0.8380 - val_loss: 0.3275 - val_accuracy: 0.8696\n",
      "Epoch 67/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3990 - accuracy: 0.8209 - val_loss: 0.3218 - val_accuracy: 0.8768\n",
      "Epoch 68/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4039 - accuracy: 0.8287 - val_loss: 0.3242 - val_accuracy: 0.8841\n",
      "Epoch 69/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3793 - accuracy: 0.8489 - val_loss: 0.3280 - val_accuracy: 0.8659\n",
      "Epoch 70/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3965 - accuracy: 0.8660 - val_loss: 0.3249 - val_accuracy: 0.8804\n",
      "Epoch 71/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3903 - accuracy: 0.8520 - val_loss: 0.3266 - val_accuracy: 0.8732\n",
      "Epoch 72/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3801 - accuracy: 0.8567 - val_loss: 0.3236 - val_accuracy: 0.8696\n",
      "Epoch 73/80\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.3776 - accuracy: 0.8489 - val_loss: 0.3216 - val_accuracy: 0.8877\n",
      "Epoch 74/80\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.4142 - accuracy: 0.8333 - val_loss: 0.3318 - val_accuracy: 0.8659\n",
      "Epoch 75/80\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.3928 - accuracy: 0.8505 - val_loss: 0.3260 - val_accuracy: 0.8659\n",
      "Epoch 76/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3777 - accuracy: 0.8536 - val_loss: 0.3248 - val_accuracy: 0.8768\n",
      "Epoch 77/80\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.4024 - accuracy: 0.8380 - val_loss: 0.3238 - val_accuracy: 0.8841\n",
      "Epoch 78/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3972 - accuracy: 0.8567 - val_loss: 0.3201 - val_accuracy: 0.8841\n",
      "Epoch 79/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3958 - accuracy: 0.8411 - val_loss: 0.3217 - val_accuracy: 0.8768\n",
      "Epoch 80/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4043 - accuracy: 0.8364 - val_loss: 0.3250 - val_accuracy: 0.8696\n",
      "Epoch 1/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.4053 - accuracy: 0.8302 - val_loss: 0.3220 - val_accuracy: 0.8913\n",
      "Epoch 2/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3850 - accuracy: 0.8474 - val_loss: 0.3193 - val_accuracy: 0.8841\n",
      "Epoch 3/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4048 - accuracy: 0.8349 - val_loss: 0.3227 - val_accuracy: 0.8913\n",
      "Epoch 4/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3816 - accuracy: 0.8536 - val_loss: 0.3222 - val_accuracy: 0.8804\n",
      "Epoch 5/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.4130 - accuracy: 0.8380 - val_loss: 0.3257 - val_accuracy: 0.8877\n",
      "Epoch 6/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3932 - accuracy: 0.8396 - val_loss: 0.3228 - val_accuracy: 0.8768\n",
      "Epoch 7/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3890 - accuracy: 0.8411 - val_loss: 0.3284 - val_accuracy: 0.8804\n",
      "Epoch 8/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4011 - accuracy: 0.8349 - val_loss: 0.3327 - val_accuracy: 0.8804\n",
      "Epoch 9/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.4002 - accuracy: 0.8427 - val_loss: 0.3289 - val_accuracy: 0.8804\n",
      "Epoch 10/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.4014 - accuracy: 0.8396 - val_loss: 0.3356 - val_accuracy: 0.8804\n",
      "Epoch 11/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3915 - accuracy: 0.8333 - val_loss: 0.3257 - val_accuracy: 0.8877\n",
      "Epoch 12/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3803 - accuracy: 0.8427 - val_loss: 0.3290 - val_accuracy: 0.8913\n",
      "Epoch 13/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3914 - accuracy: 0.8458 - val_loss: 0.3228 - val_accuracy: 0.8768\n",
      "Epoch 14/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3960 - accuracy: 0.8427 - val_loss: 0.3223 - val_accuracy: 0.8768\n",
      "Epoch 15/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3987 - accuracy: 0.8240 - val_loss: 0.3258 - val_accuracy: 0.8877\n",
      "Epoch 16/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.4084 - accuracy: 0.8442 - val_loss: 0.3244 - val_accuracy: 0.8877\n",
      "Epoch 17/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3894 - accuracy: 0.8474 - val_loss: 0.3213 - val_accuracy: 0.8913\n",
      "Epoch 18/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3701 - accuracy: 0.8458 - val_loss: 0.3207 - val_accuracy: 0.8841\n",
      "Epoch 19/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3762 - accuracy: 0.8427 - val_loss: 0.3323 - val_accuracy: 0.8732\n",
      "Epoch 20/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3780 - accuracy: 0.8380 - val_loss: 0.3267 - val_accuracy: 0.8804\n",
      "Epoch 21/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3911 - accuracy: 0.8427 - val_loss: 0.3249 - val_accuracy: 0.8804\n",
      "Epoch 22/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3761 - accuracy: 0.8551 - val_loss: 0.3370 - val_accuracy: 0.8732\n",
      "Epoch 23/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3975 - accuracy: 0.8427 - val_loss: 0.3350 - val_accuracy: 0.8732\n",
      "Epoch 24/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3751 - accuracy: 0.8474 - val_loss: 0.3275 - val_accuracy: 0.8949\n",
      "Epoch 25/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3765 - accuracy: 0.8411 - val_loss: 0.3281 - val_accuracy: 0.8768\n",
      "Epoch 26/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3815 - accuracy: 0.8474 - val_loss: 0.3293 - val_accuracy: 0.8732\n",
      "Epoch 27/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3890 - accuracy: 0.8458 - val_loss: 0.3322 - val_accuracy: 0.8768\n",
      "Epoch 28/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3639 - accuracy: 0.8676 - val_loss: 0.3217 - val_accuracy: 0.8804\n",
      "Epoch 29/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3903 - accuracy: 0.8489 - val_loss: 0.3270 - val_accuracy: 0.8804\n",
      "Epoch 30/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3950 - accuracy: 0.8318 - val_loss: 0.3256 - val_accuracy: 0.8804\n",
      "Epoch 31/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3885 - accuracy: 0.8489 - val_loss: 0.3279 - val_accuracy: 0.8841\n",
      "Epoch 32/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3872 - accuracy: 0.8458 - val_loss: 0.3339 - val_accuracy: 0.8804\n",
      "Epoch 33/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3905 - accuracy: 0.8474 - val_loss: 0.3347 - val_accuracy: 0.8877\n",
      "Epoch 34/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3929 - accuracy: 0.8427 - val_loss: 0.3308 - val_accuracy: 0.8732\n",
      "Epoch 35/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3817 - accuracy: 0.8474 - val_loss: 0.3282 - val_accuracy: 0.8804\n",
      "Epoch 36/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3869 - accuracy: 0.8380 - val_loss: 0.3303 - val_accuracy: 0.8804\n",
      "Epoch 37/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3793 - accuracy: 0.8380 - val_loss: 0.3338 - val_accuracy: 0.8696\n",
      "Epoch 38/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3761 - accuracy: 0.8536 - val_loss: 0.3261 - val_accuracy: 0.8949\n",
      "Epoch 39/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3756 - accuracy: 0.8380 - val_loss: 0.3268 - val_accuracy: 0.8804\n",
      "Epoch 40/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3921 - accuracy: 0.8364 - val_loss: 0.3341 - val_accuracy: 0.8696\n",
      "Epoch 41/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3824 - accuracy: 0.8380 - val_loss: 0.3270 - val_accuracy: 0.8732\n",
      "Epoch 42/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3757 - accuracy: 0.8505 - val_loss: 0.3308 - val_accuracy: 0.8768\n",
      "Epoch 43/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4052 - accuracy: 0.8302 - val_loss: 0.3295 - val_accuracy: 0.8804\n",
      "Epoch 44/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3922 - accuracy: 0.8380 - val_loss: 0.3317 - val_accuracy: 0.8696\n",
      "Epoch 45/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3916 - accuracy: 0.8442 - val_loss: 0.3312 - val_accuracy: 0.8804\n",
      "Epoch 46/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3900 - accuracy: 0.8520 - val_loss: 0.3231 - val_accuracy: 0.8804\n",
      "Epoch 47/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3836 - accuracy: 0.8427 - val_loss: 0.3284 - val_accuracy: 0.8913\n",
      "Epoch 48/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3730 - accuracy: 0.8458 - val_loss: 0.3275 - val_accuracy: 0.8841\n",
      "Epoch 49/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4183 - accuracy: 0.8224 - val_loss: 0.3369 - val_accuracy: 0.8659\n",
      "Epoch 50/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3859 - accuracy: 0.8380 - val_loss: 0.3327 - val_accuracy: 0.8659\n",
      "Epoch 51/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3936 - accuracy: 0.8380 - val_loss: 0.3273 - val_accuracy: 0.8877\n",
      "Epoch 52/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3929 - accuracy: 0.8520 - val_loss: 0.3356 - val_accuracy: 0.8696\n",
      "Epoch 53/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3834 - accuracy: 0.8489 - val_loss: 0.3241 - val_accuracy: 0.8841\n",
      "Epoch 54/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3894 - accuracy: 0.8380 - val_loss: 0.3286 - val_accuracy: 0.8841\n",
      "Epoch 55/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3889 - accuracy: 0.8427 - val_loss: 0.3281 - val_accuracy: 0.8986\n",
      "Epoch 56/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3797 - accuracy: 0.8287 - val_loss: 0.3200 - val_accuracy: 0.8841\n",
      "Epoch 57/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3882 - accuracy: 0.8318 - val_loss: 0.3298 - val_accuracy: 0.8732\n",
      "Epoch 58/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3823 - accuracy: 0.8489 - val_loss: 0.3379 - val_accuracy: 0.8804\n",
      "Epoch 59/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3819 - accuracy: 0.8520 - val_loss: 0.3309 - val_accuracy: 0.8877\n",
      "Epoch 60/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3892 - accuracy: 0.8364 - val_loss: 0.3323 - val_accuracy: 0.8877\n",
      "Epoch 61/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3958 - accuracy: 0.8271 - val_loss: 0.3314 - val_accuracy: 0.8768\n",
      "Epoch 62/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3944 - accuracy: 0.8380 - val_loss: 0.3291 - val_accuracy: 0.8804\n",
      "Epoch 63/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3807 - accuracy: 0.8505 - val_loss: 0.3317 - val_accuracy: 0.8804\n",
      "Epoch 64/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3976 - accuracy: 0.8396 - val_loss: 0.3333 - val_accuracy: 0.8913\n",
      "Epoch 65/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4004 - accuracy: 0.8380 - val_loss: 0.3362 - val_accuracy: 0.8768\n",
      "Epoch 66/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3949 - accuracy: 0.8427 - val_loss: 0.3237 - val_accuracy: 0.8841\n",
      "Epoch 67/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3852 - accuracy: 0.8364 - val_loss: 0.3302 - val_accuracy: 0.8913\n",
      "Epoch 68/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3913 - accuracy: 0.8380 - val_loss: 0.3277 - val_accuracy: 0.8841\n",
      "Epoch 69/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3701 - accuracy: 0.8583 - val_loss: 0.3294 - val_accuracy: 0.8696\n",
      "Epoch 70/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3785 - accuracy: 0.8318 - val_loss: 0.3252 - val_accuracy: 0.8804\n",
      "Epoch 71/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3838 - accuracy: 0.8427 - val_loss: 0.3285 - val_accuracy: 0.8732\n",
      "Epoch 72/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3926 - accuracy: 0.8489 - val_loss: 0.3244 - val_accuracy: 0.8877\n",
      "Epoch 73/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3899 - accuracy: 0.8349 - val_loss: 0.3320 - val_accuracy: 0.8913\n",
      "Epoch 74/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3865 - accuracy: 0.8505 - val_loss: 0.3268 - val_accuracy: 0.8913\n",
      "Epoch 75/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3843 - accuracy: 0.8536 - val_loss: 0.3280 - val_accuracy: 0.8841\n",
      "Epoch 76/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3863 - accuracy: 0.8396 - val_loss: 0.3269 - val_accuracy: 0.8877\n",
      "Epoch 77/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3727 - accuracy: 0.8520 - val_loss: 0.3227 - val_accuracy: 0.8841\n",
      "Epoch 78/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3834 - accuracy: 0.8458 - val_loss: 0.3210 - val_accuracy: 0.8841\n",
      "Epoch 79/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3826 - accuracy: 0.8333 - val_loss: 0.3272 - val_accuracy: 0.8877\n",
      "Epoch 80/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3822 - accuracy: 0.8520 - val_loss: 0.3182 - val_accuracy: 0.8804\n",
      "Epoch 1/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3604 - accuracy: 0.8536 - val_loss: 0.3474 - val_accuracy: 0.8551\n",
      "Epoch 2/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3817 - accuracy: 0.8458 - val_loss: 0.3521 - val_accuracy: 0.8623\n",
      "Epoch 3/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3827 - accuracy: 0.8474 - val_loss: 0.3534 - val_accuracy: 0.8587\n",
      "Epoch 4/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3858 - accuracy: 0.8489 - val_loss: 0.3523 - val_accuracy: 0.8623\n",
      "Epoch 5/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3684 - accuracy: 0.8629 - val_loss: 0.3575 - val_accuracy: 0.8659\n",
      "Epoch 6/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3717 - accuracy: 0.8489 - val_loss: 0.3595 - val_accuracy: 0.8514\n",
      "Epoch 7/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3556 - accuracy: 0.8567 - val_loss: 0.3565 - val_accuracy: 0.8551\n",
      "Epoch 8/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3671 - accuracy: 0.8489 - val_loss: 0.3573 - val_accuracy: 0.8623\n",
      "Epoch 9/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3759 - accuracy: 0.8567 - val_loss: 0.3631 - val_accuracy: 0.8514\n",
      "Epoch 10/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3570 - accuracy: 0.8520 - val_loss: 0.3553 - val_accuracy: 0.8551\n",
      "Epoch 11/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3753 - accuracy: 0.8489 - val_loss: 0.3633 - val_accuracy: 0.8514\n",
      "Epoch 12/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3824 - accuracy: 0.8536 - val_loss: 0.3585 - val_accuracy: 0.8514\n",
      "Epoch 13/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3733 - accuracy: 0.8567 - val_loss: 0.3635 - val_accuracy: 0.8551\n",
      "Epoch 14/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3623 - accuracy: 0.8489 - val_loss: 0.3645 - val_accuracy: 0.8514\n",
      "Epoch 15/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3537 - accuracy: 0.8567 - val_loss: 0.3520 - val_accuracy: 0.8551\n",
      "Epoch 16/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3834 - accuracy: 0.8520 - val_loss: 0.3573 - val_accuracy: 0.8551\n",
      "Epoch 17/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3861 - accuracy: 0.8458 - val_loss: 0.3554 - val_accuracy: 0.8514\n",
      "Epoch 18/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3997 - accuracy: 0.8411 - val_loss: 0.3627 - val_accuracy: 0.8514\n",
      "Epoch 19/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3920 - accuracy: 0.8442 - val_loss: 0.3652 - val_accuracy: 0.8478\n",
      "Epoch 20/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3679 - accuracy: 0.8505 - val_loss: 0.3594 - val_accuracy: 0.8587\n",
      "Epoch 21/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3714 - accuracy: 0.8520 - val_loss: 0.3581 - val_accuracy: 0.8551\n",
      "Epoch 22/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3686 - accuracy: 0.8536 - val_loss: 0.3548 - val_accuracy: 0.8551\n",
      "Epoch 23/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3493 - accuracy: 0.8676 - val_loss: 0.3641 - val_accuracy: 0.8478\n",
      "Epoch 24/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3677 - accuracy: 0.8536 - val_loss: 0.3533 - val_accuracy: 0.8551\n",
      "Epoch 25/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3653 - accuracy: 0.8614 - val_loss: 0.3579 - val_accuracy: 0.8478\n",
      "Epoch 26/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3485 - accuracy: 0.8692 - val_loss: 0.3616 - val_accuracy: 0.8587\n",
      "Epoch 27/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3632 - accuracy: 0.8614 - val_loss: 0.3626 - val_accuracy: 0.8478\n",
      "Epoch 28/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3756 - accuracy: 0.8520 - val_loss: 0.3596 - val_accuracy: 0.8514\n",
      "Epoch 29/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3743 - accuracy: 0.8614 - val_loss: 0.3595 - val_accuracy: 0.8442\n",
      "Epoch 30/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3595 - accuracy: 0.8505 - val_loss: 0.3675 - val_accuracy: 0.8478\n",
      "Epoch 31/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3691 - accuracy: 0.8520 - val_loss: 0.3590 - val_accuracy: 0.8514\n",
      "Epoch 32/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3624 - accuracy: 0.8614 - val_loss: 0.3553 - val_accuracy: 0.8478\n",
      "Epoch 33/80\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.3742 - accuracy: 0.8474 - val_loss: 0.3600 - val_accuracy: 0.8514\n",
      "Epoch 34/80\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.3622 - accuracy: 0.8614 - val_loss: 0.3572 - val_accuracy: 0.8587\n",
      "Epoch 35/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3753 - accuracy: 0.8520 - val_loss: 0.3544 - val_accuracy: 0.8587\n",
      "Epoch 36/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3833 - accuracy: 0.8489 - val_loss: 0.3575 - val_accuracy: 0.8587\n",
      "Epoch 37/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3644 - accuracy: 0.8505 - val_loss: 0.3569 - val_accuracy: 0.8623\n",
      "Epoch 38/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3791 - accuracy: 0.8614 - val_loss: 0.3597 - val_accuracy: 0.8551\n",
      "Epoch 39/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3694 - accuracy: 0.8583 - val_loss: 0.3528 - val_accuracy: 0.8587\n",
      "Epoch 40/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3786 - accuracy: 0.8489 - val_loss: 0.3578 - val_accuracy: 0.8587\n",
      "Epoch 41/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3699 - accuracy: 0.8645 - val_loss: 0.3543 - val_accuracy: 0.8623\n",
      "Epoch 42/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3668 - accuracy: 0.8536 - val_loss: 0.3495 - val_accuracy: 0.8623\n",
      "Epoch 43/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3559 - accuracy: 0.8520 - val_loss: 0.3539 - val_accuracy: 0.8551\n",
      "Epoch 44/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3622 - accuracy: 0.8629 - val_loss: 0.3604 - val_accuracy: 0.8587\n",
      "Epoch 45/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3680 - accuracy: 0.8583 - val_loss: 0.3574 - val_accuracy: 0.8551\n",
      "Epoch 46/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3783 - accuracy: 0.8567 - val_loss: 0.3579 - val_accuracy: 0.8587\n",
      "Epoch 47/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3625 - accuracy: 0.8598 - val_loss: 0.3624 - val_accuracy: 0.8514\n",
      "Epoch 48/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3555 - accuracy: 0.8660 - val_loss: 0.3552 - val_accuracy: 0.8587\n",
      "Epoch 49/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3592 - accuracy: 0.8551 - val_loss: 0.3694 - val_accuracy: 0.8406\n",
      "Epoch 50/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3818 - accuracy: 0.8411 - val_loss: 0.3739 - val_accuracy: 0.8370\n",
      "Epoch 51/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3686 - accuracy: 0.8505 - val_loss: 0.3637 - val_accuracy: 0.8551\n",
      "Epoch 52/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3635 - accuracy: 0.8505 - val_loss: 0.3585 - val_accuracy: 0.8551\n",
      "Epoch 53/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3480 - accuracy: 0.8769 - val_loss: 0.3672 - val_accuracy: 0.8478\n",
      "Epoch 54/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3545 - accuracy: 0.8614 - val_loss: 0.3518 - val_accuracy: 0.8551\n",
      "Epoch 55/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3736 - accuracy: 0.8458 - val_loss: 0.3535 - val_accuracy: 0.8551\n",
      "Epoch 56/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3620 - accuracy: 0.8660 - val_loss: 0.3514 - val_accuracy: 0.8659\n",
      "Epoch 57/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3738 - accuracy: 0.8598 - val_loss: 0.3504 - val_accuracy: 0.8587\n",
      "Epoch 58/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3737 - accuracy: 0.8380 - val_loss: 0.3562 - val_accuracy: 0.8514\n",
      "Epoch 59/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3719 - accuracy: 0.8505 - val_loss: 0.3569 - val_accuracy: 0.8478\n",
      "Epoch 60/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3680 - accuracy: 0.8536 - val_loss: 0.3533 - val_accuracy: 0.8551\n",
      "Epoch 61/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3567 - accuracy: 0.8567 - val_loss: 0.3503 - val_accuracy: 0.8587\n",
      "Epoch 62/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3736 - accuracy: 0.8505 - val_loss: 0.3766 - val_accuracy: 0.8370\n",
      "Epoch 63/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3785 - accuracy: 0.8520 - val_loss: 0.3606 - val_accuracy: 0.8514\n",
      "Epoch 64/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3711 - accuracy: 0.8567 - val_loss: 0.3507 - val_accuracy: 0.8659\n",
      "Epoch 65/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3565 - accuracy: 0.8660 - val_loss: 0.3511 - val_accuracy: 0.8587\n",
      "Epoch 66/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3535 - accuracy: 0.8629 - val_loss: 0.3544 - val_accuracy: 0.8587\n",
      "Epoch 67/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3473 - accuracy: 0.8614 - val_loss: 0.3568 - val_accuracy: 0.8478\n",
      "Epoch 68/80\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.3752 - accuracy: 0.8442 - val_loss: 0.3511 - val_accuracy: 0.8623\n",
      "Epoch 69/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3581 - accuracy: 0.8614 - val_loss: 0.3688 - val_accuracy: 0.8442\n",
      "Epoch 70/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3698 - accuracy: 0.8520 - val_loss: 0.3635 - val_accuracy: 0.8442\n",
      "Epoch 71/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3871 - accuracy: 0.8489 - val_loss: 0.3534 - val_accuracy: 0.8514\n",
      "Epoch 72/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3662 - accuracy: 0.8551 - val_loss: 0.3618 - val_accuracy: 0.8442\n",
      "Epoch 73/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3596 - accuracy: 0.8738 - val_loss: 0.3527 - val_accuracy: 0.8587\n",
      "Epoch 74/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3667 - accuracy: 0.8614 - val_loss: 0.3598 - val_accuracy: 0.8514\n",
      "Epoch 75/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3597 - accuracy: 0.8567 - val_loss: 0.3629 - val_accuracy: 0.8514\n",
      "Epoch 76/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3762 - accuracy: 0.8629 - val_loss: 0.3656 - val_accuracy: 0.8478\n",
      "Epoch 77/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3801 - accuracy: 0.8458 - val_loss: 0.3556 - val_accuracy: 0.8514\n",
      "Epoch 78/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3521 - accuracy: 0.8567 - val_loss: 0.3562 - val_accuracy: 0.8478\n",
      "Epoch 79/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3592 - accuracy: 0.8660 - val_loss: 0.3579 - val_accuracy: 0.8551\n",
      "Epoch 80/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3717 - accuracy: 0.8474 - val_loss: 0.3534 - val_accuracy: 0.8587\n"
     ]
    }
   ],
   "source": [
    "history_temp=[]\n",
    "MLP_F1_temp=[]\n",
    "MLP_acc_temp=[]\n",
    "MLP_acctr_temp=[]\n",
    "MLP_recall_temp=[]\n",
    "\n",
    "for i in range(5):\n",
    "  if i==0:\n",
    "    seed=100\n",
    "  elif i==1:\n",
    "    seed=123\n",
    "  elif i==2:\n",
    "    seed=200\n",
    "  elif i==3:\n",
    "    seed=231\n",
    "  else:\n",
    "    seed=321\n",
    "\n",
    "  xtrain_1, xtest_1, ytrain_1, ytest_1 = model_selection.train_test_split(x, y, stratify = y, random_state = seed,train_size = 0.7, test_size = 0.3)\n",
    "\n",
    "  ytrain_1 = to_categorical(ytrain_1, num_classes = None)\n",
    "  ytest_1 = to_categorical(ytest_1, num_classes = None)\n",
    "\n",
    "  history=model_1.fit(xtrain_1, ytrain_1, validation_data=(xtest_1, ytest_1),epochs=80, batch_size=4)\n",
    "  history_temp.append(history)\n",
    "\n",
    "  y_pred = model_1.predict(xtest_1,batch_size=None)\n",
    "  y_pred_train = model_1.predict(xtrain_1,batch_size=None)\n",
    "  ypredbool = np.argmax(y_pred, axis=1)\n",
    "  ypredbool = to_categorical(ypredbool, num_classes = None)\n",
    "  ypredbooltr = np.argmax(y_pred_train, axis=1)\n",
    "  ypredbooltr = to_categorical(ypredbooltr, num_classes = None)\n",
    "  MLP_f1_score_1 = metrics.f1_score(ytest_1, ypredbool, average=None)\n",
    "  MLP_acc_score_1=metrics.accuracy_score(ytest_1, ypredbool)\n",
    "  MLP_acc_score_train_1=metrics.accuracy_score(ytrain_1, ypredbooltr)\n",
    "  MLP_recall_score_1 = metrics.recall_score(ytest_1, ypredbool, average=None)\n",
    "\n",
    "  MLP_F1_temp.append(MLP_f1_score_1[1])\n",
    "  MLP_acc_temp.append(MLP_acc_score_1)\n",
    "  MLP_acctr_temp.append(MLP_acc_score_train_1)\n",
    "  MLP_recall_temp.append(MLP_recall_score_1[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "EZ4JhWdfyifJ",
    "outputId": "1179f056-f244-44c9-84ec-82e41b29f951"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8664495114006514, 0.89171974522293, 0.8895705521472391, 0.8958990536277602, 0.8807339449541284]\n",
      "[0.8514492753623188, 0.8768115942028986, 0.8695652173913043, 0.8804347826086957, 0.8586956521739131]\n",
      "[0.8753894080996885, 0.8691588785046729, 0.8707165109034268, 0.8644859813084113, 0.881619937694704]\n",
      "[0.869281045751634, 0.9150326797385621, 0.9477124183006536, 0.9281045751633987, 0.9411764705882353]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-3baee7a5-2b9e-4c61-bcc6-9d30c9e52b7d\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Train accuracy</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>diferent of Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.869281</td>\n",
       "      <td>0.851449</td>\n",
       "      <td>0.875389</td>\n",
       "      <td>0.023940</td>\n",
       "      <td>0.866450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123</td>\n",
       "      <td>0.915033</td>\n",
       "      <td>0.876812</td>\n",
       "      <td>0.869159</td>\n",
       "      <td>-0.007653</td>\n",
       "      <td>0.891720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>0.947712</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.870717</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.889571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>231</td>\n",
       "      <td>0.928105</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.864486</td>\n",
       "      <td>-0.015949</td>\n",
       "      <td>0.895899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.858696</td>\n",
       "      <td>0.881620</td>\n",
       "      <td>0.022924</td>\n",
       "      <td>0.880734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3baee7a5-2b9e-4c61-bcc6-9d30c9e52b7d')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-3baee7a5-2b9e-4c61-bcc6-9d30c9e52b7d button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-3baee7a5-2b9e-4c61-bcc6-9d30c9e52b7d');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   seed    Recall  Train accuracy  Accuracy  diferent of Accuracy        F1\n",
       "0   100  0.869281        0.851449  0.875389              0.023940  0.866450\n",
       "1   123  0.915033        0.876812  0.869159             -0.007653  0.891720\n",
       "2   200  0.947712        0.869565  0.870717              0.001151  0.889571\n",
       "3   231  0.928105        0.880435  0.864486             -0.015949  0.895899\n",
       "4   321  0.941176        0.858696  0.881620              0.022924  0.880734"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(MLP_F1_temp)\n",
    "print(MLP_acc_temp)\n",
    "print(MLP_acctr_temp)\n",
    "print(MLP_recall_temp)\n",
    "\n",
    "delta1=MLP_acctr_temp[0]-MLP_acc_temp[0]\n",
    "delta2=MLP_acctr_temp[1]-MLP_acc_temp[1]\n",
    "delta3=MLP_acctr_temp[2]-MLP_acc_temp[2]\n",
    "delta4=MLP_acctr_temp[3]-MLP_acc_temp[3]\n",
    "delta5=MLP_acctr_temp[4]-MLP_acc_temp[4]\n",
    "MLP_delta_temp=[delta1,delta2,delta3,delta4,delta5]\n",
    "MLP=pd.DataFrame({'seed':[100,123,200,231,321],'Recall':[MLP_recall_temp[0],MLP_recall_temp[1],MLP_recall_temp[2],MLP_recall_temp[3],\n",
    "                                                     MLP_recall_temp[4]],'Train accuracy':[MLP_acc_temp[0],MLP_acc_temp[1],MLP_acc_temp[2],\n",
    "                                                    MLP_acc_temp[3],MLP_acc_temp[4]],'Accuracy':[MLP_acctr_temp[0],MLP_acctr_temp[1],MLP_acctr_temp[2],\n",
    "                                                    MLP_acctr_temp[3],MLP_acctr_temp[4]],'diferent of Accuracy':[delta1,delta2,delta3,delta4,delta5],'F1':[MLP_F1_temp[0],MLP_F1_temp[1],MLP_F1_temp[2],\n",
    "                                                    MLP_F1_temp[3],MLP_F1_temp[4]]})\n",
    "MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-p1GK2x7O5a"
   },
   "source": [
    "#Only MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EPoYEMNG7S37",
    "outputId": "6bdde3c5-71b0-4746-8270-8123d7a61d7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_48 (Dense)            (None, 6)                 78        \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 10)                70        \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 8)                 88        \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 2)                 18        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 254\n",
      "Trainable params: 254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "re=regularizers.L2(0.1)\n",
    "def create_model_2():\n",
    "    model_2 = Sequential()\n",
    "    #model_1.add(Dropout(0.1, input_dim = 12))\n",
    "    model_2.add(Dense(6,input_dim = 12, activation = 'relu'))\n",
    "    #model_1.add(Dropout(0.1))\n",
    "    model_2.add(Dense(10, activation = 'relu'))\n",
    "    #model_1.add(Dropout(0.1))\n",
    "    model_2.add(Dense(8, activation = 'relu'))\n",
    "    model_2.add(Dense(2, activation = 'sigmoid'))\n",
    "    model_2.compile(optimizer = Adam(learning_rate=0.00036), loss = 'binary_crossentropy', metrics = ['accuracy']) \n",
    "    return model_2\n",
    "\n",
    "model_2 = create_model_2()\n",
    "\n",
    "print(model_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C4LE5mWh7lsh",
    "outputId": "102b9e48-ee6e-480b-8f3f-5fe775c80e60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.6787 - accuracy: 0.5903 - val_loss: 0.6606 - val_accuracy: 0.6304\n",
      "Epoch 2/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6184 - accuracy: 0.7944 - val_loss: 0.5915 - val_accuracy: 0.7935\n",
      "Epoch 3/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5297 - accuracy: 0.8551 - val_loss: 0.5047 - val_accuracy: 0.8297\n",
      "Epoch 4/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4366 - accuracy: 0.8645 - val_loss: 0.4379 - val_accuracy: 0.8370\n",
      "Epoch 5/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3769 - accuracy: 0.8614 - val_loss: 0.4114 - val_accuracy: 0.8406\n",
      "Epoch 6/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3509 - accuracy: 0.8676 - val_loss: 0.4012 - val_accuracy: 0.8442\n",
      "Epoch 7/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3352 - accuracy: 0.8738 - val_loss: 0.3994 - val_accuracy: 0.8478\n",
      "Epoch 8/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3279 - accuracy: 0.8738 - val_loss: 0.3958 - val_accuracy: 0.8442\n",
      "Epoch 9/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3221 - accuracy: 0.8785 - val_loss: 0.3948 - val_accuracy: 0.8370\n",
      "Epoch 10/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3185 - accuracy: 0.8816 - val_loss: 0.3979 - val_accuracy: 0.8333\n",
      "Epoch 11/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3154 - accuracy: 0.8785 - val_loss: 0.3962 - val_accuracy: 0.8333\n",
      "Epoch 12/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3138 - accuracy: 0.8769 - val_loss: 0.3982 - val_accuracy: 0.8297\n",
      "Epoch 13/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3124 - accuracy: 0.8801 - val_loss: 0.4021 - val_accuracy: 0.8225\n",
      "Epoch 14/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3112 - accuracy: 0.8832 - val_loss: 0.4010 - val_accuracy: 0.8261\n",
      "Epoch 15/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3102 - accuracy: 0.8801 - val_loss: 0.4019 - val_accuracy: 0.8225\n",
      "Epoch 16/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3094 - accuracy: 0.8801 - val_loss: 0.4023 - val_accuracy: 0.8225\n",
      "Epoch 17/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3092 - accuracy: 0.8785 - val_loss: 0.4056 - val_accuracy: 0.8225\n",
      "Epoch 18/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3082 - accuracy: 0.8816 - val_loss: 0.4036 - val_accuracy: 0.8261\n",
      "Epoch 19/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3081 - accuracy: 0.8847 - val_loss: 0.4079 - val_accuracy: 0.8188\n",
      "Epoch 20/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3080 - accuracy: 0.8832 - val_loss: 0.4099 - val_accuracy: 0.8188\n",
      "Epoch 21/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3081 - accuracy: 0.8816 - val_loss: 0.4060 - val_accuracy: 0.8225\n",
      "Epoch 22/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3076 - accuracy: 0.8832 - val_loss: 0.4050 - val_accuracy: 0.8261\n",
      "Epoch 23/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3071 - accuracy: 0.8832 - val_loss: 0.4095 - val_accuracy: 0.8188\n",
      "Epoch 24/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3065 - accuracy: 0.8847 - val_loss: 0.4122 - val_accuracy: 0.8188\n",
      "Epoch 25/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3063 - accuracy: 0.8801 - val_loss: 0.4083 - val_accuracy: 0.8225\n",
      "Epoch 26/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3066 - accuracy: 0.8863 - val_loss: 0.4069 - val_accuracy: 0.8261\n",
      "Epoch 27/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3058 - accuracy: 0.8863 - val_loss: 0.4107 - val_accuracy: 0.8188\n",
      "Epoch 28/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3056 - accuracy: 0.8847 - val_loss: 0.4100 - val_accuracy: 0.8188\n",
      "Epoch 29/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3049 - accuracy: 0.8801 - val_loss: 0.4078 - val_accuracy: 0.8225\n",
      "Epoch 30/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3056 - accuracy: 0.8847 - val_loss: 0.4108 - val_accuracy: 0.8188\n",
      "Epoch 31/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3048 - accuracy: 0.8832 - val_loss: 0.4108 - val_accuracy: 0.8225\n",
      "Epoch 32/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3043 - accuracy: 0.8832 - val_loss: 0.4093 - val_accuracy: 0.8225\n",
      "Epoch 33/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3048 - accuracy: 0.8832 - val_loss: 0.4092 - val_accuracy: 0.8225\n",
      "Epoch 34/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3045 - accuracy: 0.8832 - val_loss: 0.4089 - val_accuracy: 0.8225\n",
      "Epoch 35/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3040 - accuracy: 0.8847 - val_loss: 0.4126 - val_accuracy: 0.8188\n",
      "Epoch 36/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3030 - accuracy: 0.8832 - val_loss: 0.4093 - val_accuracy: 0.8225\n",
      "Epoch 37/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3030 - accuracy: 0.8832 - val_loss: 0.4104 - val_accuracy: 0.8225\n",
      "Epoch 38/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3029 - accuracy: 0.8847 - val_loss: 0.4123 - val_accuracy: 0.8225\n",
      "Epoch 39/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3021 - accuracy: 0.8847 - val_loss: 0.4120 - val_accuracy: 0.8225\n",
      "Epoch 40/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3034 - accuracy: 0.8847 - val_loss: 0.4121 - val_accuracy: 0.8225\n",
      "Epoch 41/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3020 - accuracy: 0.8847 - val_loss: 0.4120 - val_accuracy: 0.8225\n",
      "Epoch 42/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3019 - accuracy: 0.8832 - val_loss: 0.4104 - val_accuracy: 0.8225\n",
      "Epoch 43/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3018 - accuracy: 0.8847 - val_loss: 0.4106 - val_accuracy: 0.8225\n",
      "Epoch 44/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3023 - accuracy: 0.8863 - val_loss: 0.4144 - val_accuracy: 0.8152\n",
      "Epoch 45/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3009 - accuracy: 0.8847 - val_loss: 0.4143 - val_accuracy: 0.8116\n",
      "Epoch 46/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3008 - accuracy: 0.8863 - val_loss: 0.4115 - val_accuracy: 0.8225\n",
      "Epoch 47/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3009 - accuracy: 0.8863 - val_loss: 0.4124 - val_accuracy: 0.8225\n",
      "Epoch 48/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3009 - accuracy: 0.8879 - val_loss: 0.4132 - val_accuracy: 0.8225\n",
      "Epoch 49/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3009 - accuracy: 0.8832 - val_loss: 0.4129 - val_accuracy: 0.8225\n",
      "Epoch 50/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3009 - accuracy: 0.8863 - val_loss: 0.4117 - val_accuracy: 0.8225\n",
      "Epoch 51/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3001 - accuracy: 0.8847 - val_loss: 0.4142 - val_accuracy: 0.8152\n",
      "Epoch 52/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2988 - accuracy: 0.8847 - val_loss: 0.4128 - val_accuracy: 0.8225\n",
      "Epoch 53/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2991 - accuracy: 0.8847 - val_loss: 0.4121 - val_accuracy: 0.8225\n",
      "Epoch 54/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.2990 - accuracy: 0.8863 - val_loss: 0.4134 - val_accuracy: 0.8225\n",
      "Epoch 55/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2986 - accuracy: 0.8847 - val_loss: 0.4136 - val_accuracy: 0.8188\n",
      "Epoch 56/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2985 - accuracy: 0.8863 - val_loss: 0.4151 - val_accuracy: 0.8188\n",
      "Epoch 57/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2986 - accuracy: 0.8879 - val_loss: 0.4122 - val_accuracy: 0.8188\n",
      "Epoch 58/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2983 - accuracy: 0.8847 - val_loss: 0.4170 - val_accuracy: 0.8116\n",
      "Epoch 59/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2984 - accuracy: 0.8847 - val_loss: 0.4139 - val_accuracy: 0.8225\n",
      "Epoch 60/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2973 - accuracy: 0.8863 - val_loss: 0.4158 - val_accuracy: 0.8152\n",
      "Epoch 61/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2971 - accuracy: 0.8894 - val_loss: 0.4136 - val_accuracy: 0.8188\n",
      "Epoch 62/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2980 - accuracy: 0.8832 - val_loss: 0.4146 - val_accuracy: 0.8188\n",
      "Epoch 63/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2970 - accuracy: 0.8847 - val_loss: 0.4147 - val_accuracy: 0.8188\n",
      "Epoch 64/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2970 - accuracy: 0.8894 - val_loss: 0.4144 - val_accuracy: 0.8188\n",
      "Epoch 65/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2965 - accuracy: 0.8847 - val_loss: 0.4162 - val_accuracy: 0.8188\n",
      "Epoch 66/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2972 - accuracy: 0.8863 - val_loss: 0.4159 - val_accuracy: 0.8188\n",
      "Epoch 67/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2969 - accuracy: 0.8879 - val_loss: 0.4160 - val_accuracy: 0.8188\n",
      "Epoch 68/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2962 - accuracy: 0.8879 - val_loss: 0.4163 - val_accuracy: 0.8188\n",
      "Epoch 69/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2966 - accuracy: 0.8894 - val_loss: 0.4172 - val_accuracy: 0.8116\n",
      "Epoch 70/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2961 - accuracy: 0.8863 - val_loss: 0.4160 - val_accuracy: 0.8188\n",
      "Epoch 71/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2949 - accuracy: 0.8863 - val_loss: 0.4169 - val_accuracy: 0.8152\n",
      "Epoch 72/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2947 - accuracy: 0.8894 - val_loss: 0.4145 - val_accuracy: 0.8225\n",
      "Epoch 73/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2945 - accuracy: 0.8894 - val_loss: 0.4152 - val_accuracy: 0.8188\n",
      "Epoch 74/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2947 - accuracy: 0.8894 - val_loss: 0.4144 - val_accuracy: 0.8152\n",
      "Epoch 75/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2956 - accuracy: 0.8894 - val_loss: 0.4150 - val_accuracy: 0.8152\n",
      "Epoch 76/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.2935 - accuracy: 0.8847 - val_loss: 0.4144 - val_accuracy: 0.8225\n",
      "Epoch 77/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.2946 - accuracy: 0.8879 - val_loss: 0.4149 - val_accuracy: 0.8152\n",
      "Epoch 78/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.2939 - accuracy: 0.8925 - val_loss: 0.4146 - val_accuracy: 0.8188\n",
      "Epoch 79/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.2935 - accuracy: 0.8894 - val_loss: 0.4166 - val_accuracy: 0.8116\n",
      "Epoch 80/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.2928 - accuracy: 0.8879 - val_loss: 0.4160 - val_accuracy: 0.8152\n",
      "Epoch 1/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3301 - accuracy: 0.8723 - val_loss: 0.3276 - val_accuracy: 0.8587\n",
      "Epoch 2/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3282 - accuracy: 0.8738 - val_loss: 0.3273 - val_accuracy: 0.8659\n",
      "Epoch 3/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3264 - accuracy: 0.8769 - val_loss: 0.3268 - val_accuracy: 0.8587\n",
      "Epoch 4/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3260 - accuracy: 0.8707 - val_loss: 0.3263 - val_accuracy: 0.8659\n",
      "Epoch 5/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3259 - accuracy: 0.8738 - val_loss: 0.3276 - val_accuracy: 0.8623\n",
      "Epoch 6/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3244 - accuracy: 0.8769 - val_loss: 0.3263 - val_accuracy: 0.8696\n",
      "Epoch 7/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3240 - accuracy: 0.8769 - val_loss: 0.3264 - val_accuracy: 0.8696\n",
      "Epoch 8/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3243 - accuracy: 0.8738 - val_loss: 0.3263 - val_accuracy: 0.8732\n",
      "Epoch 9/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3228 - accuracy: 0.8738 - val_loss: 0.3261 - val_accuracy: 0.8732\n",
      "Epoch 10/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3226 - accuracy: 0.8785 - val_loss: 0.3259 - val_accuracy: 0.8732\n",
      "Epoch 11/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3214 - accuracy: 0.8769 - val_loss: 0.3261 - val_accuracy: 0.8732\n",
      "Epoch 12/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3209 - accuracy: 0.8785 - val_loss: 0.3263 - val_accuracy: 0.8732\n",
      "Epoch 13/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3219 - accuracy: 0.8769 - val_loss: 0.3270 - val_accuracy: 0.8732\n",
      "Epoch 14/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3207 - accuracy: 0.8785 - val_loss: 0.3263 - val_accuracy: 0.8732\n",
      "Epoch 15/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3201 - accuracy: 0.8723 - val_loss: 0.3259 - val_accuracy: 0.8696\n",
      "Epoch 16/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3202 - accuracy: 0.8660 - val_loss: 0.3260 - val_accuracy: 0.8732\n",
      "Epoch 17/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3192 - accuracy: 0.8754 - val_loss: 0.3265 - val_accuracy: 0.8732\n",
      "Epoch 18/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3192 - accuracy: 0.8754 - val_loss: 0.3261 - val_accuracy: 0.8696\n",
      "Epoch 19/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3186 - accuracy: 0.8785 - val_loss: 0.3260 - val_accuracy: 0.8696\n",
      "Epoch 20/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3188 - accuracy: 0.8785 - val_loss: 0.3260 - val_accuracy: 0.8696\n",
      "Epoch 21/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3188 - accuracy: 0.8707 - val_loss: 0.3264 - val_accuracy: 0.8659\n",
      "Epoch 22/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3175 - accuracy: 0.8692 - val_loss: 0.3274 - val_accuracy: 0.8732\n",
      "Epoch 23/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3176 - accuracy: 0.8738 - val_loss: 0.3266 - val_accuracy: 0.8732\n",
      "Epoch 24/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3175 - accuracy: 0.8754 - val_loss: 0.3261 - val_accuracy: 0.8696\n",
      "Epoch 25/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3171 - accuracy: 0.8754 - val_loss: 0.3259 - val_accuracy: 0.8696\n",
      "Epoch 26/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3167 - accuracy: 0.8769 - val_loss: 0.3259 - val_accuracy: 0.8696\n",
      "Epoch 27/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3170 - accuracy: 0.8738 - val_loss: 0.3264 - val_accuracy: 0.8732\n",
      "Epoch 28/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3163 - accuracy: 0.8738 - val_loss: 0.3261 - val_accuracy: 0.8696\n",
      "Epoch 29/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3163 - accuracy: 0.8801 - val_loss: 0.3261 - val_accuracy: 0.8696\n",
      "Epoch 30/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3164 - accuracy: 0.8801 - val_loss: 0.3263 - val_accuracy: 0.8696\n",
      "Epoch 31/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3158 - accuracy: 0.8738 - val_loss: 0.3262 - val_accuracy: 0.8696\n",
      "Epoch 32/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3167 - accuracy: 0.8738 - val_loss: 0.3261 - val_accuracy: 0.8696\n",
      "Epoch 33/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3157 - accuracy: 0.8754 - val_loss: 0.3260 - val_accuracy: 0.8659\n",
      "Epoch 34/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3151 - accuracy: 0.8738 - val_loss: 0.3261 - val_accuracy: 0.8696\n",
      "Epoch 35/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3150 - accuracy: 0.8769 - val_loss: 0.3262 - val_accuracy: 0.8696\n",
      "Epoch 36/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3160 - accuracy: 0.8754 - val_loss: 0.3265 - val_accuracy: 0.8696\n",
      "Epoch 37/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3145 - accuracy: 0.8738 - val_loss: 0.3263 - val_accuracy: 0.8696\n",
      "Epoch 38/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3137 - accuracy: 0.8769 - val_loss: 0.3270 - val_accuracy: 0.8768\n",
      "Epoch 39/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3153 - accuracy: 0.8707 - val_loss: 0.3265 - val_accuracy: 0.8696\n",
      "Epoch 40/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3148 - accuracy: 0.8754 - val_loss: 0.3274 - val_accuracy: 0.8732\n",
      "Epoch 41/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3142 - accuracy: 0.8738 - val_loss: 0.3271 - val_accuracy: 0.8732\n",
      "Epoch 42/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3134 - accuracy: 0.8801 - val_loss: 0.3267 - val_accuracy: 0.8696\n",
      "Epoch 43/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3129 - accuracy: 0.8754 - val_loss: 0.3266 - val_accuracy: 0.8696\n",
      "Epoch 44/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3134 - accuracy: 0.8723 - val_loss: 0.3266 - val_accuracy: 0.8696\n",
      "Epoch 45/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3125 - accuracy: 0.8723 - val_loss: 0.3273 - val_accuracy: 0.8659\n",
      "Epoch 46/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3135 - accuracy: 0.8754 - val_loss: 0.3270 - val_accuracy: 0.8696\n",
      "Epoch 47/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3119 - accuracy: 0.8738 - val_loss: 0.3269 - val_accuracy: 0.8696\n",
      "Epoch 48/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3124 - accuracy: 0.8738 - val_loss: 0.3265 - val_accuracy: 0.8696\n",
      "Epoch 49/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3129 - accuracy: 0.8738 - val_loss: 0.3271 - val_accuracy: 0.8696\n",
      "Epoch 50/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3109 - accuracy: 0.8754 - val_loss: 0.3271 - val_accuracy: 0.8696\n",
      "Epoch 51/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3115 - accuracy: 0.8754 - val_loss: 0.3268 - val_accuracy: 0.8732\n",
      "Epoch 52/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3111 - accuracy: 0.8738 - val_loss: 0.3268 - val_accuracy: 0.8732\n",
      "Epoch 53/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3118 - accuracy: 0.8723 - val_loss: 0.3268 - val_accuracy: 0.8732\n",
      "Epoch 54/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3110 - accuracy: 0.8738 - val_loss: 0.3270 - val_accuracy: 0.8696\n",
      "Epoch 55/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3101 - accuracy: 0.8723 - val_loss: 0.3274 - val_accuracy: 0.8696\n",
      "Epoch 56/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3122 - accuracy: 0.8769 - val_loss: 0.3271 - val_accuracy: 0.8696\n",
      "Epoch 57/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3108 - accuracy: 0.8754 - val_loss: 0.3270 - val_accuracy: 0.8696\n",
      "Epoch 58/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3105 - accuracy: 0.8723 - val_loss: 0.3271 - val_accuracy: 0.8732\n",
      "Epoch 59/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3097 - accuracy: 0.8738 - val_loss: 0.3274 - val_accuracy: 0.8696\n",
      "Epoch 60/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3104 - accuracy: 0.8754 - val_loss: 0.3272 - val_accuracy: 0.8696\n",
      "Epoch 61/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3094 - accuracy: 0.8723 - val_loss: 0.3274 - val_accuracy: 0.8732\n",
      "Epoch 62/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3097 - accuracy: 0.8707 - val_loss: 0.3274 - val_accuracy: 0.8732\n",
      "Epoch 63/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3098 - accuracy: 0.8754 - val_loss: 0.3272 - val_accuracy: 0.8696\n",
      "Epoch 64/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3093 - accuracy: 0.8692 - val_loss: 0.3274 - val_accuracy: 0.8732\n",
      "Epoch 65/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3086 - accuracy: 0.8754 - val_loss: 0.3275 - val_accuracy: 0.8659\n",
      "Epoch 66/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3082 - accuracy: 0.8723 - val_loss: 0.3285 - val_accuracy: 0.8768\n",
      "Epoch 67/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3090 - accuracy: 0.8723 - val_loss: 0.3274 - val_accuracy: 0.8732\n",
      "Epoch 68/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3093 - accuracy: 0.8723 - val_loss: 0.3275 - val_accuracy: 0.8732\n",
      "Epoch 69/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3085 - accuracy: 0.8754 - val_loss: 0.3272 - val_accuracy: 0.8768\n",
      "Epoch 70/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3089 - accuracy: 0.8738 - val_loss: 0.3273 - val_accuracy: 0.8732\n",
      "Epoch 71/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3091 - accuracy: 0.8738 - val_loss: 0.3275 - val_accuracy: 0.8732\n",
      "Epoch 72/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3078 - accuracy: 0.8723 - val_loss: 0.3272 - val_accuracy: 0.8768\n",
      "Epoch 73/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3076 - accuracy: 0.8738 - val_loss: 0.3275 - val_accuracy: 0.8768\n",
      "Epoch 74/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3079 - accuracy: 0.8676 - val_loss: 0.3283 - val_accuracy: 0.8768\n",
      "Epoch 75/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3072 - accuracy: 0.8723 - val_loss: 0.3291 - val_accuracy: 0.8768\n",
      "Epoch 76/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3071 - accuracy: 0.8769 - val_loss: 0.3281 - val_accuracy: 0.8696\n",
      "Epoch 77/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3075 - accuracy: 0.8769 - val_loss: 0.3277 - val_accuracy: 0.8732\n",
      "Epoch 78/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3065 - accuracy: 0.8707 - val_loss: 0.3280 - val_accuracy: 0.8732\n",
      "Epoch 79/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3075 - accuracy: 0.8769 - val_loss: 0.3289 - val_accuracy: 0.8623\n",
      "Epoch 80/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3073 - accuracy: 0.8738 - val_loss: 0.3276 - val_accuracy: 0.8768\n",
      "Epoch 1/80\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.3326 - accuracy: 0.8738 - val_loss: 0.2683 - val_accuracy: 0.8732\n",
      "Epoch 2/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3321 - accuracy: 0.8723 - val_loss: 0.2684 - val_accuracy: 0.8841\n",
      "Epoch 3/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3312 - accuracy: 0.8723 - val_loss: 0.2710 - val_accuracy: 0.8732\n",
      "Epoch 4/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3303 - accuracy: 0.8738 - val_loss: 0.2730 - val_accuracy: 0.8804\n",
      "Epoch 5/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3293 - accuracy: 0.8723 - val_loss: 0.2738 - val_accuracy: 0.8804\n",
      "Epoch 6/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3296 - accuracy: 0.8738 - val_loss: 0.2754 - val_accuracy: 0.8804\n",
      "Epoch 7/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3283 - accuracy: 0.8738 - val_loss: 0.2783 - val_accuracy: 0.8804\n",
      "Epoch 8/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3284 - accuracy: 0.8738 - val_loss: 0.2787 - val_accuracy: 0.8768\n",
      "Epoch 9/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3274 - accuracy: 0.8754 - val_loss: 0.2772 - val_accuracy: 0.8696\n",
      "Epoch 10/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3274 - accuracy: 0.8738 - val_loss: 0.2780 - val_accuracy: 0.8732\n",
      "Epoch 11/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3270 - accuracy: 0.8738 - val_loss: 0.2790 - val_accuracy: 0.8732\n",
      "Epoch 12/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3268 - accuracy: 0.8754 - val_loss: 0.2804 - val_accuracy: 0.8768\n",
      "Epoch 13/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3265 - accuracy: 0.8723 - val_loss: 0.2795 - val_accuracy: 0.8732\n",
      "Epoch 14/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3277 - accuracy: 0.8707 - val_loss: 0.2822 - val_accuracy: 0.8732\n",
      "Epoch 15/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3253 - accuracy: 0.8769 - val_loss: 0.2831 - val_accuracy: 0.8804\n",
      "Epoch 16/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3255 - accuracy: 0.8676 - val_loss: 0.2812 - val_accuracy: 0.8732\n",
      "Epoch 17/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3251 - accuracy: 0.8754 - val_loss: 0.2837 - val_accuracy: 0.8768\n",
      "Epoch 18/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3260 - accuracy: 0.8769 - val_loss: 0.2840 - val_accuracy: 0.8768\n",
      "Epoch 19/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3250 - accuracy: 0.8769 - val_loss: 0.2832 - val_accuracy: 0.8768\n",
      "Epoch 20/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3247 - accuracy: 0.8723 - val_loss: 0.2845 - val_accuracy: 0.8804\n",
      "Epoch 21/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3248 - accuracy: 0.8738 - val_loss: 0.2846 - val_accuracy: 0.8804\n",
      "Epoch 22/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3239 - accuracy: 0.8738 - val_loss: 0.2828 - val_accuracy: 0.8732\n",
      "Epoch 23/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3243 - accuracy: 0.8723 - val_loss: 0.2832 - val_accuracy: 0.8732\n",
      "Epoch 24/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3234 - accuracy: 0.8769 - val_loss: 0.2883 - val_accuracy: 0.8732\n",
      "Epoch 25/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3246 - accuracy: 0.8738 - val_loss: 0.2841 - val_accuracy: 0.8768\n",
      "Epoch 26/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3240 - accuracy: 0.8738 - val_loss: 0.2874 - val_accuracy: 0.8732\n",
      "Epoch 27/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3239 - accuracy: 0.8754 - val_loss: 0.2857 - val_accuracy: 0.8732\n",
      "Epoch 28/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3234 - accuracy: 0.8754 - val_loss: 0.2878 - val_accuracy: 0.8732\n",
      "Epoch 29/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3233 - accuracy: 0.8738 - val_loss: 0.2871 - val_accuracy: 0.8732\n",
      "Epoch 30/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3223 - accuracy: 0.8769 - val_loss: 0.2850 - val_accuracy: 0.8768\n",
      "Epoch 31/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3227 - accuracy: 0.8785 - val_loss: 0.2861 - val_accuracy: 0.8804\n",
      "Epoch 32/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3230 - accuracy: 0.8785 - val_loss: 0.2864 - val_accuracy: 0.8732\n",
      "Epoch 33/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3227 - accuracy: 0.8785 - val_loss: 0.2866 - val_accuracy: 0.8804\n",
      "Epoch 34/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3231 - accuracy: 0.8785 - val_loss: 0.2844 - val_accuracy: 0.8768\n",
      "Epoch 35/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3238 - accuracy: 0.8738 - val_loss: 0.2857 - val_accuracy: 0.8768\n",
      "Epoch 36/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3221 - accuracy: 0.8738 - val_loss: 0.2846 - val_accuracy: 0.8768\n",
      "Epoch 37/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3221 - accuracy: 0.8785 - val_loss: 0.2874 - val_accuracy: 0.8732\n",
      "Epoch 38/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3214 - accuracy: 0.8738 - val_loss: 0.2883 - val_accuracy: 0.8732\n",
      "Epoch 39/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3215 - accuracy: 0.8769 - val_loss: 0.2862 - val_accuracy: 0.8768\n",
      "Epoch 40/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3213 - accuracy: 0.8754 - val_loss: 0.2902 - val_accuracy: 0.8732\n",
      "Epoch 41/80\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.3212 - accuracy: 0.8754 - val_loss: 0.2887 - val_accuracy: 0.8732\n",
      "Epoch 42/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3207 - accuracy: 0.8754 - val_loss: 0.2886 - val_accuracy: 0.8732\n",
      "Epoch 43/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3214 - accuracy: 0.8723 - val_loss: 0.2868 - val_accuracy: 0.8768\n",
      "Epoch 44/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3201 - accuracy: 0.8738 - val_loss: 0.2873 - val_accuracy: 0.8768\n",
      "Epoch 45/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3204 - accuracy: 0.8769 - val_loss: 0.2890 - val_accuracy: 0.8732\n",
      "Epoch 46/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3201 - accuracy: 0.8769 - val_loss: 0.2872 - val_accuracy: 0.8732\n",
      "Epoch 47/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3190 - accuracy: 0.8754 - val_loss: 0.2907 - val_accuracy: 0.8732\n",
      "Epoch 48/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3195 - accuracy: 0.8738 - val_loss: 0.2865 - val_accuracy: 0.8768\n",
      "Epoch 49/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3207 - accuracy: 0.8769 - val_loss: 0.2860 - val_accuracy: 0.8732\n",
      "Epoch 50/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3197 - accuracy: 0.8769 - val_loss: 0.2884 - val_accuracy: 0.8732\n",
      "Epoch 51/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3196 - accuracy: 0.8769 - val_loss: 0.2873 - val_accuracy: 0.8768\n",
      "Epoch 52/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3195 - accuracy: 0.8754 - val_loss: 0.2880 - val_accuracy: 0.8732\n",
      "Epoch 53/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3200 - accuracy: 0.8769 - val_loss: 0.2902 - val_accuracy: 0.8732\n",
      "Epoch 54/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3192 - accuracy: 0.8769 - val_loss: 0.2873 - val_accuracy: 0.8768\n",
      "Epoch 55/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3188 - accuracy: 0.8754 - val_loss: 0.2882 - val_accuracy: 0.8768\n",
      "Epoch 56/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3196 - accuracy: 0.8769 - val_loss: 0.2890 - val_accuracy: 0.8732\n",
      "Epoch 57/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3183 - accuracy: 0.8785 - val_loss: 0.2912 - val_accuracy: 0.8696\n",
      "Epoch 58/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3189 - accuracy: 0.8785 - val_loss: 0.2885 - val_accuracy: 0.8768\n",
      "Epoch 59/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3181 - accuracy: 0.8785 - val_loss: 0.2869 - val_accuracy: 0.8804\n",
      "Epoch 60/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3184 - accuracy: 0.8723 - val_loss: 0.2869 - val_accuracy: 0.8732\n",
      "Epoch 61/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3174 - accuracy: 0.8738 - val_loss: 0.2914 - val_accuracy: 0.8659\n",
      "Epoch 62/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3179 - accuracy: 0.8769 - val_loss: 0.2913 - val_accuracy: 0.8659\n",
      "Epoch 63/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3185 - accuracy: 0.8754 - val_loss: 0.2874 - val_accuracy: 0.8768\n",
      "Epoch 64/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3179 - accuracy: 0.8723 - val_loss: 0.2888 - val_accuracy: 0.8768\n",
      "Epoch 65/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3174 - accuracy: 0.8769 - val_loss: 0.2896 - val_accuracy: 0.8732\n",
      "Epoch 66/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3172 - accuracy: 0.8754 - val_loss: 0.2878 - val_accuracy: 0.8804\n",
      "Epoch 67/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3167 - accuracy: 0.8801 - val_loss: 0.2898 - val_accuracy: 0.8768\n",
      "Epoch 68/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3172 - accuracy: 0.8754 - val_loss: 0.2882 - val_accuracy: 0.8768\n",
      "Epoch 69/80\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.3160 - accuracy: 0.8754 - val_loss: 0.2886 - val_accuracy: 0.8768\n",
      "Epoch 70/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3170 - accuracy: 0.8769 - val_loss: 0.2882 - val_accuracy: 0.8768\n",
      "Epoch 71/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3164 - accuracy: 0.8769 - val_loss: 0.2870 - val_accuracy: 0.8696\n",
      "Epoch 72/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3162 - accuracy: 0.8785 - val_loss: 0.2878 - val_accuracy: 0.8768\n",
      "Epoch 73/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3159 - accuracy: 0.8801 - val_loss: 0.2869 - val_accuracy: 0.8732\n",
      "Epoch 74/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3164 - accuracy: 0.8723 - val_loss: 0.2920 - val_accuracy: 0.8696\n",
      "Epoch 75/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3155 - accuracy: 0.8738 - val_loss: 0.2887 - val_accuracy: 0.8804\n",
      "Epoch 76/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3156 - accuracy: 0.8785 - val_loss: 0.2887 - val_accuracy: 0.8768\n",
      "Epoch 77/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3159 - accuracy: 0.8801 - val_loss: 0.2914 - val_accuracy: 0.8732\n",
      "Epoch 78/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3150 - accuracy: 0.8785 - val_loss: 0.2894 - val_accuracy: 0.8768\n",
      "Epoch 79/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3148 - accuracy: 0.8769 - val_loss: 0.2878 - val_accuracy: 0.8659\n",
      "Epoch 80/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3162 - accuracy: 0.8785 - val_loss: 0.2897 - val_accuracy: 0.8732\n",
      "Epoch 1/80\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.3137 - accuracy: 0.8645 - val_loss: 0.2935 - val_accuracy: 0.8949\n",
      "Epoch 2/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3117 - accuracy: 0.8660 - val_loss: 0.2965 - val_accuracy: 0.8841\n",
      "Epoch 3/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3131 - accuracy: 0.8707 - val_loss: 0.2957 - val_accuracy: 0.8877\n",
      "Epoch 4/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3119 - accuracy: 0.8660 - val_loss: 0.2953 - val_accuracy: 0.8949\n",
      "Epoch 5/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3109 - accuracy: 0.8676 - val_loss: 0.2963 - val_accuracy: 0.8913\n",
      "Epoch 6/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3111 - accuracy: 0.8676 - val_loss: 0.2965 - val_accuracy: 0.8949\n",
      "Epoch 7/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3100 - accuracy: 0.8707 - val_loss: 0.3000 - val_accuracy: 0.8841\n",
      "Epoch 8/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3097 - accuracy: 0.8692 - val_loss: 0.2971 - val_accuracy: 0.8877\n",
      "Epoch 9/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3094 - accuracy: 0.8707 - val_loss: 0.2975 - val_accuracy: 0.8949\n",
      "Epoch 10/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3092 - accuracy: 0.8707 - val_loss: 0.2994 - val_accuracy: 0.8949\n",
      "Epoch 11/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3088 - accuracy: 0.8692 - val_loss: 0.3012 - val_accuracy: 0.8877\n",
      "Epoch 12/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3082 - accuracy: 0.8676 - val_loss: 0.2995 - val_accuracy: 0.8949\n",
      "Epoch 13/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3090 - accuracy: 0.8692 - val_loss: 0.3000 - val_accuracy: 0.8986\n",
      "Epoch 14/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3091 - accuracy: 0.8660 - val_loss: 0.3007 - val_accuracy: 0.8986\n",
      "Epoch 15/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3081 - accuracy: 0.8692 - val_loss: 0.3013 - val_accuracy: 0.8913\n",
      "Epoch 16/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3081 - accuracy: 0.8676 - val_loss: 0.3008 - val_accuracy: 0.8986\n",
      "Epoch 17/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3075 - accuracy: 0.8707 - val_loss: 0.3006 - val_accuracy: 0.8986\n",
      "Epoch 18/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3077 - accuracy: 0.8723 - val_loss: 0.3022 - val_accuracy: 0.8949\n",
      "Epoch 19/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3092 - accuracy: 0.8692 - val_loss: 0.3015 - val_accuracy: 0.8986\n",
      "Epoch 20/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3077 - accuracy: 0.8723 - val_loss: 0.3021 - val_accuracy: 0.8986\n",
      "Epoch 21/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3073 - accuracy: 0.8692 - val_loss: 0.3026 - val_accuracy: 0.8949\n",
      "Epoch 22/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3072 - accuracy: 0.8723 - val_loss: 0.3032 - val_accuracy: 0.8949\n",
      "Epoch 23/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3068 - accuracy: 0.8676 - val_loss: 0.3025 - val_accuracy: 0.8949\n",
      "Epoch 24/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3051 - accuracy: 0.8723 - val_loss: 0.3109 - val_accuracy: 0.8913\n",
      "Epoch 25/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3079 - accuracy: 0.8707 - val_loss: 0.3049 - val_accuracy: 0.8877\n",
      "Epoch 26/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3046 - accuracy: 0.8707 - val_loss: 0.3031 - val_accuracy: 0.8949\n",
      "Epoch 27/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3069 - accuracy: 0.8676 - val_loss: 0.3050 - val_accuracy: 0.8877\n",
      "Epoch 28/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3062 - accuracy: 0.8676 - val_loss: 0.3078 - val_accuracy: 0.8949\n",
      "Epoch 29/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3068 - accuracy: 0.8692 - val_loss: 0.3090 - val_accuracy: 0.8949\n",
      "Epoch 30/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3068 - accuracy: 0.8738 - val_loss: 0.3105 - val_accuracy: 0.8949\n",
      "Epoch 31/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3052 - accuracy: 0.8645 - val_loss: 0.3105 - val_accuracy: 0.8913\n",
      "Epoch 32/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3059 - accuracy: 0.8738 - val_loss: 0.3068 - val_accuracy: 0.8841\n",
      "Epoch 33/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3056 - accuracy: 0.8738 - val_loss: 0.3076 - val_accuracy: 0.8877\n",
      "Epoch 34/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3047 - accuracy: 0.8676 - val_loss: 0.3059 - val_accuracy: 0.8913\n",
      "Epoch 35/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3046 - accuracy: 0.8660 - val_loss: 0.3095 - val_accuracy: 0.8877\n",
      "Epoch 36/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3051 - accuracy: 0.8707 - val_loss: 0.3134 - val_accuracy: 0.8949\n",
      "Epoch 37/80\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.3052 - accuracy: 0.8738 - val_loss: 0.3105 - val_accuracy: 0.8841\n",
      "Epoch 38/80\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.3046 - accuracy: 0.8723 - val_loss: 0.3075 - val_accuracy: 0.8877\n",
      "Epoch 39/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3036 - accuracy: 0.8707 - val_loss: 0.3093 - val_accuracy: 0.8913\n",
      "Epoch 40/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3042 - accuracy: 0.8723 - val_loss: 0.3075 - val_accuracy: 0.8877\n",
      "Epoch 41/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3038 - accuracy: 0.8692 - val_loss: 0.3089 - val_accuracy: 0.8877\n",
      "Epoch 42/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3041 - accuracy: 0.8723 - val_loss: 0.3085 - val_accuracy: 0.8949\n",
      "Epoch 43/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3024 - accuracy: 0.8707 - val_loss: 0.3078 - val_accuracy: 0.8913\n",
      "Epoch 44/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3047 - accuracy: 0.8738 - val_loss: 0.3075 - val_accuracy: 0.8913\n",
      "Epoch 45/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3031 - accuracy: 0.8723 - val_loss: 0.3095 - val_accuracy: 0.8877\n",
      "Epoch 46/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3028 - accuracy: 0.8723 - val_loss: 0.3090 - val_accuracy: 0.8913\n",
      "Epoch 47/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3034 - accuracy: 0.8723 - val_loss: 0.3088 - val_accuracy: 0.8913\n",
      "Epoch 48/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3009 - accuracy: 0.8738 - val_loss: 0.3086 - val_accuracy: 0.8913\n",
      "Epoch 49/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3034 - accuracy: 0.8676 - val_loss: 0.3089 - val_accuracy: 0.8913\n",
      "Epoch 50/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3021 - accuracy: 0.8676 - val_loss: 0.3089 - val_accuracy: 0.8877\n",
      "Epoch 51/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3034 - accuracy: 0.8707 - val_loss: 0.3098 - val_accuracy: 0.8877\n",
      "Epoch 52/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3018 - accuracy: 0.8707 - val_loss: 0.3154 - val_accuracy: 0.8913\n",
      "Epoch 53/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3011 - accuracy: 0.8707 - val_loss: 0.3105 - val_accuracy: 0.8877\n",
      "Epoch 54/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3024 - accuracy: 0.8707 - val_loss: 0.3108 - val_accuracy: 0.8877\n",
      "Epoch 55/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3001 - accuracy: 0.8707 - val_loss: 0.3169 - val_accuracy: 0.8841\n",
      "Epoch 56/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3024 - accuracy: 0.8676 - val_loss: 0.3123 - val_accuracy: 0.8841\n",
      "Epoch 57/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3020 - accuracy: 0.8754 - val_loss: 0.3108 - val_accuracy: 0.8877\n",
      "Epoch 58/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3018 - accuracy: 0.8660 - val_loss: 0.3144 - val_accuracy: 0.8841\n",
      "Epoch 59/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3016 - accuracy: 0.8707 - val_loss: 0.3112 - val_accuracy: 0.8913\n",
      "Epoch 60/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3009 - accuracy: 0.8692 - val_loss: 0.3128 - val_accuracy: 0.8877\n",
      "Epoch 61/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3002 - accuracy: 0.8692 - val_loss: 0.3147 - val_accuracy: 0.8841\n",
      "Epoch 62/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3007 - accuracy: 0.8723 - val_loss: 0.3122 - val_accuracy: 0.8913\n",
      "Epoch 63/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3011 - accuracy: 0.8692 - val_loss: 0.3114 - val_accuracy: 0.8877\n",
      "Epoch 64/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3007 - accuracy: 0.8676 - val_loss: 0.3165 - val_accuracy: 0.8841\n",
      "Epoch 65/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3018 - accuracy: 0.8707 - val_loss: 0.3140 - val_accuracy: 0.8841\n",
      "Epoch 66/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3002 - accuracy: 0.8723 - val_loss: 0.3174 - val_accuracy: 0.8841\n",
      "Epoch 67/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3011 - accuracy: 0.8660 - val_loss: 0.3125 - val_accuracy: 0.8804\n",
      "Epoch 68/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.2996 - accuracy: 0.8723 - val_loss: 0.3129 - val_accuracy: 0.8913\n",
      "Epoch 69/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.2997 - accuracy: 0.8723 - val_loss: 0.3123 - val_accuracy: 0.8877\n",
      "Epoch 70/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.2990 - accuracy: 0.8723 - val_loss: 0.3131 - val_accuracy: 0.8877\n",
      "Epoch 71/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.2993 - accuracy: 0.8692 - val_loss: 0.3139 - val_accuracy: 0.8913\n",
      "Epoch 72/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.2989 - accuracy: 0.8738 - val_loss: 0.3133 - val_accuracy: 0.8877\n",
      "Epoch 73/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.2990 - accuracy: 0.8723 - val_loss: 0.3157 - val_accuracy: 0.8877\n",
      "Epoch 74/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.2987 - accuracy: 0.8707 - val_loss: 0.3154 - val_accuracy: 0.8877\n",
      "Epoch 75/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.2988 - accuracy: 0.8738 - val_loss: 0.3151 - val_accuracy: 0.8877\n",
      "Epoch 76/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.2982 - accuracy: 0.8692 - val_loss: 0.3133 - val_accuracy: 0.8877\n",
      "Epoch 77/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.2986 - accuracy: 0.8738 - val_loss: 0.3187 - val_accuracy: 0.8877\n",
      "Epoch 78/80\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.2984 - accuracy: 0.8707 - val_loss: 0.3155 - val_accuracy: 0.8913\n",
      "Epoch 79/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.2977 - accuracy: 0.8707 - val_loss: 0.3144 - val_accuracy: 0.8877\n",
      "Epoch 80/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.2968 - accuracy: 0.8738 - val_loss: 0.3161 - val_accuracy: 0.8841\n",
      "Epoch 1/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.2966 - accuracy: 0.8754 - val_loss: 0.3145 - val_accuracy: 0.8804\n",
      "Epoch 2/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.2950 - accuracy: 0.8769 - val_loss: 0.3150 - val_accuracy: 0.8804\n",
      "Epoch 3/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.2927 - accuracy: 0.8785 - val_loss: 0.3216 - val_accuracy: 0.8804\n",
      "Epoch 4/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.2926 - accuracy: 0.8738 - val_loss: 0.3163 - val_accuracy: 0.8768\n",
      "Epoch 5/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.2923 - accuracy: 0.8754 - val_loss: 0.3176 - val_accuracy: 0.8768\n",
      "Epoch 6/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.2909 - accuracy: 0.8738 - val_loss: 0.3181 - val_accuracy: 0.8732\n",
      "Epoch 7/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.2891 - accuracy: 0.8785 - val_loss: 0.3168 - val_accuracy: 0.8804\n",
      "Epoch 8/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.2881 - accuracy: 0.8785 - val_loss: 0.3227 - val_accuracy: 0.8768\n",
      "Epoch 9/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.2880 - accuracy: 0.8801 - val_loss: 0.3228 - val_accuracy: 0.8804\n",
      "Epoch 10/80\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.2889 - accuracy: 0.8769 - val_loss: 0.3191 - val_accuracy: 0.8732\n",
      "Epoch 11/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2878 - accuracy: 0.8769 - val_loss: 0.3199 - val_accuracy: 0.8768\n",
      "Epoch 12/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.2885 - accuracy: 0.8785 - val_loss: 0.3193 - val_accuracy: 0.8768\n",
      "Epoch 13/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2868 - accuracy: 0.8832 - val_loss: 0.3233 - val_accuracy: 0.8732\n",
      "Epoch 14/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2867 - accuracy: 0.8769 - val_loss: 0.3222 - val_accuracy: 0.8768\n",
      "Epoch 15/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.2870 - accuracy: 0.8769 - val_loss: 0.3204 - val_accuracy: 0.8732\n",
      "Epoch 16/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.2852 - accuracy: 0.8816 - val_loss: 0.3215 - val_accuracy: 0.8696\n",
      "Epoch 17/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2869 - accuracy: 0.8769 - val_loss: 0.3224 - val_accuracy: 0.8732\n",
      "Epoch 18/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.2854 - accuracy: 0.8769 - val_loss: 0.3227 - val_accuracy: 0.8732\n",
      "Epoch 19/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2848 - accuracy: 0.8785 - val_loss: 0.3238 - val_accuracy: 0.8732\n",
      "Epoch 20/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2848 - accuracy: 0.8816 - val_loss: 0.3218 - val_accuracy: 0.8659\n",
      "Epoch 21/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2842 - accuracy: 0.8832 - val_loss: 0.3225 - val_accuracy: 0.8732\n",
      "Epoch 22/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2831 - accuracy: 0.8832 - val_loss: 0.3216 - val_accuracy: 0.8623\n",
      "Epoch 23/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2840 - accuracy: 0.8816 - val_loss: 0.3250 - val_accuracy: 0.8804\n",
      "Epoch 24/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2840 - accuracy: 0.8832 - val_loss: 0.3248 - val_accuracy: 0.8804\n",
      "Epoch 25/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2814 - accuracy: 0.8879 - val_loss: 0.3311 - val_accuracy: 0.8768\n",
      "Epoch 26/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2826 - accuracy: 0.8785 - val_loss: 0.3331 - val_accuracy: 0.8804\n",
      "Epoch 27/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2838 - accuracy: 0.8879 - val_loss: 0.3253 - val_accuracy: 0.8804\n",
      "Epoch 28/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2821 - accuracy: 0.8816 - val_loss: 0.3257 - val_accuracy: 0.8768\n",
      "Epoch 29/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2820 - accuracy: 0.8863 - val_loss: 0.3251 - val_accuracy: 0.8768\n",
      "Epoch 30/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2813 - accuracy: 0.8847 - val_loss: 0.3252 - val_accuracy: 0.8732\n",
      "Epoch 31/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2813 - accuracy: 0.8879 - val_loss: 0.3287 - val_accuracy: 0.8768\n",
      "Epoch 32/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2813 - accuracy: 0.8879 - val_loss: 0.3235 - val_accuracy: 0.8659\n",
      "Epoch 33/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2798 - accuracy: 0.8832 - val_loss: 0.3248 - val_accuracy: 0.8696\n",
      "Epoch 34/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2799 - accuracy: 0.8879 - val_loss: 0.3284 - val_accuracy: 0.8768\n",
      "Epoch 35/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2785 - accuracy: 0.8910 - val_loss: 0.3238 - val_accuracy: 0.8696\n",
      "Epoch 36/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2799 - accuracy: 0.8863 - val_loss: 0.3247 - val_accuracy: 0.8732\n",
      "Epoch 37/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2784 - accuracy: 0.8894 - val_loss: 0.3252 - val_accuracy: 0.8732\n",
      "Epoch 38/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2784 - accuracy: 0.8879 - val_loss: 0.3237 - val_accuracy: 0.8623\n",
      "Epoch 39/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2791 - accuracy: 0.8863 - val_loss: 0.3254 - val_accuracy: 0.8696\n",
      "Epoch 40/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2791 - accuracy: 0.8925 - val_loss: 0.3243 - val_accuracy: 0.8659\n",
      "Epoch 41/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2775 - accuracy: 0.8879 - val_loss: 0.3254 - val_accuracy: 0.8696\n",
      "Epoch 42/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2779 - accuracy: 0.8910 - val_loss: 0.3265 - val_accuracy: 0.8768\n",
      "Epoch 43/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2772 - accuracy: 0.8847 - val_loss: 0.3247 - val_accuracy: 0.8732\n",
      "Epoch 44/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.2765 - accuracy: 0.8816 - val_loss: 0.3244 - val_accuracy: 0.8659\n",
      "Epoch 45/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2756 - accuracy: 0.8941 - val_loss: 0.3302 - val_accuracy: 0.8696\n",
      "Epoch 46/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.2768 - accuracy: 0.8894 - val_loss: 0.3268 - val_accuracy: 0.8732\n",
      "Epoch 47/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2773 - accuracy: 0.8863 - val_loss: 0.3240 - val_accuracy: 0.8732\n",
      "Epoch 48/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2743 - accuracy: 0.8925 - val_loss: 0.3228 - val_accuracy: 0.8623\n",
      "Epoch 49/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2759 - accuracy: 0.8925 - val_loss: 0.3277 - val_accuracy: 0.8732\n",
      "Epoch 50/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2748 - accuracy: 0.8910 - val_loss: 0.3293 - val_accuracy: 0.8659\n",
      "Epoch 51/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.2751 - accuracy: 0.8894 - val_loss: 0.3268 - val_accuracy: 0.8732\n",
      "Epoch 52/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2740 - accuracy: 0.8941 - val_loss: 0.3243 - val_accuracy: 0.8623\n",
      "Epoch 53/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2737 - accuracy: 0.8941 - val_loss: 0.3293 - val_accuracy: 0.8659\n",
      "Epoch 54/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2742 - accuracy: 0.8925 - val_loss: 0.3276 - val_accuracy: 0.8732\n",
      "Epoch 55/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2737 - accuracy: 0.8910 - val_loss: 0.3255 - val_accuracy: 0.8732\n",
      "Epoch 56/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2727 - accuracy: 0.8910 - val_loss: 0.3241 - val_accuracy: 0.8623\n",
      "Epoch 57/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2745 - accuracy: 0.8956 - val_loss: 0.3238 - val_accuracy: 0.8623\n",
      "Epoch 58/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2751 - accuracy: 0.8910 - val_loss: 0.3250 - val_accuracy: 0.8732\n",
      "Epoch 59/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2722 - accuracy: 0.8941 - val_loss: 0.3313 - val_accuracy: 0.8696\n",
      "Epoch 60/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.2739 - accuracy: 0.8879 - val_loss: 0.3248 - val_accuracy: 0.8659\n",
      "Epoch 61/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.2718 - accuracy: 0.8910 - val_loss: 0.3268 - val_accuracy: 0.8732\n",
      "Epoch 62/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.2718 - accuracy: 0.8956 - val_loss: 0.3253 - val_accuracy: 0.8696\n",
      "Epoch 63/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.2714 - accuracy: 0.8956 - val_loss: 0.3247 - val_accuracy: 0.8659\n",
      "Epoch 64/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.2717 - accuracy: 0.8925 - val_loss: 0.3240 - val_accuracy: 0.8659\n",
      "Epoch 65/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.2716 - accuracy: 0.8910 - val_loss: 0.3251 - val_accuracy: 0.8696\n",
      "Epoch 66/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.2702 - accuracy: 0.8847 - val_loss: 0.3288 - val_accuracy: 0.8732\n",
      "Epoch 67/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2704 - accuracy: 0.8925 - val_loss: 0.3257 - val_accuracy: 0.8696\n",
      "Epoch 68/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2715 - accuracy: 0.8910 - val_loss: 0.3261 - val_accuracy: 0.8696\n",
      "Epoch 69/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2703 - accuracy: 0.8910 - val_loss: 0.3292 - val_accuracy: 0.8732\n",
      "Epoch 70/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2703 - accuracy: 0.8941 - val_loss: 0.3263 - val_accuracy: 0.8696\n",
      "Epoch 71/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2697 - accuracy: 0.8925 - val_loss: 0.3274 - val_accuracy: 0.8732\n",
      "Epoch 72/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2696 - accuracy: 0.9003 - val_loss: 0.3266 - val_accuracy: 0.8659\n",
      "Epoch 73/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2692 - accuracy: 0.8925 - val_loss: 0.3278 - val_accuracy: 0.8696\n",
      "Epoch 74/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2682 - accuracy: 0.8941 - val_loss: 0.3270 - val_accuracy: 0.8623\n",
      "Epoch 75/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2678 - accuracy: 0.8941 - val_loss: 0.3249 - val_accuracy: 0.8623\n",
      "Epoch 76/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2696 - accuracy: 0.8941 - val_loss: 0.3378 - val_accuracy: 0.8623\n",
      "Epoch 77/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2691 - accuracy: 0.8925 - val_loss: 0.3304 - val_accuracy: 0.8659\n",
      "Epoch 78/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.2690 - accuracy: 0.8941 - val_loss: 0.3281 - val_accuracy: 0.8696\n",
      "Epoch 79/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2682 - accuracy: 0.8941 - val_loss: 0.3284 - val_accuracy: 0.8696\n",
      "Epoch 80/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2678 - accuracy: 0.8956 - val_loss: 0.3293 - val_accuracy: 0.8732\n"
     ]
    }
   ],
   "source": [
    "history0_temp=[]\n",
    "MLP0_F1_temp=[]\n",
    "MLP0_acc_temp=[]\n",
    "MLP0_acctr_temp=[]\n",
    "MLP0_recall_temp=[]\n",
    "\n",
    "for i in range(5):\n",
    "  if i==0:\n",
    "    seed=100\n",
    "  elif i==1:\n",
    "    seed=123\n",
    "  elif i==2:\n",
    "    seed=200\n",
    "  elif i==3:\n",
    "    seed=231\n",
    "  else:\n",
    "    seed=321\n",
    "\n",
    "  xtrain_1, xtest_1, ytrain_1, ytest_1 = model_selection.train_test_split(x, y, stratify = y, random_state = seed,train_size = 0.7, test_size = 0.3)\n",
    "\n",
    "  ytrain_1 = to_categorical(ytrain_1, num_classes = None)\n",
    "  ytest_1 = to_categorical(ytest_1, num_classes = None)\n",
    "\n",
    "  history=model_2.fit(xtrain_1, ytrain_1, validation_data=(xtest_1, ytest_1),epochs=80, batch_size=4)\n",
    "  history0_temp.append(history)\n",
    "\n",
    "  y_pred = model_2.predict(xtest_1,batch_size=None)\n",
    "  y_pred_train = model_2.predict(xtrain_1,batch_size=None)\n",
    "  ypredbool = np.argmax(y_pred, axis=1)\n",
    "  ypredbool = to_categorical(ypredbool, num_classes = None)\n",
    "  ypredbooltr = np.argmax(y_pred_train, axis=1)\n",
    "  ypredbooltr = to_categorical(ypredbooltr, num_classes = None)\n",
    "  MLP0_f1_score_1 = metrics.f1_score(ytest_1, ypredbool, average=None)\n",
    "  MLP0_acc_score_1=metrics.accuracy_score(ytest_1, ypredbool)\n",
    "  MLP0_acc_score_train_1=metrics.accuracy_score(ytrain_1, ypredbooltr)\n",
    "  MLP0_recall_score_1 = metrics.recall_score(ytest_1, ypredbool, average=None)\n",
    "\n",
    "  MLP0_F1_temp.append(MLP0_f1_score_1[1])\n",
    "  MLP0_acc_temp.append(MLP0_acc_score_1)\n",
    "  MLP0_acctr_temp.append(MLP0_acc_score_train_1)\n",
    "  MLP0_recall_temp.append(MLP0_recall_score_1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "OZl2LcRq8AxN",
    "outputId": "0e2786be-107b-44f9-f2f6-d2a8782b565e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8282828282828283, 0.8896103896103896, 0.8895899053627759, 0.8974358974358974, 0.8909657320872275]\n",
      "[0.8152173913043478, 0.8768115942028986, 0.8731884057971014, 0.8840579710144928, 0.8731884057971014]\n",
      "[0.8878504672897196, 0.8722741433021807, 0.881619937694704, 0.8738317757009346, 0.8940809968847352]\n",
      "[0.803921568627451, 0.8954248366013072, 0.9215686274509803, 0.9150326797385621, 0.934640522875817]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-726c6fdd-bedd-4d3d-b9b8-857e5d9a60f9\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Train accuracy</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>diferent of Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.815217</td>\n",
       "      <td>0.887850</td>\n",
       "      <td>0.072633</td>\n",
       "      <td>0.828283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123</td>\n",
       "      <td>0.895425</td>\n",
       "      <td>0.876812</td>\n",
       "      <td>0.872274</td>\n",
       "      <td>-0.004537</td>\n",
       "      <td>0.889610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.873188</td>\n",
       "      <td>0.881620</td>\n",
       "      <td>0.008432</td>\n",
       "      <td>0.889590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>231</td>\n",
       "      <td>0.915033</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.873832</td>\n",
       "      <td>-0.010226</td>\n",
       "      <td>0.897436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321</td>\n",
       "      <td>0.934641</td>\n",
       "      <td>0.873188</td>\n",
       "      <td>0.894081</td>\n",
       "      <td>0.020893</td>\n",
       "      <td>0.890966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-726c6fdd-bedd-4d3d-b9b8-857e5d9a60f9')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-726c6fdd-bedd-4d3d-b9b8-857e5d9a60f9 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-726c6fdd-bedd-4d3d-b9b8-857e5d9a60f9');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   seed    Recall  Train accuracy  Accuracy  diferent of Accuracy        F1\n",
       "0   100  0.803922        0.815217  0.887850              0.072633  0.828283\n",
       "1   123  0.895425        0.876812  0.872274             -0.004537  0.889610\n",
       "2   200  0.921569        0.873188  0.881620              0.008432  0.889590\n",
       "3   231  0.915033        0.884058  0.873832             -0.010226  0.897436\n",
       "4   321  0.934641        0.873188  0.894081              0.020893  0.890966"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(MLP0_F1_temp)\n",
    "print(MLP0_acc_temp)\n",
    "print(MLP0_acctr_temp)\n",
    "print(MLP0_recall_temp)\n",
    "\n",
    "delta1=MLP0_acctr_temp[0]-MLP0_acc_temp[0]\n",
    "delta2=MLP0_acctr_temp[1]-MLP0_acc_temp[1]\n",
    "delta3=MLP0_acctr_temp[2]-MLP0_acc_temp[2]\n",
    "delta4=MLP0_acctr_temp[3]-MLP0_acc_temp[3]\n",
    "delta5=MLP0_acctr_temp[4]-MLP0_acc_temp[4]\n",
    "MLP0_delta_temp=[delta1,delta2,delta3,delta4,delta5]\n",
    "\n",
    "MLP0=pd.DataFrame({'seed':[100,123,200,231,321],'Recall':[MLP0_recall_temp[0],MLP0_recall_temp[1],MLP0_recall_temp[2],MLP0_recall_temp[3],\n",
    "                                                     MLP0_recall_temp[4]],'Train accuracy':[MLP0_acc_temp[0],MLP0_acc_temp[1],MLP0_acc_temp[2],\n",
    "                                                    MLP0_acc_temp[3],MLP0_acc_temp[4]],'Accuracy':[MLP0_acctr_temp[0],MLP0_acctr_temp[1],MLP0_acctr_temp[2],\n",
    "                                                    MLP0_acctr_temp[3],MLP0_acctr_temp[4]],'diferent of Accuracy':[delta1,delta2,delta3,delta4,delta5],'F1':[MLP0_F1_temp[0],MLP0_F1_temp[1],MLP0_F1_temp[2],\n",
    "                                                    MLP0_F1_temp[3],MLP0_F1_temp[4]]})\n",
    "MLP0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnYOYGZEAc-8"
   },
   "source": [
    "#MLP with only drop out layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZEtjf5pcAcFa",
    "outputId": "2edcffb7-a1fb-41da-de0b-12e73924aca8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dropout_21 (Dropout)        (None, 12)                0         \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 6)                 78        \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 6)                 0         \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 10)                70        \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 8)                 88        \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 2)                 18        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 254\n",
      "Trainable params: 254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "re=regularizers.L2(0.1)\n",
    "def create_model_3():\n",
    "    model_3 = Sequential()\n",
    "    model_3.add(Dropout(0.1, input_dim = 12))\n",
    "    model_3.add(Dense(6, activation = 'relu'))\n",
    "    model_3.add(Dropout(0.1))\n",
    "    model_3.add(Dense(10, activation = 'relu'))\n",
    "    model_3.add(Dropout(0.1))\n",
    "    model_3.add(Dense(8, activation = 'relu'))\n",
    "    model_3.add(Dense(2, activation = 'sigmoid'))\n",
    "    model_3.compile(optimizer = Adam(learning_rate=0.00036), loss = 'binary_crossentropy', metrics = ['accuracy']) \n",
    "    return model_3\n",
    "\n",
    "model_3= create_model_3()\n",
    "\n",
    "print(model_3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "US2jAKxwBJEE",
    "outputId": "254e750f-0b08-4af7-a3b3-91bee26e23b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.6883 - accuracy: 0.5530 - val_loss: 0.6747 - val_accuracy: 0.5797\n",
      "Epoch 2/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6705 - accuracy: 0.5685 - val_loss: 0.6581 - val_accuracy: 0.5761\n",
      "Epoch 3/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6514 - accuracy: 0.5810 - val_loss: 0.6312 - val_accuracy: 0.5942\n",
      "Epoch 4/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6201 - accuracy: 0.6277 - val_loss: 0.5970 - val_accuracy: 0.7138\n",
      "Epoch 5/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6051 - accuracy: 0.7321 - val_loss: 0.5677 - val_accuracy: 0.7862\n",
      "Epoch 6/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5632 - accuracy: 0.7508 - val_loss: 0.5376 - val_accuracy: 0.8007\n",
      "Epoch 7/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5339 - accuracy: 0.7804 - val_loss: 0.5073 - val_accuracy: 0.8116\n",
      "Epoch 8/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5161 - accuracy: 0.7664 - val_loss: 0.4824 - val_accuracy: 0.8188\n",
      "Epoch 9/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4671 - accuracy: 0.8022 - val_loss: 0.4565 - val_accuracy: 0.8225\n",
      "Epoch 10/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4803 - accuracy: 0.8006 - val_loss: 0.4434 - val_accuracy: 0.8370\n",
      "Epoch 11/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4620 - accuracy: 0.7991 - val_loss: 0.4306 - val_accuracy: 0.8478\n",
      "Epoch 12/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4475 - accuracy: 0.7960 - val_loss: 0.4212 - val_accuracy: 0.8442\n",
      "Epoch 13/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4471 - accuracy: 0.8022 - val_loss: 0.4148 - val_accuracy: 0.8406\n",
      "Epoch 14/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4594 - accuracy: 0.7928 - val_loss: 0.4117 - val_accuracy: 0.8442\n",
      "Epoch 15/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4205 - accuracy: 0.8302 - val_loss: 0.4064 - val_accuracy: 0.8514\n",
      "Epoch 16/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4198 - accuracy: 0.8240 - val_loss: 0.4030 - val_accuracy: 0.8514\n",
      "Epoch 17/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4215 - accuracy: 0.8100 - val_loss: 0.4017 - val_accuracy: 0.8514\n",
      "Epoch 18/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3797 - accuracy: 0.8458 - val_loss: 0.4012 - val_accuracy: 0.8478\n",
      "Epoch 19/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4282 - accuracy: 0.8178 - val_loss: 0.4001 - val_accuracy: 0.8587\n",
      "Epoch 20/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4259 - accuracy: 0.8193 - val_loss: 0.4005 - val_accuracy: 0.8587\n",
      "Epoch 21/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4143 - accuracy: 0.8131 - val_loss: 0.3983 - val_accuracy: 0.8587\n",
      "Epoch 22/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4063 - accuracy: 0.8224 - val_loss: 0.3980 - val_accuracy: 0.8587\n",
      "Epoch 23/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4247 - accuracy: 0.8271 - val_loss: 0.3977 - val_accuracy: 0.8587\n",
      "Epoch 24/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3920 - accuracy: 0.8333 - val_loss: 0.3981 - val_accuracy: 0.8587\n",
      "Epoch 25/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4104 - accuracy: 0.8427 - val_loss: 0.3992 - val_accuracy: 0.8587\n",
      "Epoch 26/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3857 - accuracy: 0.8396 - val_loss: 0.3983 - val_accuracy: 0.8587\n",
      "Epoch 27/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3921 - accuracy: 0.8427 - val_loss: 0.3988 - val_accuracy: 0.8551\n",
      "Epoch 28/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3898 - accuracy: 0.8489 - val_loss: 0.3990 - val_accuracy: 0.8551\n",
      "Epoch 29/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4247 - accuracy: 0.8193 - val_loss: 0.4007 - val_accuracy: 0.8442\n",
      "Epoch 30/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3862 - accuracy: 0.8442 - val_loss: 0.4010 - val_accuracy: 0.8442\n",
      "Epoch 31/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3901 - accuracy: 0.8302 - val_loss: 0.4008 - val_accuracy: 0.8442\n",
      "Epoch 32/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3945 - accuracy: 0.8474 - val_loss: 0.4012 - val_accuracy: 0.8442\n",
      "Epoch 33/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3784 - accuracy: 0.8505 - val_loss: 0.4017 - val_accuracy: 0.8442\n",
      "Epoch 34/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3757 - accuracy: 0.8427 - val_loss: 0.4031 - val_accuracy: 0.8442\n",
      "Epoch 35/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3658 - accuracy: 0.8583 - val_loss: 0.4033 - val_accuracy: 0.8442\n",
      "Epoch 36/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4032 - accuracy: 0.8458 - val_loss: 0.4031 - val_accuracy: 0.8442\n",
      "Epoch 37/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4027 - accuracy: 0.8396 - val_loss: 0.4038 - val_accuracy: 0.8406\n",
      "Epoch 38/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3724 - accuracy: 0.8520 - val_loss: 0.4051 - val_accuracy: 0.8406\n",
      "Epoch 39/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3794 - accuracy: 0.8474 - val_loss: 0.4079 - val_accuracy: 0.8297\n",
      "Epoch 40/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3616 - accuracy: 0.8520 - val_loss: 0.4055 - val_accuracy: 0.8333\n",
      "Epoch 41/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3771 - accuracy: 0.8396 - val_loss: 0.4045 - val_accuracy: 0.8333\n",
      "Epoch 42/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3861 - accuracy: 0.8489 - val_loss: 0.4047 - val_accuracy: 0.8333\n",
      "Epoch 43/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3540 - accuracy: 0.8567 - val_loss: 0.4062 - val_accuracy: 0.8297\n",
      "Epoch 44/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3726 - accuracy: 0.8567 - val_loss: 0.4057 - val_accuracy: 0.8333\n",
      "Epoch 45/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3634 - accuracy: 0.8458 - val_loss: 0.4040 - val_accuracy: 0.8442\n",
      "Epoch 46/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3578 - accuracy: 0.8692 - val_loss: 0.4076 - val_accuracy: 0.8297\n",
      "Epoch 47/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3822 - accuracy: 0.8427 - val_loss: 0.4046 - val_accuracy: 0.8297\n",
      "Epoch 48/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3718 - accuracy: 0.8505 - val_loss: 0.4028 - val_accuracy: 0.8333\n",
      "Epoch 49/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3816 - accuracy: 0.8411 - val_loss: 0.4022 - val_accuracy: 0.8370\n",
      "Epoch 50/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3613 - accuracy: 0.8551 - val_loss: 0.4050 - val_accuracy: 0.8297\n",
      "Epoch 51/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3694 - accuracy: 0.8489 - val_loss: 0.4028 - val_accuracy: 0.8297\n",
      "Epoch 52/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3720 - accuracy: 0.8551 - val_loss: 0.4015 - val_accuracy: 0.8370\n",
      "Epoch 53/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3765 - accuracy: 0.8551 - val_loss: 0.3996 - val_accuracy: 0.8370\n",
      "Epoch 54/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3673 - accuracy: 0.8474 - val_loss: 0.3991 - val_accuracy: 0.8370\n",
      "Epoch 55/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3664 - accuracy: 0.8551 - val_loss: 0.3978 - val_accuracy: 0.8406\n",
      "Epoch 56/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3766 - accuracy: 0.8489 - val_loss: 0.3989 - val_accuracy: 0.8406\n",
      "Epoch 57/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3582 - accuracy: 0.8520 - val_loss: 0.3974 - val_accuracy: 0.8442\n",
      "Epoch 58/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3765 - accuracy: 0.8551 - val_loss: 0.3967 - val_accuracy: 0.8442\n",
      "Epoch 59/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3370 - accuracy: 0.8707 - val_loss: 0.3980 - val_accuracy: 0.8406\n",
      "Epoch 60/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3809 - accuracy: 0.8411 - val_loss: 0.3984 - val_accuracy: 0.8333\n",
      "Epoch 61/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3517 - accuracy: 0.8567 - val_loss: 0.3985 - val_accuracy: 0.8333\n",
      "Epoch 62/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3790 - accuracy: 0.8505 - val_loss: 0.3984 - val_accuracy: 0.8333\n",
      "Epoch 63/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3550 - accuracy: 0.8660 - val_loss: 0.3966 - val_accuracy: 0.8333\n",
      "Epoch 64/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3579 - accuracy: 0.8676 - val_loss: 0.3945 - val_accuracy: 0.8478\n",
      "Epoch 65/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3635 - accuracy: 0.8489 - val_loss: 0.3937 - val_accuracy: 0.8442\n",
      "Epoch 66/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3547 - accuracy: 0.8551 - val_loss: 0.3939 - val_accuracy: 0.8370\n",
      "Epoch 67/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3551 - accuracy: 0.8614 - val_loss: 0.3950 - val_accuracy: 0.8370\n",
      "Epoch 68/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3594 - accuracy: 0.8489 - val_loss: 0.3966 - val_accuracy: 0.8333\n",
      "Epoch 69/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3732 - accuracy: 0.8583 - val_loss: 0.3973 - val_accuracy: 0.8333\n",
      "Epoch 70/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3576 - accuracy: 0.8629 - val_loss: 0.3967 - val_accuracy: 0.8333\n",
      "Epoch 71/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3485 - accuracy: 0.8660 - val_loss: 0.3974 - val_accuracy: 0.8333\n",
      "Epoch 72/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3496 - accuracy: 0.8754 - val_loss: 0.3969 - val_accuracy: 0.8333\n",
      "Epoch 73/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3847 - accuracy: 0.8629 - val_loss: 0.3965 - val_accuracy: 0.8333\n",
      "Epoch 74/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3581 - accuracy: 0.8629 - val_loss: 0.3939 - val_accuracy: 0.8478\n",
      "Epoch 75/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3555 - accuracy: 0.8458 - val_loss: 0.3969 - val_accuracy: 0.8333\n",
      "Epoch 76/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3751 - accuracy: 0.8629 - val_loss: 0.3969 - val_accuracy: 0.8333\n",
      "Epoch 77/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3510 - accuracy: 0.8676 - val_loss: 0.3936 - val_accuracy: 0.8333\n",
      "Epoch 78/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3507 - accuracy: 0.8754 - val_loss: 0.3948 - val_accuracy: 0.8333\n",
      "Epoch 79/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3380 - accuracy: 0.8692 - val_loss: 0.3956 - val_accuracy: 0.8333\n",
      "Epoch 80/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3608 - accuracy: 0.8505 - val_loss: 0.3956 - val_accuracy: 0.8333\n",
      "Epoch 1/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4094 - accuracy: 0.8224 - val_loss: 0.3295 - val_accuracy: 0.8768\n",
      "Epoch 2/80\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.3794 - accuracy: 0.8551 - val_loss: 0.3300 - val_accuracy: 0.8804\n",
      "Epoch 3/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3844 - accuracy: 0.8505 - val_loss: 0.3291 - val_accuracy: 0.8804\n",
      "Epoch 4/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3754 - accuracy: 0.8536 - val_loss: 0.3285 - val_accuracy: 0.8804\n",
      "Epoch 5/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3656 - accuracy: 0.8567 - val_loss: 0.3282 - val_accuracy: 0.8804\n",
      "Epoch 6/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3889 - accuracy: 0.8396 - val_loss: 0.3287 - val_accuracy: 0.8804\n",
      "Epoch 7/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3917 - accuracy: 0.8442 - val_loss: 0.3289 - val_accuracy: 0.8804\n",
      "Epoch 8/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3755 - accuracy: 0.8520 - val_loss: 0.3291 - val_accuracy: 0.8804\n",
      "Epoch 9/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3705 - accuracy: 0.8411 - val_loss: 0.3292 - val_accuracy: 0.8804\n",
      "Epoch 10/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3834 - accuracy: 0.8411 - val_loss: 0.3303 - val_accuracy: 0.8804\n",
      "Epoch 11/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3682 - accuracy: 0.8629 - val_loss: 0.3290 - val_accuracy: 0.8804\n",
      "Epoch 12/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3842 - accuracy: 0.8567 - val_loss: 0.3308 - val_accuracy: 0.8804\n",
      "Epoch 13/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3838 - accuracy: 0.8505 - val_loss: 0.3309 - val_accuracy: 0.8804\n",
      "Epoch 14/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3806 - accuracy: 0.8520 - val_loss: 0.3297 - val_accuracy: 0.8804\n",
      "Epoch 15/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3666 - accuracy: 0.8629 - val_loss: 0.3277 - val_accuracy: 0.8804\n",
      "Epoch 16/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3775 - accuracy: 0.8474 - val_loss: 0.3283 - val_accuracy: 0.8804\n",
      "Epoch 17/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3722 - accuracy: 0.8458 - val_loss: 0.3279 - val_accuracy: 0.8804\n",
      "Epoch 18/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3722 - accuracy: 0.8458 - val_loss: 0.3285 - val_accuracy: 0.8804\n",
      "Epoch 19/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3727 - accuracy: 0.8567 - val_loss: 0.3277 - val_accuracy: 0.8804\n",
      "Epoch 20/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3801 - accuracy: 0.8551 - val_loss: 0.3274 - val_accuracy: 0.8804\n",
      "Epoch 21/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3828 - accuracy: 0.8411 - val_loss: 0.3271 - val_accuracy: 0.8804\n",
      "Epoch 22/80\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.3631 - accuracy: 0.8520 - val_loss: 0.3270 - val_accuracy: 0.8804\n",
      "Epoch 23/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3761 - accuracy: 0.8427 - val_loss: 0.3277 - val_accuracy: 0.8804\n",
      "Epoch 24/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3718 - accuracy: 0.8520 - val_loss: 0.3290 - val_accuracy: 0.8804\n",
      "Epoch 25/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3525 - accuracy: 0.8614 - val_loss: 0.3290 - val_accuracy: 0.8841\n",
      "Epoch 26/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3789 - accuracy: 0.8489 - val_loss: 0.3304 - val_accuracy: 0.8841\n",
      "Epoch 27/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3938 - accuracy: 0.8287 - val_loss: 0.3315 - val_accuracy: 0.8841\n",
      "Epoch 28/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3754 - accuracy: 0.8536 - val_loss: 0.3311 - val_accuracy: 0.8841\n",
      "Epoch 29/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3538 - accuracy: 0.8551 - val_loss: 0.3301 - val_accuracy: 0.8841\n",
      "Epoch 30/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3770 - accuracy: 0.8411 - val_loss: 0.3291 - val_accuracy: 0.8841\n",
      "Epoch 31/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3651 - accuracy: 0.8505 - val_loss: 0.3290 - val_accuracy: 0.8841\n",
      "Epoch 32/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3689 - accuracy: 0.8442 - val_loss: 0.3298 - val_accuracy: 0.8841\n",
      "Epoch 33/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3799 - accuracy: 0.8567 - val_loss: 0.3282 - val_accuracy: 0.8804\n",
      "Epoch 34/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3786 - accuracy: 0.8567 - val_loss: 0.3290 - val_accuracy: 0.8841\n",
      "Epoch 35/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3609 - accuracy: 0.8411 - val_loss: 0.3284 - val_accuracy: 0.8804\n",
      "Epoch 36/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3587 - accuracy: 0.8660 - val_loss: 0.3289 - val_accuracy: 0.8841\n",
      "Epoch 37/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3653 - accuracy: 0.8629 - val_loss: 0.3283 - val_accuracy: 0.8804\n",
      "Epoch 38/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3690 - accuracy: 0.8583 - val_loss: 0.3273 - val_accuracy: 0.8841\n",
      "Epoch 39/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3763 - accuracy: 0.8505 - val_loss: 0.3288 - val_accuracy: 0.8841\n",
      "Epoch 40/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3575 - accuracy: 0.8442 - val_loss: 0.3284 - val_accuracy: 0.8804\n",
      "Epoch 41/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3667 - accuracy: 0.8380 - val_loss: 0.3289 - val_accuracy: 0.8804\n",
      "Epoch 42/80\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.3637 - accuracy: 0.8567 - val_loss: 0.3285 - val_accuracy: 0.8804\n",
      "Epoch 43/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3811 - accuracy: 0.8411 - val_loss: 0.3309 - val_accuracy: 0.8804\n",
      "Epoch 44/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3575 - accuracy: 0.8536 - val_loss: 0.3298 - val_accuracy: 0.8841\n",
      "Epoch 45/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3672 - accuracy: 0.8551 - val_loss: 0.3295 - val_accuracy: 0.8841\n",
      "Epoch 46/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3769 - accuracy: 0.8396 - val_loss: 0.3308 - val_accuracy: 0.8804\n",
      "Epoch 47/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3724 - accuracy: 0.8505 - val_loss: 0.3311 - val_accuracy: 0.8804\n",
      "Epoch 48/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3714 - accuracy: 0.8536 - val_loss: 0.3313 - val_accuracy: 0.8804\n",
      "Epoch 49/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3717 - accuracy: 0.8411 - val_loss: 0.3302 - val_accuracy: 0.8804\n",
      "Epoch 50/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3596 - accuracy: 0.8567 - val_loss: 0.3286 - val_accuracy: 0.8841\n",
      "Epoch 51/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3773 - accuracy: 0.8427 - val_loss: 0.3284 - val_accuracy: 0.8841\n",
      "Epoch 52/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3658 - accuracy: 0.8567 - val_loss: 0.3296 - val_accuracy: 0.8841\n",
      "Epoch 53/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3790 - accuracy: 0.8442 - val_loss: 0.3288 - val_accuracy: 0.8841\n",
      "Epoch 54/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3659 - accuracy: 0.8489 - val_loss: 0.3294 - val_accuracy: 0.8804\n",
      "Epoch 55/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3625 - accuracy: 0.8458 - val_loss: 0.3275 - val_accuracy: 0.8841\n",
      "Epoch 56/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3400 - accuracy: 0.8707 - val_loss: 0.3260 - val_accuracy: 0.8841\n",
      "Epoch 57/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3650 - accuracy: 0.8458 - val_loss: 0.3275 - val_accuracy: 0.8841\n",
      "Epoch 58/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3650 - accuracy: 0.8567 - val_loss: 0.3286 - val_accuracy: 0.8841\n",
      "Epoch 59/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3697 - accuracy: 0.8520 - val_loss: 0.3288 - val_accuracy: 0.8841\n",
      "Epoch 60/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3651 - accuracy: 0.8567 - val_loss: 0.3286 - val_accuracy: 0.8841\n",
      "Epoch 61/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3756 - accuracy: 0.8427 - val_loss: 0.3292 - val_accuracy: 0.8804\n",
      "Epoch 62/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3578 - accuracy: 0.8536 - val_loss: 0.3291 - val_accuracy: 0.8804\n",
      "Epoch 63/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3511 - accuracy: 0.8567 - val_loss: 0.3271 - val_accuracy: 0.8804\n",
      "Epoch 64/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3618 - accuracy: 0.8536 - val_loss: 0.3267 - val_accuracy: 0.8804\n",
      "Epoch 65/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3925 - accuracy: 0.8489 - val_loss: 0.3297 - val_accuracy: 0.8804\n",
      "Epoch 66/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3592 - accuracy: 0.8551 - val_loss: 0.3283 - val_accuracy: 0.8804\n",
      "Epoch 67/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3637 - accuracy: 0.8551 - val_loss: 0.3277 - val_accuracy: 0.8804\n",
      "Epoch 68/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3648 - accuracy: 0.8474 - val_loss: 0.3273 - val_accuracy: 0.8804\n",
      "Epoch 69/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3684 - accuracy: 0.8505 - val_loss: 0.3297 - val_accuracy: 0.8804\n",
      "Epoch 70/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3429 - accuracy: 0.8723 - val_loss: 0.3282 - val_accuracy: 0.8804\n",
      "Epoch 71/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3802 - accuracy: 0.8536 - val_loss: 0.3305 - val_accuracy: 0.8804\n",
      "Epoch 72/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3471 - accuracy: 0.8583 - val_loss: 0.3296 - val_accuracy: 0.8804\n",
      "Epoch 73/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3667 - accuracy: 0.8598 - val_loss: 0.3293 - val_accuracy: 0.8804\n",
      "Epoch 74/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3797 - accuracy: 0.8427 - val_loss: 0.3302 - val_accuracy: 0.8804\n",
      "Epoch 75/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3545 - accuracy: 0.8629 - val_loss: 0.3289 - val_accuracy: 0.8804\n",
      "Epoch 76/80\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.3849 - accuracy: 0.8396 - val_loss: 0.3281 - val_accuracy: 0.8804\n",
      "Epoch 77/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3636 - accuracy: 0.8520 - val_loss: 0.3279 - val_accuracy: 0.8804\n",
      "Epoch 78/80\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.3772 - accuracy: 0.8505 - val_loss: 0.3295 - val_accuracy: 0.8804\n",
      "Epoch 79/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3803 - accuracy: 0.8458 - val_loss: 0.3305 - val_accuracy: 0.8804\n",
      "Epoch 80/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3940 - accuracy: 0.8209 - val_loss: 0.3327 - val_accuracy: 0.8804\n",
      "Epoch 1/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3827 - accuracy: 0.8520 - val_loss: 0.2889 - val_accuracy: 0.8949\n",
      "Epoch 2/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3881 - accuracy: 0.8458 - val_loss: 0.2905 - val_accuracy: 0.8877\n",
      "Epoch 3/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3790 - accuracy: 0.8396 - val_loss: 0.2917 - val_accuracy: 0.8913\n",
      "Epoch 4/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3752 - accuracy: 0.8505 - val_loss: 0.2911 - val_accuracy: 0.8877\n",
      "Epoch 5/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3707 - accuracy: 0.8583 - val_loss: 0.2896 - val_accuracy: 0.8877\n",
      "Epoch 6/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3808 - accuracy: 0.8318 - val_loss: 0.2922 - val_accuracy: 0.8949\n",
      "Epoch 7/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3668 - accuracy: 0.8598 - val_loss: 0.2933 - val_accuracy: 0.8949\n",
      "Epoch 8/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3626 - accuracy: 0.8614 - val_loss: 0.2906 - val_accuracy: 0.8949\n",
      "Epoch 9/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3815 - accuracy: 0.8505 - val_loss: 0.2925 - val_accuracy: 0.8949\n",
      "Epoch 10/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3729 - accuracy: 0.8442 - val_loss: 0.2907 - val_accuracy: 0.8913\n",
      "Epoch 11/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3747 - accuracy: 0.8474 - val_loss: 0.2909 - val_accuracy: 0.8913\n",
      "Epoch 12/80\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.3764 - accuracy: 0.8536 - val_loss: 0.2907 - val_accuracy: 0.8913\n",
      "Epoch 13/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3971 - accuracy: 0.8396 - val_loss: 0.2942 - val_accuracy: 0.8949\n",
      "Epoch 14/80\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.3661 - accuracy: 0.8458 - val_loss: 0.2923 - val_accuracy: 0.8913\n",
      "Epoch 15/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3789 - accuracy: 0.8349 - val_loss: 0.2927 - val_accuracy: 0.8913\n",
      "Epoch 16/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3753 - accuracy: 0.8474 - val_loss: 0.2962 - val_accuracy: 0.8877\n",
      "Epoch 17/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3733 - accuracy: 0.8520 - val_loss: 0.2931 - val_accuracy: 0.8913\n",
      "Epoch 18/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3695 - accuracy: 0.8645 - val_loss: 0.2922 - val_accuracy: 0.8877\n",
      "Epoch 19/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3647 - accuracy: 0.8629 - val_loss: 0.2919 - val_accuracy: 0.8877\n",
      "Epoch 20/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3793 - accuracy: 0.8442 - val_loss: 0.2928 - val_accuracy: 0.8877\n",
      "Epoch 21/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3818 - accuracy: 0.8505 - val_loss: 0.2937 - val_accuracy: 0.8877\n",
      "Epoch 22/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3672 - accuracy: 0.8474 - val_loss: 0.2954 - val_accuracy: 0.8877\n",
      "Epoch 23/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3687 - accuracy: 0.8614 - val_loss: 0.2928 - val_accuracy: 0.8877\n",
      "Epoch 24/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3630 - accuracy: 0.8505 - val_loss: 0.2905 - val_accuracy: 0.8877\n",
      "Epoch 25/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3847 - accuracy: 0.8411 - val_loss: 0.2934 - val_accuracy: 0.8877\n",
      "Epoch 26/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3708 - accuracy: 0.8427 - val_loss: 0.2916 - val_accuracy: 0.8877\n",
      "Epoch 27/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3802 - accuracy: 0.8380 - val_loss: 0.2926 - val_accuracy: 0.8877\n",
      "Epoch 28/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3762 - accuracy: 0.8567 - val_loss: 0.2957 - val_accuracy: 0.8877\n",
      "Epoch 29/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3722 - accuracy: 0.8551 - val_loss: 0.2957 - val_accuracy: 0.8877\n",
      "Epoch 30/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3797 - accuracy: 0.8442 - val_loss: 0.2960 - val_accuracy: 0.8877\n",
      "Epoch 31/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4012 - accuracy: 0.8318 - val_loss: 0.2990 - val_accuracy: 0.8841\n",
      "Epoch 32/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3931 - accuracy: 0.8302 - val_loss: 0.2986 - val_accuracy: 0.8877\n",
      "Epoch 33/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3639 - accuracy: 0.8567 - val_loss: 0.2959 - val_accuracy: 0.8877\n",
      "Epoch 34/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3651 - accuracy: 0.8427 - val_loss: 0.2959 - val_accuracy: 0.8877\n",
      "Epoch 35/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3744 - accuracy: 0.8598 - val_loss: 0.2975 - val_accuracy: 0.8877\n",
      "Epoch 36/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3816 - accuracy: 0.8442 - val_loss: 0.2969 - val_accuracy: 0.8841\n",
      "Epoch 37/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3653 - accuracy: 0.8551 - val_loss: 0.2963 - val_accuracy: 0.8841\n",
      "Epoch 38/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3705 - accuracy: 0.8442 - val_loss: 0.2957 - val_accuracy: 0.8913\n",
      "Epoch 39/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3701 - accuracy: 0.8442 - val_loss: 0.2949 - val_accuracy: 0.8949\n",
      "Epoch 40/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3850 - accuracy: 0.8520 - val_loss: 0.2967 - val_accuracy: 0.8877\n",
      "Epoch 41/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3878 - accuracy: 0.8427 - val_loss: 0.3001 - val_accuracy: 0.8877\n",
      "Epoch 42/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3790 - accuracy: 0.8489 - val_loss: 0.2997 - val_accuracy: 0.8841\n",
      "Epoch 43/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3752 - accuracy: 0.8567 - val_loss: 0.2980 - val_accuracy: 0.8877\n",
      "Epoch 44/80\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.3546 - accuracy: 0.8551 - val_loss: 0.2938 - val_accuracy: 0.8841\n",
      "Epoch 45/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3726 - accuracy: 0.8567 - val_loss: 0.2930 - val_accuracy: 0.8841\n",
      "Epoch 46/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3866 - accuracy: 0.8505 - val_loss: 0.2958 - val_accuracy: 0.8841\n",
      "Epoch 47/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3638 - accuracy: 0.8583 - val_loss: 0.2966 - val_accuracy: 0.8841\n",
      "Epoch 48/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3726 - accuracy: 0.8458 - val_loss: 0.2960 - val_accuracy: 0.8804\n",
      "Epoch 49/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3674 - accuracy: 0.8551 - val_loss: 0.2952 - val_accuracy: 0.8913\n",
      "Epoch 50/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3824 - accuracy: 0.8442 - val_loss: 0.2973 - val_accuracy: 0.8804\n",
      "Epoch 51/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3707 - accuracy: 0.8567 - val_loss: 0.2980 - val_accuracy: 0.8804\n",
      "Epoch 52/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3624 - accuracy: 0.8489 - val_loss: 0.2967 - val_accuracy: 0.8841\n",
      "Epoch 53/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3729 - accuracy: 0.8474 - val_loss: 0.2949 - val_accuracy: 0.8804\n",
      "Epoch 54/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3765 - accuracy: 0.8458 - val_loss: 0.2961 - val_accuracy: 0.8841\n",
      "Epoch 55/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3767 - accuracy: 0.8474 - val_loss: 0.2963 - val_accuracy: 0.8841\n",
      "Epoch 56/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3720 - accuracy: 0.8489 - val_loss: 0.2950 - val_accuracy: 0.8841\n",
      "Epoch 57/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3511 - accuracy: 0.8536 - val_loss: 0.2939 - val_accuracy: 0.8841\n",
      "Epoch 58/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3947 - accuracy: 0.8442 - val_loss: 0.2984 - val_accuracy: 0.8841\n",
      "Epoch 59/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3832 - accuracy: 0.8567 - val_loss: 0.2979 - val_accuracy: 0.8841\n",
      "Epoch 60/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3885 - accuracy: 0.8520 - val_loss: 0.2986 - val_accuracy: 0.8841\n",
      "Epoch 61/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3640 - accuracy: 0.8520 - val_loss: 0.2982 - val_accuracy: 0.8841\n",
      "Epoch 62/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3764 - accuracy: 0.8396 - val_loss: 0.2986 - val_accuracy: 0.8841\n",
      "Epoch 63/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3691 - accuracy: 0.8520 - val_loss: 0.2987 - val_accuracy: 0.8841\n",
      "Epoch 64/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3741 - accuracy: 0.8458 - val_loss: 0.2971 - val_accuracy: 0.8841\n",
      "Epoch 65/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3667 - accuracy: 0.8551 - val_loss: 0.2962 - val_accuracy: 0.8841\n",
      "Epoch 66/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3685 - accuracy: 0.8660 - val_loss: 0.2969 - val_accuracy: 0.8841\n",
      "Epoch 67/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3683 - accuracy: 0.8489 - val_loss: 0.2947 - val_accuracy: 0.8841\n",
      "Epoch 68/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3713 - accuracy: 0.8442 - val_loss: 0.2951 - val_accuracy: 0.8841\n",
      "Epoch 69/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3618 - accuracy: 0.8489 - val_loss: 0.2945 - val_accuracy: 0.8841\n",
      "Epoch 70/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3793 - accuracy: 0.8458 - val_loss: 0.2979 - val_accuracy: 0.8804\n",
      "Epoch 71/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3661 - accuracy: 0.8551 - val_loss: 0.2960 - val_accuracy: 0.8841\n",
      "Epoch 72/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3659 - accuracy: 0.8520 - val_loss: 0.2944 - val_accuracy: 0.8841\n",
      "Epoch 73/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3709 - accuracy: 0.8536 - val_loss: 0.2950 - val_accuracy: 0.8841\n",
      "Epoch 74/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3622 - accuracy: 0.8505 - val_loss: 0.2932 - val_accuracy: 0.8841\n",
      "Epoch 75/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3856 - accuracy: 0.8489 - val_loss: 0.2978 - val_accuracy: 0.8841\n",
      "Epoch 76/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3776 - accuracy: 0.8489 - val_loss: 0.2972 - val_accuracy: 0.8949\n",
      "Epoch 77/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3803 - accuracy: 0.8520 - val_loss: 0.3001 - val_accuracy: 0.8841\n",
      "Epoch 78/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3734 - accuracy: 0.8505 - val_loss: 0.2994 - val_accuracy: 0.8877\n",
      "Epoch 79/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3766 - accuracy: 0.8411 - val_loss: 0.2997 - val_accuracy: 0.8913\n",
      "Epoch 80/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3503 - accuracy: 0.8614 - val_loss: 0.2981 - val_accuracy: 0.8877\n",
      "Epoch 1/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3800 - accuracy: 0.8411 - val_loss: 0.3009 - val_accuracy: 0.8986\n",
      "Epoch 2/80\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.3787 - accuracy: 0.8333 - val_loss: 0.3026 - val_accuracy: 0.9022\n",
      "Epoch 3/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3737 - accuracy: 0.8505 - val_loss: 0.3031 - val_accuracy: 0.8986\n",
      "Epoch 4/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3708 - accuracy: 0.8489 - val_loss: 0.3027 - val_accuracy: 0.8986\n",
      "Epoch 5/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3772 - accuracy: 0.8505 - val_loss: 0.3039 - val_accuracy: 0.8949\n",
      "Epoch 6/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3680 - accuracy: 0.8411 - val_loss: 0.3015 - val_accuracy: 0.8986\n",
      "Epoch 7/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3715 - accuracy: 0.8489 - val_loss: 0.3008 - val_accuracy: 0.8986\n",
      "Epoch 8/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3578 - accuracy: 0.8567 - val_loss: 0.3012 - val_accuracy: 0.8986\n",
      "Epoch 9/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3629 - accuracy: 0.8489 - val_loss: 0.2992 - val_accuracy: 0.8986\n",
      "Epoch 10/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3840 - accuracy: 0.8349 - val_loss: 0.3033 - val_accuracy: 0.8949\n",
      "Epoch 11/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3763 - accuracy: 0.8380 - val_loss: 0.3025 - val_accuracy: 0.8986\n",
      "Epoch 12/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3617 - accuracy: 0.8474 - val_loss: 0.3029 - val_accuracy: 0.8913\n",
      "Epoch 13/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3718 - accuracy: 0.8442 - val_loss: 0.3050 - val_accuracy: 0.8949\n",
      "Epoch 14/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3684 - accuracy: 0.8380 - val_loss: 0.3071 - val_accuracy: 0.8986\n",
      "Epoch 15/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3605 - accuracy: 0.8442 - val_loss: 0.3067 - val_accuracy: 0.8949\n",
      "Epoch 16/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3675 - accuracy: 0.8505 - val_loss: 0.3041 - val_accuracy: 0.8949\n",
      "Epoch 17/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3892 - accuracy: 0.8396 - val_loss: 0.3098 - val_accuracy: 0.8913\n",
      "Epoch 18/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3712 - accuracy: 0.8442 - val_loss: 0.3079 - val_accuracy: 0.8949\n",
      "Epoch 19/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3735 - accuracy: 0.8536 - val_loss: 0.3084 - val_accuracy: 0.8986\n",
      "Epoch 20/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3565 - accuracy: 0.8458 - val_loss: 0.3048 - val_accuracy: 0.8986\n",
      "Epoch 21/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3523 - accuracy: 0.8614 - val_loss: 0.3014 - val_accuracy: 0.8986\n",
      "Epoch 22/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3618 - accuracy: 0.8520 - val_loss: 0.3039 - val_accuracy: 0.8986\n",
      "Epoch 23/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3774 - accuracy: 0.8458 - val_loss: 0.3059 - val_accuracy: 0.8986\n",
      "Epoch 24/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3719 - accuracy: 0.8302 - val_loss: 0.3064 - val_accuracy: 0.8986\n",
      "Epoch 25/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3538 - accuracy: 0.8583 - val_loss: 0.3026 - val_accuracy: 0.8986\n",
      "Epoch 26/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3578 - accuracy: 0.8458 - val_loss: 0.3019 - val_accuracy: 0.8986\n",
      "Epoch 27/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3552 - accuracy: 0.8520 - val_loss: 0.3016 - val_accuracy: 0.8986\n",
      "Epoch 28/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3669 - accuracy: 0.8598 - val_loss: 0.3021 - val_accuracy: 0.8986\n",
      "Epoch 29/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3608 - accuracy: 0.8583 - val_loss: 0.3031 - val_accuracy: 0.8986\n",
      "Epoch 30/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3789 - accuracy: 0.8364 - val_loss: 0.3037 - val_accuracy: 0.8986\n",
      "Epoch 31/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3710 - accuracy: 0.8489 - val_loss: 0.3068 - val_accuracy: 0.8949\n",
      "Epoch 32/80\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.3752 - accuracy: 0.8427 - val_loss: 0.3081 - val_accuracy: 0.8949\n",
      "Epoch 33/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3536 - accuracy: 0.8520 - val_loss: 0.3035 - val_accuracy: 0.8986\n",
      "Epoch 34/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3527 - accuracy: 0.8489 - val_loss: 0.3030 - val_accuracy: 0.8986\n",
      "Epoch 35/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3643 - accuracy: 0.8505 - val_loss: 0.3036 - val_accuracy: 0.8986\n",
      "Epoch 36/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3787 - accuracy: 0.8411 - val_loss: 0.3066 - val_accuracy: 0.8949\n",
      "Epoch 37/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3843 - accuracy: 0.8396 - val_loss: 0.3074 - val_accuracy: 0.8949\n",
      "Epoch 38/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3628 - accuracy: 0.8536 - val_loss: 0.3053 - val_accuracy: 0.8986\n",
      "Epoch 39/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3567 - accuracy: 0.8520 - val_loss: 0.3050 - val_accuracy: 0.8949\n",
      "Epoch 40/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3689 - accuracy: 0.8536 - val_loss: 0.3064 - val_accuracy: 0.8949\n",
      "Epoch 41/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3701 - accuracy: 0.8442 - val_loss: 0.3087 - val_accuracy: 0.8913\n",
      "Epoch 42/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3816 - accuracy: 0.8349 - val_loss: 0.3103 - val_accuracy: 0.8949\n",
      "Epoch 43/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3726 - accuracy: 0.8489 - val_loss: 0.3087 - val_accuracy: 0.8949\n",
      "Epoch 44/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3617 - accuracy: 0.8474 - val_loss: 0.3091 - val_accuracy: 0.8949\n",
      "Epoch 45/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3655 - accuracy: 0.8551 - val_loss: 0.3082 - val_accuracy: 0.8913\n",
      "Epoch 46/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3736 - accuracy: 0.8396 - val_loss: 0.3086 - val_accuracy: 0.8913\n",
      "Epoch 47/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3639 - accuracy: 0.8427 - val_loss: 0.3109 - val_accuracy: 0.8877\n",
      "Epoch 48/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3614 - accuracy: 0.8583 - val_loss: 0.3074 - val_accuracy: 0.8913\n",
      "Epoch 49/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3685 - accuracy: 0.8598 - val_loss: 0.3054 - val_accuracy: 0.8949\n",
      "Epoch 50/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3734 - accuracy: 0.8458 - val_loss: 0.3057 - val_accuracy: 0.8949\n",
      "Epoch 51/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3754 - accuracy: 0.8364 - val_loss: 0.3080 - val_accuracy: 0.8949\n",
      "Epoch 52/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3799 - accuracy: 0.8396 - val_loss: 0.3115 - val_accuracy: 0.8913\n",
      "Epoch 53/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3789 - accuracy: 0.8505 - val_loss: 0.3107 - val_accuracy: 0.8913\n",
      "Epoch 54/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3684 - accuracy: 0.8442 - val_loss: 0.3116 - val_accuracy: 0.8949\n",
      "Epoch 55/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3689 - accuracy: 0.8505 - val_loss: 0.3103 - val_accuracy: 0.8913\n",
      "Epoch 56/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3763 - accuracy: 0.8396 - val_loss: 0.3093 - val_accuracy: 0.8913\n",
      "Epoch 57/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3741 - accuracy: 0.8536 - val_loss: 0.3070 - val_accuracy: 0.8913\n",
      "Epoch 58/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3568 - accuracy: 0.8442 - val_loss: 0.3077 - val_accuracy: 0.8986\n",
      "Epoch 59/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3528 - accuracy: 0.8536 - val_loss: 0.3054 - val_accuracy: 0.8913\n",
      "Epoch 60/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3722 - accuracy: 0.8442 - val_loss: 0.3072 - val_accuracy: 0.8913\n",
      "Epoch 61/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3600 - accuracy: 0.8567 - val_loss: 0.3068 - val_accuracy: 0.8877\n",
      "Epoch 62/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3611 - accuracy: 0.8458 - val_loss: 0.3109 - val_accuracy: 0.8986\n",
      "Epoch 63/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3524 - accuracy: 0.8505 - val_loss: 0.3088 - val_accuracy: 0.8986\n",
      "Epoch 64/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3607 - accuracy: 0.8614 - val_loss: 0.3057 - val_accuracy: 0.8949\n",
      "Epoch 65/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3700 - accuracy: 0.8380 - val_loss: 0.3074 - val_accuracy: 0.8949\n",
      "Epoch 66/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3551 - accuracy: 0.8458 - val_loss: 0.3074 - val_accuracy: 0.8949\n",
      "Epoch 67/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3544 - accuracy: 0.8614 - val_loss: 0.3069 - val_accuracy: 0.8949\n",
      "Epoch 68/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3632 - accuracy: 0.8520 - val_loss: 0.3082 - val_accuracy: 0.8949\n",
      "Epoch 69/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3550 - accuracy: 0.8520 - val_loss: 0.3062 - val_accuracy: 0.8949\n",
      "Epoch 70/80\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.3556 - accuracy: 0.8474 - val_loss: 0.3085 - val_accuracy: 0.8913\n",
      "Epoch 71/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3612 - accuracy: 0.8520 - val_loss: 0.3072 - val_accuracy: 0.8913\n",
      "Epoch 72/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3541 - accuracy: 0.8411 - val_loss: 0.3069 - val_accuracy: 0.8877\n",
      "Epoch 73/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3715 - accuracy: 0.8396 - val_loss: 0.3091 - val_accuracy: 0.8913\n",
      "Epoch 74/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3634 - accuracy: 0.8505 - val_loss: 0.3076 - val_accuracy: 0.8913\n",
      "Epoch 75/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3655 - accuracy: 0.8442 - val_loss: 0.3089 - val_accuracy: 0.8949\n",
      "Epoch 76/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3790 - accuracy: 0.8458 - val_loss: 0.3103 - val_accuracy: 0.8949\n",
      "Epoch 77/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3704 - accuracy: 0.8489 - val_loss: 0.3084 - val_accuracy: 0.8949\n",
      "Epoch 78/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3522 - accuracy: 0.8474 - val_loss: 0.3057 - val_accuracy: 0.8949\n",
      "Epoch 79/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3590 - accuracy: 0.8458 - val_loss: 0.3072 - val_accuracy: 0.8949\n",
      "Epoch 80/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3522 - accuracy: 0.8520 - val_loss: 0.3058 - val_accuracy: 0.8913\n",
      "Epoch 1/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3285 - accuracy: 0.8707 - val_loss: 0.3209 - val_accuracy: 0.8659\n",
      "Epoch 2/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3505 - accuracy: 0.8583 - val_loss: 0.3222 - val_accuracy: 0.8659\n",
      "Epoch 3/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3324 - accuracy: 0.8723 - val_loss: 0.3212 - val_accuracy: 0.8659\n",
      "Epoch 4/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3460 - accuracy: 0.8551 - val_loss: 0.3219 - val_accuracy: 0.8659\n",
      "Epoch 5/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3361 - accuracy: 0.8660 - val_loss: 0.3208 - val_accuracy: 0.8659\n",
      "Epoch 6/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3505 - accuracy: 0.8536 - val_loss: 0.3215 - val_accuracy: 0.8623\n",
      "Epoch 7/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3397 - accuracy: 0.8738 - val_loss: 0.3198 - val_accuracy: 0.8623\n",
      "Epoch 8/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3614 - accuracy: 0.8520 - val_loss: 0.3236 - val_accuracy: 0.8659\n",
      "Epoch 9/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3395 - accuracy: 0.8520 - val_loss: 0.3227 - val_accuracy: 0.8659\n",
      "Epoch 10/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3479 - accuracy: 0.8629 - val_loss: 0.3249 - val_accuracy: 0.8696\n",
      "Epoch 11/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3320 - accuracy: 0.8692 - val_loss: 0.3248 - val_accuracy: 0.8659\n",
      "Epoch 12/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3408 - accuracy: 0.8614 - val_loss: 0.3230 - val_accuracy: 0.8623\n",
      "Epoch 13/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3351 - accuracy: 0.8785 - val_loss: 0.3238 - val_accuracy: 0.8623\n",
      "Epoch 14/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3488 - accuracy: 0.8692 - val_loss: 0.3262 - val_accuracy: 0.8659\n",
      "Epoch 15/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3367 - accuracy: 0.8692 - val_loss: 0.3253 - val_accuracy: 0.8659\n",
      "Epoch 16/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3507 - accuracy: 0.8629 - val_loss: 0.3261 - val_accuracy: 0.8696\n",
      "Epoch 17/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3480 - accuracy: 0.8660 - val_loss: 0.3268 - val_accuracy: 0.8659\n",
      "Epoch 18/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3538 - accuracy: 0.8551 - val_loss: 0.3255 - val_accuracy: 0.8696\n",
      "Epoch 19/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3211 - accuracy: 0.8863 - val_loss: 0.3251 - val_accuracy: 0.8659\n",
      "Epoch 20/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3482 - accuracy: 0.8660 - val_loss: 0.3264 - val_accuracy: 0.8659\n",
      "Epoch 21/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3276 - accuracy: 0.8692 - val_loss: 0.3269 - val_accuracy: 0.8659\n",
      "Epoch 22/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3320 - accuracy: 0.8614 - val_loss: 0.3251 - val_accuracy: 0.8659\n",
      "Epoch 23/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3491 - accuracy: 0.8567 - val_loss: 0.3257 - val_accuracy: 0.8659\n",
      "Epoch 24/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3443 - accuracy: 0.8645 - val_loss: 0.3260 - val_accuracy: 0.8659\n",
      "Epoch 25/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3653 - accuracy: 0.8583 - val_loss: 0.3281 - val_accuracy: 0.8659\n",
      "Epoch 26/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3360 - accuracy: 0.8707 - val_loss: 0.3263 - val_accuracy: 0.8659\n",
      "Epoch 27/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3454 - accuracy: 0.8598 - val_loss: 0.3251 - val_accuracy: 0.8659\n",
      "Epoch 28/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3467 - accuracy: 0.8629 - val_loss: 0.3261 - val_accuracy: 0.8659\n",
      "Epoch 29/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3308 - accuracy: 0.8692 - val_loss: 0.3255 - val_accuracy: 0.8659\n",
      "Epoch 30/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3535 - accuracy: 0.8520 - val_loss: 0.3263 - val_accuracy: 0.8696\n",
      "Epoch 31/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3681 - accuracy: 0.8551 - val_loss: 0.3279 - val_accuracy: 0.8659\n",
      "Epoch 32/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3457 - accuracy: 0.8692 - val_loss: 0.3270 - val_accuracy: 0.8623\n",
      "Epoch 33/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3467 - accuracy: 0.8645 - val_loss: 0.3257 - val_accuracy: 0.8659\n",
      "Epoch 34/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3493 - accuracy: 0.8660 - val_loss: 0.3292 - val_accuracy: 0.8659\n",
      "Epoch 35/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3448 - accuracy: 0.8754 - val_loss: 0.3273 - val_accuracy: 0.8659\n",
      "Epoch 36/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3348 - accuracy: 0.8723 - val_loss: 0.3255 - val_accuracy: 0.8623\n",
      "Epoch 37/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3375 - accuracy: 0.8614 - val_loss: 0.3254 - val_accuracy: 0.8623\n",
      "Epoch 38/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3499 - accuracy: 0.8645 - val_loss: 0.3258 - val_accuracy: 0.8659\n",
      "Epoch 39/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3563 - accuracy: 0.8520 - val_loss: 0.3264 - val_accuracy: 0.8696\n",
      "Epoch 40/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3342 - accuracy: 0.8723 - val_loss: 0.3265 - val_accuracy: 0.8659\n",
      "Epoch 41/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3396 - accuracy: 0.8660 - val_loss: 0.3238 - val_accuracy: 0.8623\n",
      "Epoch 42/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3448 - accuracy: 0.8676 - val_loss: 0.3247 - val_accuracy: 0.8623\n",
      "Epoch 43/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3263 - accuracy: 0.8676 - val_loss: 0.3235 - val_accuracy: 0.8659\n",
      "Epoch 44/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3400 - accuracy: 0.8660 - val_loss: 0.3259 - val_accuracy: 0.8659\n",
      "Epoch 45/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3592 - accuracy: 0.8520 - val_loss: 0.3257 - val_accuracy: 0.8659\n",
      "Epoch 46/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3455 - accuracy: 0.8614 - val_loss: 0.3245 - val_accuracy: 0.8659\n",
      "Epoch 47/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3542 - accuracy: 0.8489 - val_loss: 0.3272 - val_accuracy: 0.8623\n",
      "Epoch 48/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3505 - accuracy: 0.8614 - val_loss: 0.3282 - val_accuracy: 0.8623\n",
      "Epoch 49/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3508 - accuracy: 0.8551 - val_loss: 0.3293 - val_accuracy: 0.8623\n",
      "Epoch 50/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3208 - accuracy: 0.8785 - val_loss: 0.3253 - val_accuracy: 0.8587\n",
      "Epoch 51/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3415 - accuracy: 0.8707 - val_loss: 0.3265 - val_accuracy: 0.8623\n",
      "Epoch 52/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3286 - accuracy: 0.8723 - val_loss: 0.3247 - val_accuracy: 0.8587\n",
      "Epoch 53/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3445 - accuracy: 0.8692 - val_loss: 0.3266 - val_accuracy: 0.8623\n",
      "Epoch 54/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3473 - accuracy: 0.8551 - val_loss: 0.3268 - val_accuracy: 0.8623\n",
      "Epoch 55/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3447 - accuracy: 0.8520 - val_loss: 0.3279 - val_accuracy: 0.8623\n",
      "Epoch 56/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3346 - accuracy: 0.8629 - val_loss: 0.3282 - val_accuracy: 0.8623\n",
      "Epoch 57/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3338 - accuracy: 0.8660 - val_loss: 0.3281 - val_accuracy: 0.8623\n",
      "Epoch 58/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3382 - accuracy: 0.8645 - val_loss: 0.3293 - val_accuracy: 0.8623\n",
      "Epoch 59/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3366 - accuracy: 0.8754 - val_loss: 0.3258 - val_accuracy: 0.8587\n",
      "Epoch 60/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3518 - accuracy: 0.8676 - val_loss: 0.3263 - val_accuracy: 0.8587\n",
      "Epoch 61/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3425 - accuracy: 0.8629 - val_loss: 0.3283 - val_accuracy: 0.8623\n",
      "Epoch 62/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3397 - accuracy: 0.8676 - val_loss: 0.3295 - val_accuracy: 0.8623\n",
      "Epoch 63/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3471 - accuracy: 0.8551 - val_loss: 0.3294 - val_accuracy: 0.8623\n",
      "Epoch 64/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3548 - accuracy: 0.8551 - val_loss: 0.3298 - val_accuracy: 0.8623\n",
      "Epoch 65/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3433 - accuracy: 0.8505 - val_loss: 0.3306 - val_accuracy: 0.8587\n",
      "Epoch 66/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3373 - accuracy: 0.8723 - val_loss: 0.3315 - val_accuracy: 0.8551\n",
      "Epoch 67/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3513 - accuracy: 0.8692 - val_loss: 0.3329 - val_accuracy: 0.8587\n",
      "Epoch 68/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3391 - accuracy: 0.8645 - val_loss: 0.3306 - val_accuracy: 0.8623\n",
      "Epoch 69/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3472 - accuracy: 0.8676 - val_loss: 0.3310 - val_accuracy: 0.8551\n",
      "Epoch 70/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3375 - accuracy: 0.8629 - val_loss: 0.3320 - val_accuracy: 0.8587\n",
      "Epoch 71/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3405 - accuracy: 0.8723 - val_loss: 0.3312 - val_accuracy: 0.8587\n",
      "Epoch 72/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3597 - accuracy: 0.8567 - val_loss: 0.3316 - val_accuracy: 0.8623\n",
      "Epoch 73/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3542 - accuracy: 0.8629 - val_loss: 0.3296 - val_accuracy: 0.8623\n",
      "Epoch 74/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3285 - accuracy: 0.8660 - val_loss: 0.3320 - val_accuracy: 0.8623\n",
      "Epoch 75/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3346 - accuracy: 0.8707 - val_loss: 0.3302 - val_accuracy: 0.8587\n",
      "Epoch 76/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3443 - accuracy: 0.8505 - val_loss: 0.3323 - val_accuracy: 0.8623\n",
      "Epoch 77/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3252 - accuracy: 0.8738 - val_loss: 0.3293 - val_accuracy: 0.8659\n",
      "Epoch 78/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3317 - accuracy: 0.8754 - val_loss: 0.3290 - val_accuracy: 0.8623\n",
      "Epoch 79/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3226 - accuracy: 0.8676 - val_loss: 0.3292 - val_accuracy: 0.8623\n",
      "Epoch 80/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3480 - accuracy: 0.8536 - val_loss: 0.3298 - val_accuracy: 0.8623\n"
     ]
    }
   ],
   "source": [
    "historyd_temp=[]\n",
    "MLPd_F1_temp=[]\n",
    "MLPd_acc_temp=[]\n",
    "MLPd_acctr_temp=[]\n",
    "MLPd_recall_temp=[]\n",
    "\n",
    "for i in range(5):\n",
    "  if i==0:\n",
    "    seed=100\n",
    "  elif i==1:\n",
    "    seed=123\n",
    "  elif i==2:\n",
    "    seed=200\n",
    "  elif i==3:\n",
    "    seed=231\n",
    "  else:\n",
    "    seed=321\n",
    "\n",
    "  xtrain_1, xtest_1, ytrain_1, ytest_1 = model_selection.train_test_split(x, y, stratify = y, random_state = seed,train_size = 0.7, test_size = 0.3)\n",
    "\n",
    "  ytrain_1 = to_categorical(ytrain_1, num_classes = None)\n",
    "  ytest_1 = to_categorical(ytest_1, num_classes = None)\n",
    "\n",
    "  history=model_3.fit(xtrain_1, ytrain_1, validation_data=(xtest_1, ytest_1),epochs=80, batch_size=4)\n",
    "  historyd_temp.append(history)\n",
    "\n",
    "  y_pred = model_3.predict(xtest_1,batch_size=None)\n",
    "  y_pred_train = model_3.predict(xtrain_1,batch_size=None)\n",
    "  ypredbool = np.argmax(y_pred, axis=1)\n",
    "  ypredbool = to_categorical(ypredbool, num_classes = None)\n",
    "  ypredbooltr = np.argmax(y_pred_train, axis=1)\n",
    "  ypredbooltr = to_categorical(ypredbooltr, num_classes = None)\n",
    "  MLPd_f1_score_1 = metrics.f1_score(ytest_1, ypredbool, average=None)\n",
    "  MLPd_acc_score_1=metrics.accuracy_score(ytest_1, ypredbool)\n",
    "  MLPd_acc_score_train_1=metrics.accuracy_score(ytrain_1, ypredbooltr)\n",
    "  MLPd_recall_score_1 = metrics.recall_score(ytest_1, ypredbool, average=None)\n",
    "\n",
    "  MLPd_F1_temp.append(MLPd_f1_score_1[1])\n",
    "  MLPd_acc_temp.append(MLPd_acc_score_1)\n",
    "  MLPd_acctr_temp.append(MLPd_acc_score_train_1)\n",
    "  MLPd_recall_temp.append(MLPd_recall_score_1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "id": "REE74RWmBa-N",
    "outputId": "8bc399b7-873a-41de-d82d-4e3ec9e3067b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.847682119205298, 0.8938906752411575, 0.9034267912772587, 0.9038461538461539, 0.8819875776397514]\n",
      "[0.8333333333333334, 0.8804347826086957, 0.8876811594202898, 0.8913043478260869, 0.8623188405797102]\n",
      "[0.8847352024922118, 0.8769470404984424, 0.8753894080996885, 0.8707165109034268, 0.8862928348909658]\n",
      "[0.8366013071895425, 0.9084967320261438, 0.9477124183006536, 0.9215686274509803, 0.9281045751633987]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-70e912c0-1d82-49ec-98be-bb439ee161d3\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Train accuracy</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>diferent of Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.836601</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.884735</td>\n",
       "      <td>0.051402</td>\n",
       "      <td>0.847682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123</td>\n",
       "      <td>0.908497</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.876947</td>\n",
       "      <td>-0.003488</td>\n",
       "      <td>0.893891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>0.947712</td>\n",
       "      <td>0.887681</td>\n",
       "      <td>0.875389</td>\n",
       "      <td>-0.012292</td>\n",
       "      <td>0.903427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>231</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.870717</td>\n",
       "      <td>-0.020588</td>\n",
       "      <td>0.903846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321</td>\n",
       "      <td>0.928105</td>\n",
       "      <td>0.862319</td>\n",
       "      <td>0.886293</td>\n",
       "      <td>0.023974</td>\n",
       "      <td>0.881988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-70e912c0-1d82-49ec-98be-bb439ee161d3')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-70e912c0-1d82-49ec-98be-bb439ee161d3 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-70e912c0-1d82-49ec-98be-bb439ee161d3');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   seed    Recall  Train accuracy  Accuracy  diferent of Accuracy        F1\n",
       "0   100  0.836601        0.833333  0.884735              0.051402  0.847682\n",
       "1   123  0.908497        0.880435  0.876947             -0.003488  0.893891\n",
       "2   200  0.947712        0.887681  0.875389             -0.012292  0.903427\n",
       "3   231  0.921569        0.891304  0.870717             -0.020588  0.903846\n",
       "4   321  0.928105        0.862319  0.886293              0.023974  0.881988"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(MLPd_F1_temp)\n",
    "print(MLPd_acc_temp)\n",
    "print(MLPd_acctr_temp)\n",
    "print(MLPd_recall_temp)\n",
    "\n",
    "delta1=MLPd_acctr_temp[0]-MLPd_acc_temp[0]\n",
    "delta2=MLPd_acctr_temp[1]-MLPd_acc_temp[1]\n",
    "delta3=MLPd_acctr_temp[2]-MLPd_acc_temp[2]\n",
    "delta4=MLPd_acctr_temp[3]-MLPd_acc_temp[3]\n",
    "delta5=MLPd_acctr_temp[4]-MLPd_acc_temp[4]\n",
    "MLPd_delta_temp=[delta1,delta2,delta3,delta4,delta5]\n",
    "\n",
    "MLPd=pd.DataFrame({'seed':[100,123,200,231,321],'Recall':[MLPd_recall_temp[0],MLPd_recall_temp[1],MLPd_recall_temp[2],MLPd_recall_temp[3],\n",
    "                                                     MLPd_recall_temp[4]],'Train accuracy':[MLPd_acc_temp[0],MLPd_acc_temp[1],MLPd_acc_temp[2],\n",
    "                                                    MLPd_acc_temp[3],MLPd_acc_temp[4]],'Accuracy':[MLPd_acctr_temp[0],MLPd_acctr_temp[1],MLPd_acctr_temp[2],\n",
    "                                                    MLPd_acctr_temp[3],MLPd_acctr_temp[4]],'diferent of Accuracy':[delta1,delta2,delta3,delta4,delta5],'F1':[MLPd_F1_temp[0],MLPd_F1_temp[1],MLPd_F1_temp[2],\n",
    "                                                    MLPd_F1_temp[3],MLPd_F1_temp[4]]})\n",
    "MLPd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86X2A_MwClWt"
   },
   "source": [
    "#MLP with only l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ywyne5TnCrf4",
    "outputId": "a92a16b9-4dba-45e7-ee00-f258a61bd67a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dropout_15 (Dropout)        (None, 12)                0         \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 6)                 78        \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 6)                 0         \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 10)                70        \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 8)                 88        \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 2)                 18        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 254\n",
      "Trainable params: 254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "re=regularizers.L2(0.1)\n",
    "def create_model_4():\n",
    "    model_1 = Sequential()\n",
    "    #model_1.add(Dropout(0.1, input_dim = 12))\n",
    "    model_1.add(Dense(6,  input_dim = 12,activation = 'relu',kernel_regularizer=re))\n",
    "    #model_1.add(Dropout(0.1))\n",
    "    model_1.add(Dense(10, activation = 'relu'))\n",
    "    #model_1.add(Dropout(0.1))\n",
    "    model_1.add(Dense(8, activation = 'relu'))\n",
    "    model_1.add(Dense(2, activation = 'sigmoid'))\n",
    "    model_1.compile(optimizer = Adam(learning_rate=0.00036), loss = 'binary_crossentropy', metrics = ['accuracy']) \n",
    "    return model_1\n",
    "\n",
    "model_4 = create_model_4()\n",
    "\n",
    "print(model_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WuN0YP-LDD0q",
    "outputId": "07def9cb-63f3-43de-b2b7-9fd4e3ab0bc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "161/161 [==============================] - 2s 4ms/step - loss: 1.4904 - accuracy: 0.6698 - val_loss: 1.3693 - val_accuracy: 0.7572\n",
      "Epoch 2/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 1.2626 - accuracy: 0.7897 - val_loss: 1.1676 - val_accuracy: 0.7790\n",
      "Epoch 3/80\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 1.0780 - accuracy: 0.7960 - val_loss: 1.0032 - val_accuracy: 0.7935\n",
      "Epoch 4/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.9207 - accuracy: 0.8100 - val_loss: 0.8631 - val_accuracy: 0.8152\n",
      "Epoch 5/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.7820 - accuracy: 0.8162 - val_loss: 0.7436 - val_accuracy: 0.8080\n",
      "Epoch 6/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.6671 - accuracy: 0.8224 - val_loss: 0.6562 - val_accuracy: 0.8188\n",
      "Epoch 7/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5815 - accuracy: 0.8614 - val_loss: 0.5902 - val_accuracy: 0.8225\n",
      "Epoch 8/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.5164 - accuracy: 0.8645 - val_loss: 0.5474 - val_accuracy: 0.8261\n",
      "Epoch 9/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4725 - accuracy: 0.8707 - val_loss: 0.5154 - val_accuracy: 0.8297\n",
      "Epoch 10/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4417 - accuracy: 0.8707 - val_loss: 0.4977 - val_accuracy: 0.8297\n",
      "Epoch 11/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.4214 - accuracy: 0.8738 - val_loss: 0.4843 - val_accuracy: 0.8297\n",
      "Epoch 12/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.4061 - accuracy: 0.8738 - val_loss: 0.4736 - val_accuracy: 0.8333\n",
      "Epoch 13/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3944 - accuracy: 0.8692 - val_loss: 0.4712 - val_accuracy: 0.8225\n",
      "Epoch 14/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3870 - accuracy: 0.8707 - val_loss: 0.4591 - val_accuracy: 0.8442\n",
      "Epoch 15/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3788 - accuracy: 0.8801 - val_loss: 0.4581 - val_accuracy: 0.8225\n",
      "Epoch 16/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3724 - accuracy: 0.8785 - val_loss: 0.4586 - val_accuracy: 0.8152\n",
      "Epoch 17/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3685 - accuracy: 0.8723 - val_loss: 0.4547 - val_accuracy: 0.8080\n",
      "Epoch 18/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3654 - accuracy: 0.8754 - val_loss: 0.4451 - val_accuracy: 0.8225\n",
      "Epoch 19/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3590 - accuracy: 0.8832 - val_loss: 0.4482 - val_accuracy: 0.8152\n",
      "Epoch 20/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3562 - accuracy: 0.8723 - val_loss: 0.4380 - val_accuracy: 0.8478\n",
      "Epoch 21/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3543 - accuracy: 0.8785 - val_loss: 0.4374 - val_accuracy: 0.8261\n",
      "Epoch 22/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3511 - accuracy: 0.8723 - val_loss: 0.4424 - val_accuracy: 0.8188\n",
      "Epoch 23/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3485 - accuracy: 0.8707 - val_loss: 0.4345 - val_accuracy: 0.8225\n",
      "Epoch 24/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3477 - accuracy: 0.8754 - val_loss: 0.4329 - val_accuracy: 0.8225\n",
      "Epoch 25/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3462 - accuracy: 0.8738 - val_loss: 0.4310 - val_accuracy: 0.8297\n",
      "Epoch 26/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3442 - accuracy: 0.8769 - val_loss: 0.4325 - val_accuracy: 0.8188\n",
      "Epoch 27/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3421 - accuracy: 0.8754 - val_loss: 0.4286 - val_accuracy: 0.8297\n",
      "Epoch 28/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3431 - accuracy: 0.8723 - val_loss: 0.4302 - val_accuracy: 0.8188\n",
      "Epoch 29/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3389 - accuracy: 0.8723 - val_loss: 0.4258 - val_accuracy: 0.8406\n",
      "Epoch 30/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3390 - accuracy: 0.8847 - val_loss: 0.4379 - val_accuracy: 0.8116\n",
      "Epoch 31/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3364 - accuracy: 0.8738 - val_loss: 0.4258 - val_accuracy: 0.8152\n",
      "Epoch 32/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3366 - accuracy: 0.8769 - val_loss: 0.4232 - val_accuracy: 0.8333\n",
      "Epoch 33/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3362 - accuracy: 0.8676 - val_loss: 0.4265 - val_accuracy: 0.8152\n",
      "Epoch 34/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3338 - accuracy: 0.8769 - val_loss: 0.4253 - val_accuracy: 0.8152\n",
      "Epoch 35/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3350 - accuracy: 0.8769 - val_loss: 0.4288 - val_accuracy: 0.8152\n",
      "Epoch 36/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3316 - accuracy: 0.8754 - val_loss: 0.4228 - val_accuracy: 0.8333\n",
      "Epoch 37/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3326 - accuracy: 0.8738 - val_loss: 0.4304 - val_accuracy: 0.8152\n",
      "Epoch 38/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3309 - accuracy: 0.8785 - val_loss: 0.4210 - val_accuracy: 0.8370\n",
      "Epoch 39/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3321 - accuracy: 0.8801 - val_loss: 0.4232 - val_accuracy: 0.8188\n",
      "Epoch 40/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3313 - accuracy: 0.8769 - val_loss: 0.4229 - val_accuracy: 0.8152\n",
      "Epoch 41/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3292 - accuracy: 0.8801 - val_loss: 0.4283 - val_accuracy: 0.8116\n",
      "Epoch 42/80\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.3269 - accuracy: 0.8801 - val_loss: 0.4196 - val_accuracy: 0.8370\n",
      "Epoch 43/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3272 - accuracy: 0.8816 - val_loss: 0.4240 - val_accuracy: 0.8152\n",
      "Epoch 44/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3284 - accuracy: 0.8801 - val_loss: 0.4234 - val_accuracy: 0.8152\n",
      "Epoch 45/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3266 - accuracy: 0.8801 - val_loss: 0.4246 - val_accuracy: 0.8116\n",
      "Epoch 46/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3260 - accuracy: 0.8785 - val_loss: 0.4312 - val_accuracy: 0.8116\n",
      "Epoch 47/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3253 - accuracy: 0.8769 - val_loss: 0.4300 - val_accuracy: 0.8116\n",
      "Epoch 48/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3257 - accuracy: 0.8738 - val_loss: 0.4169 - val_accuracy: 0.8333\n",
      "Epoch 49/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3252 - accuracy: 0.8723 - val_loss: 0.4162 - val_accuracy: 0.8297\n",
      "Epoch 50/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3246 - accuracy: 0.8738 - val_loss: 0.4163 - val_accuracy: 0.8333\n",
      "Epoch 51/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3258 - accuracy: 0.8785 - val_loss: 0.4178 - val_accuracy: 0.8261\n",
      "Epoch 52/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3243 - accuracy: 0.8754 - val_loss: 0.4194 - val_accuracy: 0.8188\n",
      "Epoch 53/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3227 - accuracy: 0.8738 - val_loss: 0.4244 - val_accuracy: 0.8152\n",
      "Epoch 54/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3169 - accuracy: 0.8785 - val_loss: 0.4467 - val_accuracy: 0.8152\n",
      "Epoch 55/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3289 - accuracy: 0.8754 - val_loss: 0.4201 - val_accuracy: 0.8225\n",
      "Epoch 56/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3229 - accuracy: 0.8785 - val_loss: 0.4172 - val_accuracy: 0.8225\n",
      "Epoch 57/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3216 - accuracy: 0.8801 - val_loss: 0.4170 - val_accuracy: 0.8225\n",
      "Epoch 58/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3234 - accuracy: 0.8801 - val_loss: 0.4208 - val_accuracy: 0.8152\n",
      "Epoch 59/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3213 - accuracy: 0.8738 - val_loss: 0.4173 - val_accuracy: 0.8261\n",
      "Epoch 60/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3232 - accuracy: 0.8816 - val_loss: 0.4182 - val_accuracy: 0.8188\n",
      "Epoch 61/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3223 - accuracy: 0.8754 - val_loss: 0.4154 - val_accuracy: 0.8297\n",
      "Epoch 62/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3206 - accuracy: 0.8754 - val_loss: 0.4143 - val_accuracy: 0.8333\n",
      "Epoch 63/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3231 - accuracy: 0.8707 - val_loss: 0.4173 - val_accuracy: 0.8225\n",
      "Epoch 64/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3179 - accuracy: 0.8801 - val_loss: 0.4319 - val_accuracy: 0.8116\n",
      "Epoch 65/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3206 - accuracy: 0.8801 - val_loss: 0.4156 - val_accuracy: 0.8297\n",
      "Epoch 66/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3207 - accuracy: 0.8738 - val_loss: 0.4185 - val_accuracy: 0.8188\n",
      "Epoch 67/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3191 - accuracy: 0.8754 - val_loss: 0.4167 - val_accuracy: 0.8225\n",
      "Epoch 68/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3205 - accuracy: 0.8816 - val_loss: 0.4217 - val_accuracy: 0.8188\n",
      "Epoch 69/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3222 - accuracy: 0.8816 - val_loss: 0.4202 - val_accuracy: 0.8152\n",
      "Epoch 70/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3190 - accuracy: 0.8816 - val_loss: 0.4137 - val_accuracy: 0.8370\n",
      "Epoch 71/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3184 - accuracy: 0.8769 - val_loss: 0.4188 - val_accuracy: 0.8225\n",
      "Epoch 72/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3195 - accuracy: 0.8832 - val_loss: 0.4216 - val_accuracy: 0.8152\n",
      "Epoch 73/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3179 - accuracy: 0.8785 - val_loss: 0.4203 - val_accuracy: 0.8188\n",
      "Epoch 74/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3178 - accuracy: 0.8832 - val_loss: 0.4150 - val_accuracy: 0.8297\n",
      "Epoch 75/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3178 - accuracy: 0.8832 - val_loss: 0.4284 - val_accuracy: 0.8116\n",
      "Epoch 76/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3195 - accuracy: 0.8863 - val_loss: 0.4130 - val_accuracy: 0.8333\n",
      "Epoch 77/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3199 - accuracy: 0.8816 - val_loss: 0.4160 - val_accuracy: 0.8261\n",
      "Epoch 78/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3168 - accuracy: 0.8769 - val_loss: 0.4249 - val_accuracy: 0.8188\n",
      "Epoch 79/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3194 - accuracy: 0.8769 - val_loss: 0.4174 - val_accuracy: 0.8152\n",
      "Epoch 80/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3142 - accuracy: 0.8785 - val_loss: 0.4466 - val_accuracy: 0.8116\n",
      "Epoch 1/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3511 - accuracy: 0.8583 - val_loss: 0.3415 - val_accuracy: 0.8696\n",
      "Epoch 2/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3452 - accuracy: 0.8645 - val_loss: 0.3420 - val_accuracy: 0.8696\n",
      "Epoch 3/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3470 - accuracy: 0.8645 - val_loss: 0.3427 - val_accuracy: 0.8696\n",
      "Epoch 4/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3411 - accuracy: 0.8645 - val_loss: 0.3503 - val_accuracy: 0.8659\n",
      "Epoch 5/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3454 - accuracy: 0.8645 - val_loss: 0.3479 - val_accuracy: 0.8732\n",
      "Epoch 6/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3434 - accuracy: 0.8567 - val_loss: 0.3435 - val_accuracy: 0.8768\n",
      "Epoch 7/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3404 - accuracy: 0.8692 - val_loss: 0.3454 - val_accuracy: 0.8696\n",
      "Epoch 8/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3415 - accuracy: 0.8567 - val_loss: 0.3448 - val_accuracy: 0.8732\n",
      "Epoch 9/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3385 - accuracy: 0.8723 - val_loss: 0.3530 - val_accuracy: 0.8696\n",
      "Epoch 10/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3423 - accuracy: 0.8598 - val_loss: 0.3446 - val_accuracy: 0.8732\n",
      "Epoch 11/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3408 - accuracy: 0.8614 - val_loss: 0.3451 - val_accuracy: 0.8696\n",
      "Epoch 12/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3389 - accuracy: 0.8645 - val_loss: 0.3458 - val_accuracy: 0.8732\n",
      "Epoch 13/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3401 - accuracy: 0.8645 - val_loss: 0.3455 - val_accuracy: 0.8732\n",
      "Epoch 14/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3386 - accuracy: 0.8676 - val_loss: 0.3443 - val_accuracy: 0.8732\n",
      "Epoch 15/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3388 - accuracy: 0.8692 - val_loss: 0.3463 - val_accuracy: 0.8732\n",
      "Epoch 16/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3396 - accuracy: 0.8676 - val_loss: 0.3460 - val_accuracy: 0.8732\n",
      "Epoch 17/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3377 - accuracy: 0.8660 - val_loss: 0.3472 - val_accuracy: 0.8696\n",
      "Epoch 18/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3406 - accuracy: 0.8645 - val_loss: 0.3462 - val_accuracy: 0.8732\n",
      "Epoch 19/80\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.3396 - accuracy: 0.8692 - val_loss: 0.3495 - val_accuracy: 0.8768\n",
      "Epoch 20/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3376 - accuracy: 0.8707 - val_loss: 0.3480 - val_accuracy: 0.8768\n",
      "Epoch 21/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3427 - accuracy: 0.8692 - val_loss: 0.3478 - val_accuracy: 0.8696\n",
      "Epoch 22/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3414 - accuracy: 0.8598 - val_loss: 0.3474 - val_accuracy: 0.8768\n",
      "Epoch 23/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3382 - accuracy: 0.8660 - val_loss: 0.3454 - val_accuracy: 0.8696\n",
      "Epoch 24/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3395 - accuracy: 0.8738 - val_loss: 0.3472 - val_accuracy: 0.8659\n",
      "Epoch 25/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3367 - accuracy: 0.8723 - val_loss: 0.3521 - val_accuracy: 0.8732\n",
      "Epoch 26/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3419 - accuracy: 0.8660 - val_loss: 0.3473 - val_accuracy: 0.8768\n",
      "Epoch 27/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3399 - accuracy: 0.8676 - val_loss: 0.3477 - val_accuracy: 0.8659\n",
      "Epoch 28/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3325 - accuracy: 0.8723 - val_loss: 0.3572 - val_accuracy: 0.8696\n",
      "Epoch 29/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3373 - accuracy: 0.8723 - val_loss: 0.3473 - val_accuracy: 0.8768\n",
      "Epoch 30/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3375 - accuracy: 0.8692 - val_loss: 0.3479 - val_accuracy: 0.8768\n",
      "Epoch 31/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3392 - accuracy: 0.8723 - val_loss: 0.3496 - val_accuracy: 0.8768\n",
      "Epoch 32/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3367 - accuracy: 0.8629 - val_loss: 0.3483 - val_accuracy: 0.8659\n",
      "Epoch 33/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3389 - accuracy: 0.8645 - val_loss: 0.3470 - val_accuracy: 0.8659\n",
      "Epoch 34/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3355 - accuracy: 0.8754 - val_loss: 0.3493 - val_accuracy: 0.8696\n",
      "Epoch 35/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3362 - accuracy: 0.8629 - val_loss: 0.3555 - val_accuracy: 0.8768\n",
      "Epoch 36/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3365 - accuracy: 0.8660 - val_loss: 0.3502 - val_accuracy: 0.8696\n",
      "Epoch 37/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3345 - accuracy: 0.8676 - val_loss: 0.3526 - val_accuracy: 0.8804\n",
      "Epoch 38/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3362 - accuracy: 0.8614 - val_loss: 0.3504 - val_accuracy: 0.8732\n",
      "Epoch 39/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3383 - accuracy: 0.8692 - val_loss: 0.3492 - val_accuracy: 0.8696\n",
      "Epoch 40/80\n",
      "161/161 [==============================] - 1s 7ms/step - loss: 0.3382 - accuracy: 0.8629 - val_loss: 0.3487 - val_accuracy: 0.8732\n",
      "Epoch 41/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3369 - accuracy: 0.8692 - val_loss: 0.3571 - val_accuracy: 0.8732\n",
      "Epoch 42/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3412 - accuracy: 0.8645 - val_loss: 0.3519 - val_accuracy: 0.8696\n",
      "Epoch 43/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3368 - accuracy: 0.8660 - val_loss: 0.3490 - val_accuracy: 0.8732\n",
      "Epoch 44/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3388 - accuracy: 0.8707 - val_loss: 0.3507 - val_accuracy: 0.8768\n",
      "Epoch 45/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3354 - accuracy: 0.8645 - val_loss: 0.3476 - val_accuracy: 0.8696\n",
      "Epoch 46/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3364 - accuracy: 0.8676 - val_loss: 0.3551 - val_accuracy: 0.8804\n",
      "Epoch 47/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3348 - accuracy: 0.8707 - val_loss: 0.3505 - val_accuracy: 0.8732\n",
      "Epoch 48/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3371 - accuracy: 0.8645 - val_loss: 0.3484 - val_accuracy: 0.8732\n",
      "Epoch 49/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3365 - accuracy: 0.8707 - val_loss: 0.3494 - val_accuracy: 0.8732\n",
      "Epoch 50/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3364 - accuracy: 0.8723 - val_loss: 0.3486 - val_accuracy: 0.8768\n",
      "Epoch 51/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3346 - accuracy: 0.8660 - val_loss: 0.3535 - val_accuracy: 0.8804\n",
      "Epoch 52/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3356 - accuracy: 0.8660 - val_loss: 0.3508 - val_accuracy: 0.8732\n",
      "Epoch 53/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3383 - accuracy: 0.8676 - val_loss: 0.3499 - val_accuracy: 0.8732\n",
      "Epoch 54/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3360 - accuracy: 0.8676 - val_loss: 0.3488 - val_accuracy: 0.8732\n",
      "Epoch 55/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3371 - accuracy: 0.8723 - val_loss: 0.3484 - val_accuracy: 0.8696\n",
      "Epoch 56/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3369 - accuracy: 0.8676 - val_loss: 0.3495 - val_accuracy: 0.8696\n",
      "Epoch 57/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3366 - accuracy: 0.8723 - val_loss: 0.3490 - val_accuracy: 0.8659\n",
      "Epoch 58/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3338 - accuracy: 0.8660 - val_loss: 0.3487 - val_accuracy: 0.8696\n",
      "Epoch 59/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3353 - accuracy: 0.8707 - val_loss: 0.3488 - val_accuracy: 0.8732\n",
      "Epoch 60/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3337 - accuracy: 0.8785 - val_loss: 0.3506 - val_accuracy: 0.8804\n",
      "Epoch 61/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3362 - accuracy: 0.8692 - val_loss: 0.3494 - val_accuracy: 0.8732\n",
      "Epoch 62/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3366 - accuracy: 0.8707 - val_loss: 0.3497 - val_accuracy: 0.8732\n",
      "Epoch 63/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3384 - accuracy: 0.8692 - val_loss: 0.3476 - val_accuracy: 0.8696\n",
      "Epoch 64/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3356 - accuracy: 0.8738 - val_loss: 0.3483 - val_accuracy: 0.8696\n",
      "Epoch 65/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3346 - accuracy: 0.8707 - val_loss: 0.3508 - val_accuracy: 0.8804\n",
      "Epoch 66/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3352 - accuracy: 0.8754 - val_loss: 0.3485 - val_accuracy: 0.8696\n",
      "Epoch 67/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3357 - accuracy: 0.8692 - val_loss: 0.3489 - val_accuracy: 0.8696\n",
      "Epoch 68/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3358 - accuracy: 0.8707 - val_loss: 0.3493 - val_accuracy: 0.8732\n",
      "Epoch 69/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3338 - accuracy: 0.8801 - val_loss: 0.3490 - val_accuracy: 0.8768\n",
      "Epoch 70/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3400 - accuracy: 0.8645 - val_loss: 0.3483 - val_accuracy: 0.8804\n",
      "Epoch 71/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3349 - accuracy: 0.8707 - val_loss: 0.3481 - val_accuracy: 0.8696\n",
      "Epoch 72/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3368 - accuracy: 0.8754 - val_loss: 0.3518 - val_accuracy: 0.8841\n",
      "Epoch 73/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3317 - accuracy: 0.8707 - val_loss: 0.3543 - val_accuracy: 0.8696\n",
      "Epoch 74/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3352 - accuracy: 0.8692 - val_loss: 0.3489 - val_accuracy: 0.8768\n",
      "Epoch 75/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3385 - accuracy: 0.8629 - val_loss: 0.3518 - val_accuracy: 0.8804\n",
      "Epoch 76/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3348 - accuracy: 0.8723 - val_loss: 0.3543 - val_accuracy: 0.8804\n",
      "Epoch 77/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3389 - accuracy: 0.8645 - val_loss: 0.3483 - val_accuracy: 0.8768\n",
      "Epoch 78/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3363 - accuracy: 0.8645 - val_loss: 0.3482 - val_accuracy: 0.8696\n",
      "Epoch 79/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3361 - accuracy: 0.8723 - val_loss: 0.3527 - val_accuracy: 0.8732\n",
      "Epoch 80/80\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.3331 - accuracy: 0.8614 - val_loss: 0.3597 - val_accuracy: 0.8659\n",
      "Epoch 1/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3526 - accuracy: 0.8707 - val_loss: 0.3103 - val_accuracy: 0.8841\n",
      "Epoch 2/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3508 - accuracy: 0.8676 - val_loss: 0.3184 - val_accuracy: 0.8732\n",
      "Epoch 3/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3502 - accuracy: 0.8692 - val_loss: 0.3065 - val_accuracy: 0.8804\n",
      "Epoch 4/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3473 - accuracy: 0.8660 - val_loss: 0.3267 - val_accuracy: 0.8732\n",
      "Epoch 5/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3485 - accuracy: 0.8707 - val_loss: 0.3193 - val_accuracy: 0.8732\n",
      "Epoch 6/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3516 - accuracy: 0.8598 - val_loss: 0.3142 - val_accuracy: 0.8768\n",
      "Epoch 7/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3526 - accuracy: 0.8676 - val_loss: 0.3113 - val_accuracy: 0.8732\n",
      "Epoch 8/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3485 - accuracy: 0.8692 - val_loss: 0.3119 - val_accuracy: 0.8732\n",
      "Epoch 9/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3463 - accuracy: 0.8660 - val_loss: 0.3097 - val_accuracy: 0.8804\n",
      "Epoch 10/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3502 - accuracy: 0.8738 - val_loss: 0.3111 - val_accuracy: 0.8732\n",
      "Epoch 11/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3470 - accuracy: 0.8707 - val_loss: 0.3201 - val_accuracy: 0.8659\n",
      "Epoch 12/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3477 - accuracy: 0.8707 - val_loss: 0.3160 - val_accuracy: 0.8696\n",
      "Epoch 13/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3472 - accuracy: 0.8738 - val_loss: 0.3120 - val_accuracy: 0.8804\n",
      "Epoch 14/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3490 - accuracy: 0.8738 - val_loss: 0.3113 - val_accuracy: 0.8841\n",
      "Epoch 15/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3440 - accuracy: 0.8676 - val_loss: 0.3266 - val_accuracy: 0.8732\n",
      "Epoch 16/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3463 - accuracy: 0.8692 - val_loss: 0.3229 - val_accuracy: 0.8732\n",
      "Epoch 17/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3506 - accuracy: 0.8676 - val_loss: 0.3188 - val_accuracy: 0.8696\n",
      "Epoch 18/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3487 - accuracy: 0.8676 - val_loss: 0.3208 - val_accuracy: 0.8659\n",
      "Epoch 19/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3471 - accuracy: 0.8676 - val_loss: 0.3137 - val_accuracy: 0.8768\n",
      "Epoch 20/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3462 - accuracy: 0.8769 - val_loss: 0.3126 - val_accuracy: 0.8841\n",
      "Epoch 21/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3461 - accuracy: 0.8692 - val_loss: 0.3130 - val_accuracy: 0.8804\n",
      "Epoch 22/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3481 - accuracy: 0.8583 - val_loss: 0.3286 - val_accuracy: 0.8732\n",
      "Epoch 23/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3470 - accuracy: 0.8676 - val_loss: 0.3175 - val_accuracy: 0.8696\n",
      "Epoch 24/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3469 - accuracy: 0.8676 - val_loss: 0.3156 - val_accuracy: 0.8659\n",
      "Epoch 25/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3449 - accuracy: 0.8692 - val_loss: 0.3135 - val_accuracy: 0.8768\n",
      "Epoch 26/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3465 - accuracy: 0.8707 - val_loss: 0.3273 - val_accuracy: 0.8732\n",
      "Epoch 27/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3460 - accuracy: 0.8645 - val_loss: 0.3345 - val_accuracy: 0.8768\n",
      "Epoch 28/80\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.3522 - accuracy: 0.8536 - val_loss: 0.3231 - val_accuracy: 0.8696\n",
      "Epoch 29/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3469 - accuracy: 0.8629 - val_loss: 0.3113 - val_accuracy: 0.8804\n",
      "Epoch 30/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3479 - accuracy: 0.8692 - val_loss: 0.3158 - val_accuracy: 0.8696\n",
      "Epoch 31/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3475 - accuracy: 0.8707 - val_loss: 0.3159 - val_accuracy: 0.8659\n",
      "Epoch 32/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3479 - accuracy: 0.8692 - val_loss: 0.3224 - val_accuracy: 0.8659\n",
      "Epoch 33/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3498 - accuracy: 0.8614 - val_loss: 0.3254 - val_accuracy: 0.8732\n",
      "Epoch 34/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3452 - accuracy: 0.8692 - val_loss: 0.3191 - val_accuracy: 0.8659\n",
      "Epoch 35/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3491 - accuracy: 0.8692 - val_loss: 0.3115 - val_accuracy: 0.8804\n",
      "Epoch 36/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3471 - accuracy: 0.8629 - val_loss: 0.3193 - val_accuracy: 0.8659\n",
      "Epoch 37/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3468 - accuracy: 0.8629 - val_loss: 0.3280 - val_accuracy: 0.8768\n",
      "Epoch 38/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3450 - accuracy: 0.8629 - val_loss: 0.3140 - val_accuracy: 0.8804\n",
      "Epoch 39/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3461 - accuracy: 0.8692 - val_loss: 0.3140 - val_accuracy: 0.8768\n",
      "Epoch 40/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3479 - accuracy: 0.8676 - val_loss: 0.3239 - val_accuracy: 0.8696\n",
      "Epoch 41/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3471 - accuracy: 0.8692 - val_loss: 0.3160 - val_accuracy: 0.8696\n",
      "Epoch 42/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3511 - accuracy: 0.8692 - val_loss: 0.3173 - val_accuracy: 0.8732\n",
      "Epoch 43/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3461 - accuracy: 0.8645 - val_loss: 0.3135 - val_accuracy: 0.8877\n",
      "Epoch 44/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3477 - accuracy: 0.8614 - val_loss: 0.3137 - val_accuracy: 0.8696\n",
      "Epoch 45/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3463 - accuracy: 0.8723 - val_loss: 0.3118 - val_accuracy: 0.8732\n",
      "Epoch 46/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3453 - accuracy: 0.8567 - val_loss: 0.3152 - val_accuracy: 0.8696\n",
      "Epoch 47/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3455 - accuracy: 0.8723 - val_loss: 0.3249 - val_accuracy: 0.8696\n",
      "Epoch 48/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3449 - accuracy: 0.8660 - val_loss: 0.3278 - val_accuracy: 0.8696\n",
      "Epoch 49/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3504 - accuracy: 0.8629 - val_loss: 0.3151 - val_accuracy: 0.8732\n",
      "Epoch 50/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3473 - accuracy: 0.8598 - val_loss: 0.3181 - val_accuracy: 0.8659\n",
      "Epoch 51/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3451 - accuracy: 0.8629 - val_loss: 0.3120 - val_accuracy: 0.8804\n",
      "Epoch 52/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3460 - accuracy: 0.8692 - val_loss: 0.3228 - val_accuracy: 0.8696\n",
      "Epoch 53/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3454 - accuracy: 0.8692 - val_loss: 0.3128 - val_accuracy: 0.8804\n",
      "Epoch 54/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3496 - accuracy: 0.8614 - val_loss: 0.3202 - val_accuracy: 0.8623\n",
      "Epoch 55/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3445 - accuracy: 0.8707 - val_loss: 0.3126 - val_accuracy: 0.8804\n",
      "Epoch 56/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3457 - accuracy: 0.8598 - val_loss: 0.3124 - val_accuracy: 0.8804\n",
      "Epoch 57/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3485 - accuracy: 0.8692 - val_loss: 0.3147 - val_accuracy: 0.8623\n",
      "Epoch 58/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3427 - accuracy: 0.8614 - val_loss: 0.3430 - val_accuracy: 0.8659\n",
      "Epoch 59/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3494 - accuracy: 0.8598 - val_loss: 0.3262 - val_accuracy: 0.8696\n",
      "Epoch 60/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3441 - accuracy: 0.8738 - val_loss: 0.3134 - val_accuracy: 0.8804\n",
      "Epoch 61/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3457 - accuracy: 0.8645 - val_loss: 0.3174 - val_accuracy: 0.8659\n",
      "Epoch 62/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3511 - accuracy: 0.8676 - val_loss: 0.3162 - val_accuracy: 0.8659\n",
      "Epoch 63/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3488 - accuracy: 0.8660 - val_loss: 0.3117 - val_accuracy: 0.8768\n",
      "Epoch 64/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3457 - accuracy: 0.8614 - val_loss: 0.3164 - val_accuracy: 0.8659\n",
      "Epoch 65/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3450 - accuracy: 0.8660 - val_loss: 0.3312 - val_accuracy: 0.8768\n",
      "Epoch 66/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3487 - accuracy: 0.8676 - val_loss: 0.3121 - val_accuracy: 0.8841\n",
      "Epoch 67/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3460 - accuracy: 0.8629 - val_loss: 0.3122 - val_accuracy: 0.8768\n",
      "Epoch 68/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3441 - accuracy: 0.8629 - val_loss: 0.3270 - val_accuracy: 0.8696\n",
      "Epoch 69/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3454 - accuracy: 0.8676 - val_loss: 0.3120 - val_accuracy: 0.8804\n",
      "Epoch 70/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3481 - accuracy: 0.8660 - val_loss: 0.3296 - val_accuracy: 0.8768\n",
      "Epoch 71/80\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.3459 - accuracy: 0.8645 - val_loss: 0.3225 - val_accuracy: 0.8659\n",
      "Epoch 72/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3495 - accuracy: 0.8629 - val_loss: 0.3153 - val_accuracy: 0.8659\n",
      "Epoch 73/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3484 - accuracy: 0.8707 - val_loss: 0.3124 - val_accuracy: 0.8768\n",
      "Epoch 74/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3463 - accuracy: 0.8645 - val_loss: 0.3113 - val_accuracy: 0.8804\n",
      "Epoch 75/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3498 - accuracy: 0.8660 - val_loss: 0.3127 - val_accuracy: 0.8732\n",
      "Epoch 76/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3487 - accuracy: 0.8614 - val_loss: 0.3114 - val_accuracy: 0.8804\n",
      "Epoch 77/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3465 - accuracy: 0.8629 - val_loss: 0.3266 - val_accuracy: 0.8696\n",
      "Epoch 78/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3497 - accuracy: 0.8645 - val_loss: 0.3160 - val_accuracy: 0.8696\n",
      "Epoch 79/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3461 - accuracy: 0.8707 - val_loss: 0.3166 - val_accuracy: 0.8696\n",
      "Epoch 80/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3467 - accuracy: 0.8598 - val_loss: 0.3128 - val_accuracy: 0.8768\n",
      "Epoch 1/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3453 - accuracy: 0.8583 - val_loss: 0.3149 - val_accuracy: 0.8877\n",
      "Epoch 2/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3534 - accuracy: 0.8505 - val_loss: 0.3087 - val_accuracy: 0.8913\n",
      "Epoch 3/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3486 - accuracy: 0.8629 - val_loss: 0.3312 - val_accuracy: 0.8877\n",
      "Epoch 4/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3507 - accuracy: 0.8567 - val_loss: 0.3105 - val_accuracy: 0.8841\n",
      "Epoch 5/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3458 - accuracy: 0.8629 - val_loss: 0.3121 - val_accuracy: 0.8986\n",
      "Epoch 6/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3486 - accuracy: 0.8567 - val_loss: 0.3111 - val_accuracy: 0.8913\n",
      "Epoch 7/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3470 - accuracy: 0.8520 - val_loss: 0.3164 - val_accuracy: 0.8913\n",
      "Epoch 8/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3481 - accuracy: 0.8583 - val_loss: 0.3097 - val_accuracy: 0.8877\n",
      "Epoch 9/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3469 - accuracy: 0.8567 - val_loss: 0.3094 - val_accuracy: 0.8841\n",
      "Epoch 10/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3490 - accuracy: 0.8567 - val_loss: 0.3119 - val_accuracy: 0.8841\n",
      "Epoch 11/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3467 - accuracy: 0.8614 - val_loss: 0.3098 - val_accuracy: 0.8877\n",
      "Epoch 12/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3485 - accuracy: 0.8567 - val_loss: 0.3117 - val_accuracy: 0.8949\n",
      "Epoch 13/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3464 - accuracy: 0.8583 - val_loss: 0.3098 - val_accuracy: 0.8913\n",
      "Epoch 14/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3447 - accuracy: 0.8489 - val_loss: 0.3138 - val_accuracy: 0.8877\n",
      "Epoch 15/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3473 - accuracy: 0.8614 - val_loss: 0.3106 - val_accuracy: 0.8841\n",
      "Epoch 16/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3469 - accuracy: 0.8645 - val_loss: 0.3168 - val_accuracy: 0.8913\n",
      "Epoch 17/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3451 - accuracy: 0.8614 - val_loss: 0.3111 - val_accuracy: 0.8877\n",
      "Epoch 18/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3479 - accuracy: 0.8629 - val_loss: 0.3091 - val_accuracy: 0.8841\n",
      "Epoch 19/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3522 - accuracy: 0.8598 - val_loss: 0.3094 - val_accuracy: 0.8841\n",
      "Epoch 20/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3506 - accuracy: 0.8645 - val_loss: 0.3106 - val_accuracy: 0.8877\n",
      "Epoch 21/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3469 - accuracy: 0.8598 - val_loss: 0.3098 - val_accuracy: 0.8841\n",
      "Epoch 22/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3499 - accuracy: 0.8567 - val_loss: 0.3216 - val_accuracy: 0.8913\n",
      "Epoch 23/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3472 - accuracy: 0.8567 - val_loss: 0.3113 - val_accuracy: 0.8877\n",
      "Epoch 24/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3485 - accuracy: 0.8629 - val_loss: 0.3094 - val_accuracy: 0.8804\n",
      "Epoch 25/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3463 - accuracy: 0.8598 - val_loss: 0.3099 - val_accuracy: 0.8841\n",
      "Epoch 26/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3485 - accuracy: 0.8614 - val_loss: 0.3146 - val_accuracy: 0.8949\n",
      "Epoch 27/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3466 - accuracy: 0.8614 - val_loss: 0.3094 - val_accuracy: 0.8841\n",
      "Epoch 28/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3462 - accuracy: 0.8505 - val_loss: 0.3215 - val_accuracy: 0.8841\n",
      "Epoch 29/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3432 - accuracy: 0.8520 - val_loss: 0.3132 - val_accuracy: 0.8877\n",
      "Epoch 30/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3444 - accuracy: 0.8583 - val_loss: 0.3168 - val_accuracy: 0.8986\n",
      "Epoch 31/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3451 - accuracy: 0.8614 - val_loss: 0.3200 - val_accuracy: 0.8949\n",
      "Epoch 32/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3454 - accuracy: 0.8645 - val_loss: 0.3258 - val_accuracy: 0.8913\n",
      "Epoch 33/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3475 - accuracy: 0.8551 - val_loss: 0.3137 - val_accuracy: 0.8949\n",
      "Epoch 34/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3477 - accuracy: 0.8614 - val_loss: 0.3098 - val_accuracy: 0.8949\n",
      "Epoch 35/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3462 - accuracy: 0.8551 - val_loss: 0.3121 - val_accuracy: 0.8986\n",
      "Epoch 36/80\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.3449 - accuracy: 0.8629 - val_loss: 0.3089 - val_accuracy: 0.8913\n",
      "Epoch 37/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3500 - accuracy: 0.8598 - val_loss: 0.3100 - val_accuracy: 0.8841\n",
      "Epoch 38/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3489 - accuracy: 0.8567 - val_loss: 0.3114 - val_accuracy: 0.8986\n",
      "Epoch 39/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3451 - accuracy: 0.8629 - val_loss: 0.3124 - val_accuracy: 0.8877\n",
      "Epoch 40/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3439 - accuracy: 0.8676 - val_loss: 0.3206 - val_accuracy: 0.8949\n",
      "Epoch 41/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3470 - accuracy: 0.8567 - val_loss: 0.3114 - val_accuracy: 0.8877\n",
      "Epoch 42/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3452 - accuracy: 0.8598 - val_loss: 0.3133 - val_accuracy: 0.8949\n",
      "Epoch 43/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3447 - accuracy: 0.8614 - val_loss: 0.3139 - val_accuracy: 0.8986\n",
      "Epoch 44/80\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.3470 - accuracy: 0.8583 - val_loss: 0.3111 - val_accuracy: 0.8877\n",
      "Epoch 45/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3395 - accuracy: 0.8660 - val_loss: 0.3169 - val_accuracy: 0.8986\n",
      "Epoch 46/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3460 - accuracy: 0.8567 - val_loss: 0.3176 - val_accuracy: 0.8986\n",
      "Epoch 47/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3493 - accuracy: 0.8598 - val_loss: 0.3148 - val_accuracy: 0.8986\n",
      "Epoch 48/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3480 - accuracy: 0.8583 - val_loss: 0.3138 - val_accuracy: 0.8949\n",
      "Epoch 49/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3536 - accuracy: 0.8583 - val_loss: 0.3113 - val_accuracy: 0.8877\n",
      "Epoch 50/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3446 - accuracy: 0.8660 - val_loss: 0.3153 - val_accuracy: 0.8986\n",
      "Epoch 51/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3479 - accuracy: 0.8536 - val_loss: 0.3124 - val_accuracy: 0.8877\n",
      "Epoch 52/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3465 - accuracy: 0.8629 - val_loss: 0.3097 - val_accuracy: 0.8877\n",
      "Epoch 53/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3455 - accuracy: 0.8645 - val_loss: 0.3097 - val_accuracy: 0.8877\n",
      "Epoch 54/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3464 - accuracy: 0.8629 - val_loss: 0.3103 - val_accuracy: 0.8841\n",
      "Epoch 55/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3463 - accuracy: 0.8567 - val_loss: 0.3092 - val_accuracy: 0.8841\n",
      "Epoch 56/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3502 - accuracy: 0.8489 - val_loss: 0.3108 - val_accuracy: 0.8913\n",
      "Epoch 57/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3442 - accuracy: 0.8583 - val_loss: 0.3134 - val_accuracy: 0.8877\n",
      "Epoch 58/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3475 - accuracy: 0.8567 - val_loss: 0.3119 - val_accuracy: 0.8877\n",
      "Epoch 59/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3448 - accuracy: 0.8614 - val_loss: 0.3146 - val_accuracy: 0.8986\n",
      "Epoch 60/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3469 - accuracy: 0.8520 - val_loss: 0.3154 - val_accuracy: 0.8949\n",
      "Epoch 61/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3476 - accuracy: 0.8598 - val_loss: 0.3102 - val_accuracy: 0.8913\n",
      "Epoch 62/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3477 - accuracy: 0.8567 - val_loss: 0.3105 - val_accuracy: 0.8877\n",
      "Epoch 63/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3478 - accuracy: 0.8598 - val_loss: 0.3108 - val_accuracy: 0.8804\n",
      "Epoch 64/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3473 - accuracy: 0.8692 - val_loss: 0.3192 - val_accuracy: 0.8949\n",
      "Epoch 65/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3491 - accuracy: 0.8583 - val_loss: 0.3110 - val_accuracy: 0.8913\n",
      "Epoch 66/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3494 - accuracy: 0.8567 - val_loss: 0.3097 - val_accuracy: 0.8804\n",
      "Epoch 67/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3449 - accuracy: 0.8583 - val_loss: 0.3095 - val_accuracy: 0.8804\n",
      "Epoch 68/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3464 - accuracy: 0.8567 - val_loss: 0.3099 - val_accuracy: 0.8804\n",
      "Epoch 69/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3466 - accuracy: 0.8614 - val_loss: 0.3122 - val_accuracy: 0.8949\n",
      "Epoch 70/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3485 - accuracy: 0.8583 - val_loss: 0.3106 - val_accuracy: 0.8877\n",
      "Epoch 71/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3492 - accuracy: 0.8598 - val_loss: 0.3184 - val_accuracy: 0.8949\n",
      "Epoch 72/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3473 - accuracy: 0.8614 - val_loss: 0.3127 - val_accuracy: 0.8949\n",
      "Epoch 73/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3457 - accuracy: 0.8551 - val_loss: 0.3131 - val_accuracy: 0.8949\n",
      "Epoch 74/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3473 - accuracy: 0.8567 - val_loss: 0.3109 - val_accuracy: 0.8877\n",
      "Epoch 75/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3486 - accuracy: 0.8645 - val_loss: 0.3104 - val_accuracy: 0.8804\n",
      "Epoch 76/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3460 - accuracy: 0.8676 - val_loss: 0.3097 - val_accuracy: 0.8841\n",
      "Epoch 77/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3465 - accuracy: 0.8614 - val_loss: 0.3098 - val_accuracy: 0.8804\n",
      "Epoch 78/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3486 - accuracy: 0.8629 - val_loss: 0.3168 - val_accuracy: 0.8913\n",
      "Epoch 79/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3449 - accuracy: 0.8567 - val_loss: 0.3119 - val_accuracy: 0.8877\n",
      "Epoch 80/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3461 - accuracy: 0.8551 - val_loss: 0.3104 - val_accuracy: 0.8877\n",
      "Epoch 1/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3268 - accuracy: 0.8816 - val_loss: 0.3435 - val_accuracy: 0.8514\n",
      "Epoch 2/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3303 - accuracy: 0.8769 - val_loss: 0.3534 - val_accuracy: 0.8623\n",
      "Epoch 3/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3299 - accuracy: 0.8738 - val_loss: 0.3488 - val_accuracy: 0.8623\n",
      "Epoch 4/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3271 - accuracy: 0.8723 - val_loss: 0.3467 - val_accuracy: 0.8478\n",
      "Epoch 5/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3290 - accuracy: 0.8723 - val_loss: 0.3568 - val_accuracy: 0.8514\n",
      "Epoch 6/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3315 - accuracy: 0.8738 - val_loss: 0.3443 - val_accuracy: 0.8659\n",
      "Epoch 7/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3302 - accuracy: 0.8723 - val_loss: 0.3688 - val_accuracy: 0.8478\n",
      "Epoch 8/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3333 - accuracy: 0.8676 - val_loss: 0.3474 - val_accuracy: 0.8623\n",
      "Epoch 9/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3330 - accuracy: 0.8785 - val_loss: 0.3429 - val_accuracy: 0.8623\n",
      "Epoch 10/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3322 - accuracy: 0.8754 - val_loss: 0.3435 - val_accuracy: 0.8659\n",
      "Epoch 11/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3314 - accuracy: 0.8754 - val_loss: 0.3438 - val_accuracy: 0.8587\n",
      "Epoch 12/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3281 - accuracy: 0.8723 - val_loss: 0.3442 - val_accuracy: 0.8623\n",
      "Epoch 13/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3300 - accuracy: 0.8738 - val_loss: 0.3449 - val_accuracy: 0.8623\n",
      "Epoch 14/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3291 - accuracy: 0.8707 - val_loss: 0.3468 - val_accuracy: 0.8659\n",
      "Epoch 15/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3293 - accuracy: 0.8692 - val_loss: 0.3550 - val_accuracy: 0.8514\n",
      "Epoch 16/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3300 - accuracy: 0.8723 - val_loss: 0.3495 - val_accuracy: 0.8623\n",
      "Epoch 17/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3312 - accuracy: 0.8738 - val_loss: 0.3512 - val_accuracy: 0.8623\n",
      "Epoch 18/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3280 - accuracy: 0.8754 - val_loss: 0.3645 - val_accuracy: 0.8551\n",
      "Epoch 19/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3306 - accuracy: 0.8754 - val_loss: 0.3449 - val_accuracy: 0.8587\n",
      "Epoch 20/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3305 - accuracy: 0.8692 - val_loss: 0.3438 - val_accuracy: 0.8623\n",
      "Epoch 21/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3295 - accuracy: 0.8676 - val_loss: 0.3451 - val_accuracy: 0.8659\n",
      "Epoch 22/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3263 - accuracy: 0.8738 - val_loss: 0.3789 - val_accuracy: 0.8442\n",
      "Epoch 23/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3299 - accuracy: 0.8738 - val_loss: 0.3439 - val_accuracy: 0.8623\n",
      "Epoch 24/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3303 - accuracy: 0.8738 - val_loss: 0.3445 - val_accuracy: 0.8623\n",
      "Epoch 25/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3279 - accuracy: 0.8692 - val_loss: 0.3573 - val_accuracy: 0.8514\n",
      "Epoch 26/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3298 - accuracy: 0.8769 - val_loss: 0.3480 - val_accuracy: 0.8623\n",
      "Epoch 27/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3289 - accuracy: 0.8723 - val_loss: 0.3449 - val_accuracy: 0.8587\n",
      "Epoch 28/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3280 - accuracy: 0.8723 - val_loss: 0.3566 - val_accuracy: 0.8514\n",
      "Epoch 29/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3308 - accuracy: 0.8738 - val_loss: 0.3507 - val_accuracy: 0.8659\n",
      "Epoch 30/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3286 - accuracy: 0.8707 - val_loss: 0.3557 - val_accuracy: 0.8514\n",
      "Epoch 31/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3358 - accuracy: 0.8707 - val_loss: 0.3541 - val_accuracy: 0.8623\n",
      "Epoch 32/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3295 - accuracy: 0.8707 - val_loss: 0.3624 - val_accuracy: 0.8551\n",
      "Epoch 33/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3276 - accuracy: 0.8723 - val_loss: 0.3626 - val_accuracy: 0.8551\n",
      "Epoch 34/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3330 - accuracy: 0.8785 - val_loss: 0.3624 - val_accuracy: 0.8551\n",
      "Epoch 35/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3313 - accuracy: 0.8723 - val_loss: 0.3543 - val_accuracy: 0.8623\n",
      "Epoch 36/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3326 - accuracy: 0.8676 - val_loss: 0.3457 - val_accuracy: 0.8659\n",
      "Epoch 37/80\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.3292 - accuracy: 0.8738 - val_loss: 0.3541 - val_accuracy: 0.8587\n",
      "Epoch 38/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3322 - accuracy: 0.8738 - val_loss: 0.3444 - val_accuracy: 0.8623\n",
      "Epoch 39/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3283 - accuracy: 0.8785 - val_loss: 0.3449 - val_accuracy: 0.8623\n",
      "Epoch 40/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3310 - accuracy: 0.8707 - val_loss: 0.3500 - val_accuracy: 0.8659\n",
      "Epoch 41/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3283 - accuracy: 0.8754 - val_loss: 0.3488 - val_accuracy: 0.8659\n",
      "Epoch 42/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3273 - accuracy: 0.8785 - val_loss: 0.3667 - val_accuracy: 0.8478\n",
      "Epoch 43/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3270 - accuracy: 0.8738 - val_loss: 0.3444 - val_accuracy: 0.8623\n",
      "Epoch 44/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3296 - accuracy: 0.8676 - val_loss: 0.3472 - val_accuracy: 0.8623\n",
      "Epoch 45/80\n",
      "161/161 [==============================] - 1s 6ms/step - loss: 0.3280 - accuracy: 0.8645 - val_loss: 0.3441 - val_accuracy: 0.8623\n",
      "Epoch 46/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3308 - accuracy: 0.8692 - val_loss: 0.3456 - val_accuracy: 0.8551\n",
      "Epoch 47/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3279 - accuracy: 0.8754 - val_loss: 0.3477 - val_accuracy: 0.8587\n",
      "Epoch 48/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3301 - accuracy: 0.8723 - val_loss: 0.3575 - val_accuracy: 0.8514\n",
      "Epoch 49/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3297 - accuracy: 0.8723 - val_loss: 0.3440 - val_accuracy: 0.8659\n",
      "Epoch 50/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3332 - accuracy: 0.8707 - val_loss: 0.3596 - val_accuracy: 0.8478\n",
      "Epoch 51/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3301 - accuracy: 0.8738 - val_loss: 0.3557 - val_accuracy: 0.8514\n",
      "Epoch 52/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3295 - accuracy: 0.8738 - val_loss: 0.3587 - val_accuracy: 0.8514\n",
      "Epoch 53/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3256 - accuracy: 0.8785 - val_loss: 0.3835 - val_accuracy: 0.8406\n",
      "Epoch 54/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3301 - accuracy: 0.8754 - val_loss: 0.3502 - val_accuracy: 0.8659\n",
      "Epoch 55/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3283 - accuracy: 0.8738 - val_loss: 0.3435 - val_accuracy: 0.8623\n",
      "Epoch 56/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3285 - accuracy: 0.8754 - val_loss: 0.3452 - val_accuracy: 0.8659\n",
      "Epoch 57/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3309 - accuracy: 0.8676 - val_loss: 0.3447 - val_accuracy: 0.8587\n",
      "Epoch 58/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3302 - accuracy: 0.8769 - val_loss: 0.3477 - val_accuracy: 0.8587\n",
      "Epoch 59/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3283 - accuracy: 0.8738 - val_loss: 0.3446 - val_accuracy: 0.8659\n",
      "Epoch 60/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3287 - accuracy: 0.8692 - val_loss: 0.3444 - val_accuracy: 0.8659\n",
      "Epoch 61/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3281 - accuracy: 0.8769 - val_loss: 0.3610 - val_accuracy: 0.8478\n",
      "Epoch 62/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3296 - accuracy: 0.8707 - val_loss: 0.3458 - val_accuracy: 0.8587\n",
      "Epoch 63/80\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3283 - accuracy: 0.8629 - val_loss: 0.3447 - val_accuracy: 0.8623\n",
      "Epoch 64/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3275 - accuracy: 0.8738 - val_loss: 0.3453 - val_accuracy: 0.8623\n",
      "Epoch 65/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3322 - accuracy: 0.8754 - val_loss: 0.3475 - val_accuracy: 0.8659\n",
      "Epoch 66/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3270 - accuracy: 0.8707 - val_loss: 0.3446 - val_accuracy: 0.8696\n",
      "Epoch 67/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3284 - accuracy: 0.8785 - val_loss: 0.3585 - val_accuracy: 0.8478\n",
      "Epoch 68/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3282 - accuracy: 0.8707 - val_loss: 0.3481 - val_accuracy: 0.8659\n",
      "Epoch 69/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3280 - accuracy: 0.8738 - val_loss: 0.3510 - val_accuracy: 0.8659\n",
      "Epoch 70/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3290 - accuracy: 0.8738 - val_loss: 0.3621 - val_accuracy: 0.8478\n",
      "Epoch 71/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3305 - accuracy: 0.8692 - val_loss: 0.3508 - val_accuracy: 0.8659\n",
      "Epoch 72/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3303 - accuracy: 0.8707 - val_loss: 0.3529 - val_accuracy: 0.8514\n",
      "Epoch 73/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3307 - accuracy: 0.8676 - val_loss: 0.3516 - val_accuracy: 0.8623\n",
      "Epoch 74/80\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.3321 - accuracy: 0.8754 - val_loss: 0.3451 - val_accuracy: 0.8623\n",
      "Epoch 75/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3271 - accuracy: 0.8692 - val_loss: 0.3606 - val_accuracy: 0.8478\n",
      "Epoch 76/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3329 - accuracy: 0.8692 - val_loss: 0.3488 - val_accuracy: 0.8587\n",
      "Epoch 77/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3293 - accuracy: 0.8723 - val_loss: 0.3472 - val_accuracy: 0.8587\n",
      "Epoch 78/80\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.3298 - accuracy: 0.8738 - val_loss: 0.3503 - val_accuracy: 0.8623\n",
      "Epoch 79/80\n",
      "161/161 [==============================] - 1s 5ms/step - loss: 0.3303 - accuracy: 0.8723 - val_loss: 0.3521 - val_accuracy: 0.8587\n",
      "Epoch 80/80\n",
      "161/161 [==============================] - 1s 4ms/step - loss: 0.3302 - accuracy: 0.8738 - val_loss: 0.3550 - val_accuracy: 0.8551\n"
     ]
    }
   ],
   "source": [
    "historyl2_temp=[]\n",
    "MLPl2_F1_temp=[]\n",
    "MLPl2_acc_temp=[]\n",
    "MLPl2_acctr_temp=[]\n",
    "MLPl2_recall_temp=[]\n",
    "\n",
    "for i in range(5):\n",
    "  if i==0:\n",
    "    seed=100\n",
    "  elif i==1:\n",
    "    seed=123\n",
    "  elif i==2:\n",
    "    seed=200\n",
    "  elif i==3:\n",
    "    seed=231\n",
    "  else:\n",
    "    seed=321\n",
    "\n",
    "  xtrain_1, xtest_1, ytrain_1, ytest_1 = model_selection.train_test_split(x, y, stratify = y, random_state = seed,train_size = 0.7, test_size = 0.3)\n",
    "\n",
    "  ytrain_1 = to_categorical(ytrain_1, num_classes = None)\n",
    "  ytest_1 = to_categorical(ytest_1, num_classes = None)\n",
    "\n",
    "  history=model_4.fit(xtrain_1, ytrain_1, validation_data=(xtest_1, ytest_1),epochs=80, batch_size=4)\n",
    "  history_temp.append(history)\n",
    "\n",
    "  y_pred = model_4.predict(xtest_1,batch_size=None)\n",
    "  y_pred_train = model_4.predict(xtrain_1,batch_size=None)\n",
    "  ypredbool = np.argmax(y_pred, axis=1)\n",
    "  ypredbool = to_categorical(ypredbool, num_classes = None)\n",
    "  ypredbooltr = np.argmax(y_pred_train, axis=1)\n",
    "  ypredbooltr = to_categorical(ypredbooltr, num_classes = None)\n",
    "  MLPl2_f1_score_1 = metrics.f1_score(ytest_1, ypredbool, average=None)\n",
    "  MLPl2_acc_score_1=metrics.accuracy_score(ytest_1, ypredbool)\n",
    "  MLPl2_acc_score_train_1=metrics.accuracy_score(ytrain_1, ypredbooltr)\n",
    "  MLPl2_recall_score_1 = metrics.recall_score(ytest_1, ypredbool, average=None)\n",
    "\n",
    "  MLPl2_F1_temp.append(MLPl2_f1_score_1[1])\n",
    "  MLPl2_acc_temp.append(MLPl2_acc_score_1)\n",
    "  MLPl2_acctr_temp.append(MLPl2_acc_score_train_1)\n",
    "  MLPl2_recall_temp.append(MLPl2_recall_score_1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "0E2mnf6FDVVB",
    "outputId": "6dbf2754-c130-44b7-e59f-63b81382b86d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8169014084507041, 0.877076411960133, 0.8924050632911392, 0.9022082018927446, 0.8773006134969324]\n",
      "[0.8115942028985508, 0.8659420289855072, 0.8768115942028986, 0.8876811594202898, 0.855072463768116]\n",
      "[0.8738317757009346, 0.8644859813084113, 0.8738317757009346, 0.8660436137071651, 0.8769470404984424]\n",
      "[0.7581699346405228, 0.8627450980392157, 0.9215686274509803, 0.934640522875817, 0.934640522875817]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-85783dcf-76e2-43b3-84f3-40097cd89a7d\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Train accuracy</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>diferent of Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.758170</td>\n",
       "      <td>0.811594</td>\n",
       "      <td>0.873832</td>\n",
       "      <td>0.062238</td>\n",
       "      <td>0.816901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.865942</td>\n",
       "      <td>0.864486</td>\n",
       "      <td>-0.001456</td>\n",
       "      <td>0.877076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.876812</td>\n",
       "      <td>0.873832</td>\n",
       "      <td>-0.002980</td>\n",
       "      <td>0.892405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>231</td>\n",
       "      <td>0.934641</td>\n",
       "      <td>0.887681</td>\n",
       "      <td>0.866044</td>\n",
       "      <td>-0.021638</td>\n",
       "      <td>0.902208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321</td>\n",
       "      <td>0.934641</td>\n",
       "      <td>0.855072</td>\n",
       "      <td>0.876947</td>\n",
       "      <td>0.021875</td>\n",
       "      <td>0.877301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-85783dcf-76e2-43b3-84f3-40097cd89a7d')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-85783dcf-76e2-43b3-84f3-40097cd89a7d button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-85783dcf-76e2-43b3-84f3-40097cd89a7d');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   seed    Recall  Train accuracy  Accuracy  diferent of Accuracy        F1\n",
       "0   100  0.758170        0.811594  0.873832              0.062238  0.816901\n",
       "1   123  0.862745        0.865942  0.864486             -0.001456  0.877076\n",
       "2   200  0.921569        0.876812  0.873832             -0.002980  0.892405\n",
       "3   231  0.934641        0.887681  0.866044             -0.021638  0.902208\n",
       "4   321  0.934641        0.855072  0.876947              0.021875  0.877301"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(MLPl2_F1_temp)\n",
    "print(MLPl2_acc_temp)\n",
    "print(MLPl2_acctr_temp)\n",
    "print(MLPl2_recall_temp)\n",
    "\n",
    "delta1=MLPl2_acctr_temp[0]-MLPl2_acc_temp[0]\n",
    "delta2=MLPl2_acctr_temp[1]-MLPl2_acc_temp[1]\n",
    "delta3=MLPl2_acctr_temp[2]-MLPl2_acc_temp[2]\n",
    "delta4=MLPl2_acctr_temp[3]-MLPl2_acc_temp[3]\n",
    "delta5=MLPl2_acctr_temp[4]-MLPl2_acc_temp[4]\n",
    "MLPl2_delta_temp=[delta1,delta2,delta3,delta4,delta5]\n",
    "\n",
    "MLPl2=pd.DataFrame({'seed':[100,123,200,231,321],'Recall':[MLPl2_recall_temp[0],MLPl2_recall_temp[1],MLPl2_recall_temp[2],MLPl2_recall_temp[3],\n",
    "                                                     MLPl2_recall_temp[4]],'Train accuracy':[MLPl2_acc_temp[0],MLPl2_acc_temp[1],MLPl2_acc_temp[2],\n",
    "                                                    MLPl2_acc_temp[3],MLPl2_acc_temp[4]],'Accuracy':[MLPl2_acctr_temp[0],MLPl2_acctr_temp[1],MLPl2_acctr_temp[2],\n",
    "                                                    MLPl2_acctr_temp[3],MLPl2_acctr_temp[4]],'diferent of Accuracy':[delta1,delta2,delta3,delta4,delta5],'F1':[MLPl2_F1_temp[0],MLPl2_F1_temp[1],MLPl2_F1_temp[2],\n",
    "                                                    MLPl2_F1_temp[3],MLPl2_F1_temp[4]]})\n",
    "MLPl2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zx5tz_2D1yW"
   },
   "source": [
    "#Result from MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1FFIuDnWEoGh",
    "outputId": "84dc8d4b-9073-4185-d479-4faebe1c9310"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only MLP \n",
      "\n",
      "   seed    Recall  Train accuracy  Accuracy  diferent of Accuracy        F1\n",
      "0   100  0.803922        0.815217  0.887850              0.072633  0.828283\n",
      "1   123  0.895425        0.876812  0.872274             -0.004537  0.889610\n",
      "2   200  0.921569        0.873188  0.881620              0.008432  0.889590\n",
      "3   231  0.915033        0.884058  0.873832             -0.010226  0.897436\n",
      "4   321  0.934641        0.873188  0.894081              0.020893  0.890966 \n",
      "\n",
      "MLP with only drop out layer \n",
      "\n",
      "   seed    Recall  Train accuracy  Accuracy  diferent of Accuracy        F1\n",
      "0   100  0.836601        0.833333  0.884735              0.051402  0.847682\n",
      "1   123  0.908497        0.880435  0.876947             -0.003488  0.893891\n",
      "2   200  0.947712        0.887681  0.875389             -0.012292  0.903427\n",
      "3   231  0.921569        0.891304  0.870717             -0.020588  0.903846\n",
      "4   321  0.928105        0.862319  0.886293              0.023974  0.881988 \n",
      "\n",
      "MLP with only l2 \n",
      "\n",
      "   seed    Recall  Train accuracy  Accuracy  diferent of Accuracy        F1\n",
      "0   100  0.758170        0.811594  0.873832              0.062238  0.816901\n",
      "1   123  0.862745        0.865942  0.864486             -0.001456  0.877076\n",
      "2   200  0.921569        0.876812  0.873832             -0.002980  0.892405\n",
      "3   231  0.934641        0.887681  0.866044             -0.021638  0.902208\n",
      "4   321  0.934641        0.855072  0.876947              0.021875  0.877301 \n",
      "\n",
      "MLP with drop out layer & l2 \n",
      "\n",
      "   seed    Recall  Train accuracy  Accuracy  diferent of Accuracy        F1\n",
      "0   100  0.869281        0.851449  0.875389              0.023940  0.866450\n",
      "1   123  0.915033        0.876812  0.869159             -0.007653  0.891720\n",
      "2   200  0.947712        0.869565  0.870717              0.001151  0.889571\n",
      "3   231  0.928105        0.880435  0.864486             -0.015949  0.895899\n",
      "4   321  0.941176        0.858696  0.881620              0.022924  0.880734 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Only MLP\",\"\\n\")\n",
    "print(MLP0,\"\\n\")\n",
    "print(\"MLP with only drop out layer\",\"\\n\")\n",
    "print(MLPd,\"\\n\")\n",
    "print(\"MLP with only l2\",\"\\n\")\n",
    "print(MLPl2,\"\\n\")\n",
    "print(\"MLP with drop out layer & l2\",\"\\n\")\n",
    "print(MLP,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "rqJ85tR2GnxS",
    "outputId": "ebd74d18-2bb7-4e40-d807-52816863c88a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall mean & S.D. \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-8d575d9c-34f8-4e59-ba14-599706dbb9d1\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLP method</th>\n",
       "      <th>Mean</th>\n",
       "      <th>S.D.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Only MLP</td>\n",
       "      <td>0.894118</td>\n",
       "      <td>0.052369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP with drop out</td>\n",
       "      <td>0.908497</td>\n",
       "      <td>0.042609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP with l2</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.075518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP with l2 &amp; drop out</td>\n",
       "      <td>0.920261</td>\n",
       "      <td>0.031140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d575d9c-34f8-4e59-ba14-599706dbb9d1')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-8d575d9c-34f8-4e59-ba14-599706dbb9d1 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-8d575d9c-34f8-4e59-ba14-599706dbb9d1');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "               MLP method      Mean      S.D.\n",
       "0                Only MLP  0.894118  0.052369\n",
       "1       MLP with drop out  0.908497  0.042609\n",
       "2             MLP with l2  0.882353  0.075518\n",
       "3  MLP with l2 & drop out  0.920261  0.031140"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP0_xb=statistics.mean(MLP0_recall_temp)\n",
    "MLPd_xb=statistics.mean(MLPd_recall_temp)\n",
    "MLPl2_xb=statistics.mean(MLPl2_recall_temp)\n",
    "MLP_xb=statistics.mean(MLP_recall_temp)\n",
    "\n",
    "MLP0_sd=statistics.stdev(MLP0_recall_temp)\n",
    "MLPd_sd=statistics.stdev(MLPd_recall_temp)\n",
    "MLPl2_sd=statistics.stdev(MLPl2_recall_temp)\n",
    "MLP_sd=statistics.stdev(MLP_recall_temp)\n",
    "\n",
    "MLP_recall_xb_sd_frame=pd.DataFrame({'MLP method':[\"Only MLP\",\"MLP with drop out\",\"MLP with l2\",\"MLP with l2 & drop out\"],'Mean':[MLP0_xb,MLPd_xb,MLPl2_xb,MLP_xb],\n",
    "                              'S.D.':[MLP0_sd,MLPd_sd,MLPl2_sd,MLP_sd]})\n",
    "                         \n",
    "\n",
    "MLP0_xb=statistics.mean(MLP0_F1_temp)\n",
    "MLPd_xb=statistics.mean(MLPd_F1_temp)\n",
    "MLPl2_xb=statistics.mean(MLPl2_F1_temp)\n",
    "MLP_xb=statistics.mean(MLP_F1_temp)\n",
    "\n",
    "MLP0_sd=statistics.stdev(MLP0_F1_temp)\n",
    "MLPd_sd=statistics.stdev(MLPd_F1_temp)\n",
    "MLPl2_sd=statistics.stdev(MLPl2_F1_temp)\n",
    "MLP_sd=statistics.stdev(MLP_F1_temp)\n",
    "\n",
    "MLP_F1_xb_sd_frame=pd.DataFrame({'MLP method':[\"Only MLP\",\"MLP with drop out\",\"MLP with l2\",\"MLP with l2 & drop out\"],'Mean':[MLP0_xb,MLPd_xb,MLPl2_xb,MLP_xb],\n",
    "                              'S.D.':[MLP0_sd,MLPd_sd,MLPl2_sd,MLP_sd]})\n",
    "   \n",
    "\n",
    "MLP0_xb=statistics.mean(MLP0_acctr_temp)\n",
    "MLPd_xb=statistics.mean(MLPd_acctr_temp)\n",
    "MLPl2_xb=statistics.mean(MLPl2_acctr_temp)\n",
    "MLP_xb=statistics.mean(MLP_acctr_temp)\n",
    "\n",
    "MLP0_sd=statistics.stdev(MLP0_acctr_temp)\n",
    "MLPd_sd=statistics.stdev(MLPd_acctr_temp)\n",
    "MLPl2_sd=statistics.stdev(MLPl2_acctr_temp)\n",
    "MLP_sd=statistics.stdev(MLP_acctr_temp)\n",
    "\n",
    "MLP_acctr_xb_sd_frame=pd.DataFrame({'MLP method':[\"Only MLP\",\"MLP with drop out\",\"MLP with l2\",\"MLP with l2 & drop out\"],'Mean':[MLP0_xb,MLPd_xb,MLPl2_xb,MLP_xb],\n",
    "                              'S.D.':[MLP0_sd,MLPd_sd,MLPl2_sd,MLP_sd]})\n",
    "      \n",
    "\n",
    "MLP0_xb=statistics.mean(MLP0_acc_temp)\n",
    "MLPd_xb=statistics.mean(MLPd_acc_temp)\n",
    "MLPl2_xb=statistics.mean(MLPl2_acc_temp)\n",
    "MLP_xb=statistics.mean(MLP_acc_temp)\n",
    "\n",
    "MLP0_sd=statistics.stdev(MLP0_acc_temp)\n",
    "MLPd_sd=statistics.stdev(MLPd_acc_temp)\n",
    "MLPl2_sd=statistics.stdev(MLPl2_acc_temp)\n",
    "MLP_sd=statistics.stdev(MLP_acc_temp)\n",
    "\n",
    "MLP_acc_xb_sd_frame=pd.DataFrame({'MLP method':[\"Only MLP\",\"MLP with drop out\",\"MLP with l2\",\"MLP with l2 & drop out\"],'Mean':[MLP0_xb,MLPd_xb,MLPl2_xb,MLP_xb],\n",
    "                              'S.D.':[MLP0_sd,MLPd_sd,MLPl2_sd,MLP_sd]})\n",
    "\n",
    "\n",
    "MLP0_xb=statistics.mean(MLP0_delta_temp)\n",
    "MLPd_xb=statistics.mean(MLPd_delta_temp)\n",
    "MLPl2_xb=statistics.mean(MLPl2_delta_temp)\n",
    "MLP_xb=statistics.mean(MLP_delta_temp)\n",
    "\n",
    "MLP0_sd=statistics.stdev(MLP0_delta_temp)\n",
    "MLPd_sd=statistics.stdev(MLPd_delta_temp)\n",
    "MLPl2_sd=statistics.stdev(MLPl2_delta_temp)\n",
    "MLP_sd=statistics.stdev(MLP_delta_temp)\n",
    "\n",
    "MLP_delta_xb_sd_frame=pd.DataFrame({'MLP method':[\"Only MLP\",\"MLP with drop out\",\"MLP with l2\",\"MLP with l2 & drop out\"],'Mean':[MLP0_xb,MLPd_xb,MLPl2_xb,MLP_xb],\n",
    "                              'S.D.':[MLP0_sd,MLPd_sd,MLPl2_sd,MLP_sd]})\n",
    "print(\"Recall mean & S.D. \\n\") \n",
    "MLP_recall_xb_sd_frame      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "RVRyMLlscRUB",
    "outputId": "3f0d1a41-6ebb-472d-9225-0f62b4b6036e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1mean & S.D. \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-d0838177-40e2-4b7b-a842-a08033780b89\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLP method</th>\n",
       "      <th>Mean</th>\n",
       "      <th>S.D.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Only MLP</td>\n",
       "      <td>0.879177</td>\n",
       "      <td>0.028635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP with drop out</td>\n",
       "      <td>0.886167</td>\n",
       "      <td>0.023284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP with l2</td>\n",
       "      <td>0.873178</td>\n",
       "      <td>0.033210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP with l2 &amp; drop out</td>\n",
       "      <td>0.884875</td>\n",
       "      <td>0.011695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d0838177-40e2-4b7b-a842-a08033780b89')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-d0838177-40e2-4b7b-a842-a08033780b89 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-d0838177-40e2-4b7b-a842-a08033780b89');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "               MLP method      Mean      S.D.\n",
       "0                Only MLP  0.879177  0.028635\n",
       "1       MLP with drop out  0.886167  0.023284\n",
       "2             MLP with l2  0.873178  0.033210\n",
       "3  MLP with l2 & drop out  0.884875  0.011695"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"F1 mean & S.D. \\n\") \n",
    "MLP_F1_xb_sd_frame   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "tn8to6cKckhk",
    "outputId": "acc4f6c3-5cc1-4a39-8c92-7669bf037701"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-accuracy mean & S.D. \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-10f20df2-d19c-4413-9f89-2901de96fce9\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLP method</th>\n",
       "      <th>Mean</th>\n",
       "      <th>S.D.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Only MLP</td>\n",
       "      <td>0.881931</td>\n",
       "      <td>0.009241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP with drop out</td>\n",
       "      <td>0.878816</td>\n",
       "      <td>0.006553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP with l2</td>\n",
       "      <td>0.871028</td>\n",
       "      <td>0.005441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP with l2 &amp; drop out</td>\n",
       "      <td>0.872274</td>\n",
       "      <td>0.006516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-10f20df2-d19c-4413-9f89-2901de96fce9')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-10f20df2-d19c-4413-9f89-2901de96fce9 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-10f20df2-d19c-4413-9f89-2901de96fce9');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "               MLP method      Mean      S.D.\n",
       "0                Only MLP  0.881931  0.009241\n",
       "1       MLP with drop out  0.878816  0.006553\n",
       "2             MLP with l2  0.871028  0.005441\n",
       "3  MLP with l2 & drop out  0.872274  0.006516"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Train-accuracy mean & S.D. \\n\") \n",
    "MLP_acctr_xb_sd_frame   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "y_kXjr3Ycr0_",
    "outputId": "5a1e4f25-1155-45af-8c6a-d82d6a904e88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy mean & S.D. \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-516bfc95-3981-4239-8f80-5962b2bb6440\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLP method</th>\n",
       "      <th>Mean</th>\n",
       "      <th>S.D.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Only MLP</td>\n",
       "      <td>0.864493</td>\n",
       "      <td>0.027901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP with drop out</td>\n",
       "      <td>0.871014</td>\n",
       "      <td>0.023842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP with l2</td>\n",
       "      <td>0.859420</td>\n",
       "      <td>0.029368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP with l2 &amp; drop out</td>\n",
       "      <td>0.867391</td>\n",
       "      <td>0.012180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-516bfc95-3981-4239-8f80-5962b2bb6440')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-516bfc95-3981-4239-8f80-5962b2bb6440 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-516bfc95-3981-4239-8f80-5962b2bb6440');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "               MLP method      Mean      S.D.\n",
       "0                Only MLP  0.864493  0.027901\n",
       "1       MLP with drop out  0.871014  0.023842\n",
       "2             MLP with l2  0.859420  0.029368\n",
       "3  MLP with l2 & drop out  0.867391  0.012180"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Accuracy mean & S.D. \\n\") \n",
    "MLP_acc_xb_sd_frame   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "7gViOdUecu8O",
    "outputId": "82260bf8-d2cb-46b2-dd53-40c1bd9d6189"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different of accuracy mean & S.D. \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-bb55fc00-77c2-4745-bc27-7e2d332c09d6\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLP method</th>\n",
       "      <th>Mean</th>\n",
       "      <th>S.D.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Only MLP</td>\n",
       "      <td>0.017439</td>\n",
       "      <td>0.033120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP with drop out</td>\n",
       "      <td>0.007802</td>\n",
       "      <td>0.029579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP with l2</td>\n",
       "      <td>0.011608</td>\n",
       "      <td>0.032239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP with l2 &amp; drop out</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.017984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bb55fc00-77c2-4745-bc27-7e2d332c09d6')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-bb55fc00-77c2-4745-bc27-7e2d332c09d6 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-bb55fc00-77c2-4745-bc27-7e2d332c09d6');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "               MLP method      Mean      S.D.\n",
       "0                Only MLP  0.017439  0.033120\n",
       "1       MLP with drop out  0.007802  0.029579\n",
       "2             MLP with l2  0.011608  0.032239\n",
       "3  MLP with l2 & drop out  0.004883  0.017984"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Different of accuracy mean & S.D. \\n\") \n",
    "MLP_delta_xb_sd_frame   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqCoIKWFLqQB"
   },
   "source": [
    "#Data manage other method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "GYqFa-9dLp-T"
   },
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('/content/drive/MyDrive/Ai data/heart.csv')\n",
    "\n",
    "binary_vars_list=['Sex','ExerciseAngina']\n",
    "def binary_map(x):\n",
    "    return x.map({'Y':1,'N':0,'M':1,'F':0})\n",
    "    \n",
    "\n",
    "#applying the function to the housing variables list\n",
    "data1[binary_vars_list] = data1[binary_vars_list].apply(binary_map)\n",
    "\n",
    "# Defining the map function\n",
    "def dummies(x,df):\n",
    "    temp = pd.get_dummies(df[x], drop_first = False)\n",
    "    df = pd.concat([df, temp], axis = 1)\n",
    "    df.drop([x], axis = 1, inplace = True)\n",
    "    return df\n",
    "# Applying the function to the data\n",
    "\n",
    "data1 = dummies('ChestPainType',data1)\n",
    "data1 = dummies('RestingECG',data1)\n",
    "data1= dummies('ST_Slope',data1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "numeric_vars = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']\n",
    "data1[numeric_vars] = scaler.fit_transform(data1[numeric_vars])\n",
    "\n",
    "data1.drop(['TA'], axis = 1, inplace = True)\n",
    "data1.drop(['LVH'], axis = 1, inplace = True)\n",
    "data1.drop(['Normal'], axis = 1, inplace = True)\n",
    "data1.drop(['ST'], axis = 1, inplace = True)\n",
    "data1.drop(['RestingBP'], axis = 1, inplace = True)\n",
    "data1.drop(['Down'], axis = 1, inplace = True)\n",
    "\n",
    "x1 = data1.drop('HeartDisease',axis=1)\n",
    "y1 = data1['HeartDisease']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1Og7BLOLJQ5"
   },
   "source": [
    "#Other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "TifSfGdVLNN2"
   },
   "outputs": [],
   "source": [
    "lr_F1_temp=[]\n",
    "lr_acc_temp=[]\n",
    "lr_acctr_temp=[]\n",
    "lr_recall_temp=[]\n",
    "lr_delta_temp=[]\n",
    "\n",
    "knn_F1_temp=[]\n",
    "knn_acc_temp=[]\n",
    "knn_acctr_temp=[]\n",
    "knn_recall_temp=[]\n",
    "knn_delta_temp=[]\n",
    "\n",
    "dt_F1_temp=[]\n",
    "dt_acc_temp=[]\n",
    "dt_acctr_temp=[]\n",
    "dt_recall_temp=[]\n",
    "dt_delta_temp=[]\n",
    "\n",
    "rf_F1_temp=[]\n",
    "rf_acc_temp=[]\n",
    "rf_acctr_temp=[]\n",
    "rf_recall_temp=[]\n",
    "rf_delta_temp=[]\n",
    "\n",
    "nb_F1_temp=[]\n",
    "nb_acc_temp=[]\n",
    "nb_acctr_temp=[]\n",
    "nb_recall_temp=[]\n",
    "nb_delta_temp=[]\n",
    "\n",
    "svc_F1_temp=[]\n",
    "svc_acc_temp=[]\n",
    "svc_acctr_temp=[]\n",
    "svc_recall_temp=[]\n",
    "svc_delta_temp=[]\n",
    "\n",
    "for i in range(5):\n",
    "  if i==0:\n",
    "    seed=100\n",
    "  elif i==1:\n",
    "    seed=123\n",
    "  elif i==2:\n",
    "    seed=200\n",
    "  elif i==3:\n",
    "    seed=231\n",
    "  else:\n",
    "    seed=321\n",
    "\n",
    "  xtrain_1, xtest_1, ytrain_1, ytest_1 = model_selection.train_test_split(x, y, stratify = y, random_state = seed,train_size = 0.7, test_size = 0.3)\n",
    "\n",
    "  #logistic Regression\n",
    "  logreg = LogisticRegression()\n",
    "  logreg.fit(xtrain_1, ytrain_1)\n",
    "  y_pred_log = logreg.predict(xtest_1)\n",
    "  y_pred_log_train = logreg.predict(xtrain_1)\n",
    "  lr_acc_score=metrics.accuracy_score(ytest_1, y_pred_log)\n",
    "  lr_recall_score=metrics.recall_score(ytest_1, y_pred_log, average=None)\n",
    "  lr_acc_score_train=metrics.accuracy_score(ytrain_1, y_pred_log_train)\n",
    "  lr_f1_score=metrics.f1_score(ytest_1, y_pred_log, average=None)\n",
    "  deltacc=lr_acc_score_train-lr_acc_score \n",
    "\n",
    "  lr_delta_temp.append(deltacc)\n",
    "  lr_F1_temp.append(lr_f1_score[1])\n",
    "  lr_acc_temp.append(lr_acc_score)\n",
    "  lr_acctr_temp.append(lr_acc_score_train)\n",
    "  lr_recall_temp.append(lr_recall_score[1])\n",
    "\n",
    "  #KNN\n",
    "  knn = KNeighborsClassifier()\n",
    "  knn.fit(xtrain_1, ytrain_1)\n",
    "  y_pred_knn = knn.predict(xtest_1)\n",
    "  y_pred_knn_train = knn.predict(xtrain_1)\n",
    "  knn_acc_score=metrics.accuracy_score(ytrain_1, y_pred_knn_train)\n",
    "  knn_acc_score_train=metrics.accuracy_score(ytest_1, y_pred_knn)\n",
    "  knn_recall_score=metrics.recall_score(ytest_1, y_pred_knn, average=None)\n",
    "  knn_f1_score=metrics.f1_score(ytest_1, y_pred_knn, average=None)\n",
    "  deltacc=knn_acc_score_train-knn_acc_score \n",
    "  \n",
    "  knn_delta_temp.append(deltacc)\n",
    "  knn_F1_temp.append(knn_f1_score[1])\n",
    "  knn_acc_temp.append(knn_acc_score)\n",
    "  knn_acctr_temp.append(knn_acc_score_train)\n",
    "  knn_recall_temp.append(knn_recall_score[1])\n",
    " \n",
    "  #Decision tree\n",
    "  tree = DecisionTreeClassifier()\n",
    "  tree.fit(xtrain_1, ytrain_1)\n",
    "  y_pred_tree = tree.predict(xtest_1)\n",
    "  y_pred_tree_train = tree.predict(xtrain_1)\n",
    "  tree_acc_score=metrics.accuracy_score(ytest_1, y_pred_tree)\n",
    "  tree_acc_score_train=metrics.accuracy_score(ytrain_1, y_pred_tree_train)\n",
    "  tree_recall_score=metrics.recall_score(ytest_1, y_pred_tree, average=None)\n",
    "  tree_f1_score=metrics.f1_score(ytest_1, y_pred_tree, average=None)\n",
    "  deltacc=tree_acc_score_train-tree_acc_score \n",
    "  \n",
    "  dt_delta_temp.append(deltacc)\n",
    "  dt_F1_temp.append(tree_f1_score[1])\n",
    "  dt_acc_temp.append(tree_acc_score)\n",
    "  dt_acctr_temp.append(tree_acc_score_train)\n",
    "  dt_recall_temp.append(tree_recall_score[1])\n",
    "\n",
    "  #random forest\n",
    "  rfc = RandomForestClassifier()\n",
    "  rfc.fit(xtrain_1, ytrain_1)\n",
    "  y_pred_forest = rfc.predict(xtest_1)\n",
    "  y_pred_forest_train = rfc.predict(xtrain_1)\n",
    "  rfc_acc_score=metrics.accuracy_score(ytest_1, y_pred_forest)\n",
    "  rfc_acc_score_train=metrics.accuracy_score(ytrain_1, y_pred_forest_train)\n",
    "  rfc_recall_score=metrics.recall_score(ytest_1, y_pred_forest, average=None)\n",
    "  rfc_f1_score=metrics.f1_score(ytest_1, y_pred_forest, average=None)\n",
    "  deltacc=rfc_acc_score_train-rfc_acc_score \n",
    "  \n",
    "  rf_delta_temp.append(deltacc)\n",
    "  rf_F1_temp.append(rfc_f1_score[1])\n",
    "  rf_acc_temp.append(rfc_acc_score)\n",
    "  rf_acctr_temp.append(rfc_acc_score_train)\n",
    "  rf_recall_temp.append(rfc_recall_score[1])\n",
    "\n",
    "  #naive bayes\n",
    "  nb = GaussianNB()\n",
    "  nb.fit(xtrain_1,ytrain_1)\n",
    "  y_pred_nb = nb.predict(xtest_1)\n",
    "  y_pred_nb_train = nb.predict(xtrain_1)\n",
    "  nb_conf_matrix = confusion_matrix(ytest_1,y_pred_nb)\n",
    "  nb_acc_score = metrics.accuracy_score(ytest_1, y_pred_nb)\n",
    "  nb_acc_score_train = metrics.accuracy_score(ytrain_1, y_pred_nb_train)\n",
    "  nb_recall_score=metrics.recall_score(ytest_1, y_pred_nb, average=None)\n",
    "  nb_f1_score=metrics.f1_score(ytest_1, y_pred_nb, average=None)\n",
    "  deltacc=nb_acc_score_train-nb_acc_score \n",
    "  \n",
    "  nb_delta_temp.append(deltacc)\n",
    "  nb_F1_temp.append(nb_f1_score[1])\n",
    "  nb_acc_temp.append(nb_acc_score)\n",
    "  nb_acctr_temp.append(nb_acc_score_train)\n",
    "  nb_recall_temp.append(nb_recall_score[1])\n",
    "\n",
    "  #SVC\n",
    "  svc =  SVC(kernel='poly', C=2)\n",
    "  svc.fit(xtrain_1, ytrain_1)\n",
    "  y_pred_svc = svc.predict(xtest_1)\n",
    "  y_pred_svc_train = svc.predict(xtrain_1)\n",
    "  svc_conf_matrix = confusion_matrix(ytest_1, y_pred_svc)\n",
    "  svc_acc_score = metrics.accuracy_score(ytest_1, y_pred_svc)\n",
    "  svc_acc_score_train= metrics.accuracy_score(ytrain_1, y_pred_svc_train)\n",
    "  svc_recall_score=metrics.recall_score(ytest_1, y_pred_svc, average=None)\n",
    "  svc_f1_score = metrics.f1_score(ytest_1, y_pred_svc, average=None)\n",
    "  deltacc=svc_acc_score_train-svc_acc_score \n",
    "  \n",
    "  svc_delta_temp.append(deltacc)\n",
    "  svc_F1_temp.append(svc_f1_score[1])\n",
    "  svc_acc_temp.append(svc_acc_score)\n",
    "  svc_acctr_temp.append(svc_acc_score_train)\n",
    "  svc_recall_temp.append(svc_recall_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wrxkx3RST61"
   },
   "source": [
    "#Other methods result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "iCWqIDOCSSpa",
    "outputId": "0e45818a-e97c-42e5-b2a3-7e72e9e029bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare recall score\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-d90892f8-adf9-48a7-9a3f-6047a31c4bcf\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>100</th>\n",
       "      <th>123</th>\n",
       "      <th>200</th>\n",
       "      <th>231</th>\n",
       "      <th>300</th>\n",
       "      <th>mean</th>\n",
       "      <th>S.D.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic regression</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.908497</td>\n",
       "      <td>0.915033</td>\n",
       "      <td>0.915033</td>\n",
       "      <td>0.915033</td>\n",
       "      <td>0.895425</td>\n",
       "      <td>0.040290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.908497</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.915033</td>\n",
       "      <td>0.894118</td>\n",
       "      <td>0.029010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision tree</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.849673</td>\n",
       "      <td>0.771242</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.794771</td>\n",
       "      <td>0.034150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random forest</td>\n",
       "      <td>0.816993</td>\n",
       "      <td>0.915033</td>\n",
       "      <td>0.954248</td>\n",
       "      <td>0.908497</td>\n",
       "      <td>0.928105</td>\n",
       "      <td>0.904575</td>\n",
       "      <td>0.052001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NB</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.934641</td>\n",
       "      <td>0.908497</td>\n",
       "      <td>0.899346</td>\n",
       "      <td>0.035679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.915033</td>\n",
       "      <td>0.915033</td>\n",
       "      <td>0.899346</td>\n",
       "      <td>0.054410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d90892f8-adf9-48a7-9a3f-6047a31c4bcf')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-d90892f8-adf9-48a7-9a3f-6047a31c4bcf button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-d90892f8-adf9-48a7-9a3f-6047a31c4bcf');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                 model       100       123       200       231       300  \\\n",
       "0  Logistic regression  0.823529  0.908497  0.915033  0.915033  0.915033   \n",
       "1                  KNN  0.843137  0.901961  0.908497  0.901961  0.915033   \n",
       "2        Decision tree  0.764706  0.784314  0.849673  0.771242  0.803922   \n",
       "3        Random forest  0.816993  0.915033  0.954248  0.908497  0.928105   \n",
       "4                   NB  0.843137  0.888889  0.921569  0.934641  0.908497   \n",
       "5                  SVC  0.803922  0.921569  0.941176  0.915033  0.915033   \n",
       "\n",
       "       mean      S.D.  \n",
       "0  0.895425  0.040290  \n",
       "1  0.894118  0.029010  \n",
       "2  0.794771  0.034150  \n",
       "3  0.904575  0.052001  \n",
       "4  0.899346  0.035679  \n",
       "5  0.899346  0.054410  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_xd=statistics.mean(lr_recall_temp)\n",
    "knn_xd=statistics.mean(knn_recall_temp)\n",
    "dt_xd=statistics.mean(dt_recall_temp)\n",
    "rf_xd=statistics.mean(rf_recall_temp)\n",
    "nb_xd=statistics.mean(nb_recall_temp)\n",
    "svc_xd=statistics.mean(svc_recall_temp)\n",
    "\n",
    "lr_sd=statistics.stdev(lr_recall_temp)\n",
    "knn_sd=statistics.stdev(knn_recall_temp)\n",
    "dt_sd=statistics.stdev(dt_recall_temp)\n",
    "rf_sd=statistics.stdev(rf_recall_temp)\n",
    "nb_sd=statistics.stdev(nb_recall_temp)\n",
    "svc_sd=statistics.stdev(svc_recall_temp)\n",
    "\n",
    "ex_recall_res=pd.DataFrame({'model':['Logistic regression','KNN','Decision tree','Random forest','NB','SVC'],'100':[lr_recall_temp[0],knn_recall_temp[0],dt_recall_temp[0],\n",
    "                                                                                                            rf_recall_temp[0],nb_recall_temp[0],svc_recall_temp[0]],'123':[lr_recall_temp[1],knn_recall_temp[1],dt_recall_temp[1]\n",
    "                                                                                                            ,rf_recall_temp[1],nb_recall_temp[1],svc_recall_temp[1]],'200':[lr_recall_temp[2],knn_recall_temp[2],dt_recall_temp[2]\n",
    "                                                                                                            ,rf_recall_temp[2],nb_recall_temp[2],svc_recall_temp[2]],'231':[lr_recall_temp[3],knn_recall_temp[3],dt_recall_temp[3]\n",
    "                                                                                                             ,rf_recall_temp[3],nb_recall_temp[3],svc_recall_temp[3]],'300':[lr_recall_temp[4],knn_recall_temp[4],dt_recall_temp[4]\n",
    "                                                                                                             ,rf_recall_temp[4],nb_recall_temp[4],svc_recall_temp[4]],'mean':[lr_xd,knn_xd,dt_xd,rf_xd,\n",
    "                                                                                                              nb_xd,svc_xd],'S.D.':[lr_sd,knn_sd,dt_sd,rf_sd,\n",
    "                                                                                                              nb_sd,svc_sd]})\n",
    "\n",
    "lr_xd=statistics.mean(lr_F1_temp)\n",
    "knn_xd=statistics.mean(knn_F1_temp)\n",
    "dt_xd=statistics.mean(dt_F1_temp)\n",
    "rf_xd=statistics.mean(rf_F1_temp)\n",
    "nb_xd=statistics.mean(nb_F1_temp)\n",
    "svc_xd=statistics.mean(svc_F1_temp)\n",
    "\n",
    "lr_sd=statistics.stdev(lr_F1_temp)\n",
    "knn_sd=statistics.stdev(knn_F1_temp)\n",
    "dt_sd=statistics.stdev(dt_F1_temp)\n",
    "rf_sd=statistics.stdev(rf_F1_temp)\n",
    "nb_sd=statistics.stdev(nb_F1_temp)\n",
    "svc_sd=statistics.stdev(svc_F1_temp)\n",
    "\n",
    "ex_F1_res=pd.DataFrame({'model':['Logistic regression','KNN','Decision tree','Random forest','NB','SVC'],'100':[lr_F1_temp[0],knn_F1_temp[0],dt_F1_temp[0],\n",
    "                                                                                                            rf_F1_temp[0],nb_F1_temp[0],svc_F1_temp[0]],'123':[lr_F1_temp[1],knn_F1_temp[1],dt_F1_temp[1]\n",
    "                                                                                                            ,rf_F1_temp[1],nb_F1_temp[1],svc_F1_temp[1]],'200':[lr_F1_temp[2],knn_F1_temp[2],dt_F1_temp[2]\n",
    "                                                                                                            ,rf_F1_temp[2],nb_F1_temp[2],svc_F1_temp[2]],'231':[lr_F1_temp[3],knn_F1_temp[3],dt_F1_temp[3]\n",
    "                                                                                                             ,rf_F1_temp[3],nb_F1_temp[3],svc_F1_temp[3]],'300':[lr_F1_temp[4],knn_F1_temp[4],dt_F1_temp[4]\n",
    "                                                                                                             ,rf_F1_temp[4],nb_F1_temp[4],svc_F1_temp[4]],'mean':[lr_xd,knn_xd,dt_xd,rf_xd,\n",
    "                                                                                                              nb_xd,svc_xd],'S.D.':[lr_sd,knn_sd,dt_sd,rf_sd,\n",
    "                                                                                                              nb_sd,svc_sd]})\n",
    "\n",
    "lr_xd=statistics.mean(lr_acctr_temp)\n",
    "knn_xd=statistics.mean(knn_acctr_temp)\n",
    "dt_xd=statistics.mean(dt_acctr_temp)\n",
    "rf_xd=statistics.mean(rf_acctr_temp)\n",
    "nb_xd=statistics.mean(nb_acctr_temp)\n",
    "svc_xd=statistics.mean(svc_acctr_temp)\n",
    "\n",
    "lr_sd=statistics.stdev(lr_acctr_temp)\n",
    "knn_sd=statistics.stdev(knn_acctr_temp)\n",
    "dt_sd=statistics.stdev(dt_acctr_temp)\n",
    "rf_sd=statistics.stdev(rf_acctr_temp)\n",
    "nb_sd=statistics.stdev(nb_acctr_temp)\n",
    "svc_sd=statistics.stdev(svc_acctr_temp)\n",
    "\n",
    "ex_acctr_res=pd.DataFrame({'model':['Logistic regression','KNN','Decision tree','Random forest','NB','SVC'],'100':[lr_acctr_temp[0],knn_acctr_temp[0],dt_acctr_temp[0],\n",
    "                                                                                                            rf_acctr_temp[0],nb_acctr_temp[0],svc_acctr_temp[0]],'123':[lr_acctr_temp[1],knn_acctr_temp[1],dt_acctr_temp[1]\n",
    "                                                                                                            ,rf_acctr_temp[1],nb_acctr_temp[1],svc_acctr_temp[1]],'200':[lr_acctr_temp[2],knn_acctr_temp[2],dt_acctr_temp[2]\n",
    "                                                                                                            ,rf_acctr_temp[2],nb_acctr_temp[2],svc_acctr_temp[2]],'231':[lr_acctr_temp[3],knn_acctr_temp[3],dt_acctr_temp[3]\n",
    "                                                                                                             ,rf_acctr_temp[3],nb_acctr_temp[3],svc_acctr_temp[3]],'300':[lr_acctr_temp[4],knn_acctr_temp[4],dt_acctr_temp[4]\n",
    "                                                                                                             ,rf_acctr_temp[4],nb_acctr_temp[4],svc_acctr_temp[4]],'mean':[lr_xd,knn_xd,dt_xd,rf_xd,\n",
    "                                                                                                              nb_xd,svc_xd],'S.D.':[lr_sd,knn_sd,dt_sd,rf_sd,\n",
    "                                                                                                              nb_sd,svc_sd]})\n",
    "\n",
    "lr_xd=statistics.mean(lr_acc_temp)\n",
    "knn_xd=statistics.mean(knn_acc_temp)\n",
    "dt_xd=statistics.mean(dt_acc_temp)\n",
    "rf_xd=statistics.mean(rf_acc_temp)\n",
    "nb_xd=statistics.mean(nb_acc_temp)\n",
    "svc_xd=statistics.mean(svc_acc_temp)\n",
    "\n",
    "lr_sd=statistics.stdev(lr_acc_temp)\n",
    "knn_sd=statistics.stdev(knn_acc_temp)\n",
    "dt_sd=statistics.stdev(dt_acc_temp)\n",
    "rf_sd=statistics.stdev(rf_acc_temp)\n",
    "nb_sd=statistics.stdev(nb_acc_temp)\n",
    "svc_sd=statistics.stdev(svc_acc_temp)\n",
    "\n",
    "ex_acc_res=pd.DataFrame({'model':['Logistic regression','KNN','Decision tree','Random forest','NB','SVC'],'100':[lr_acc_temp[0],knn_acc_temp[0],dt_acc_temp[0],\n",
    "                                                                                                            rf_acc_temp[0],nb_acc_temp[0],svc_acc_temp[0]],'123':[lr_acc_temp[1],knn_acc_temp[1],dt_acc_temp[1]\n",
    "                                                                                                            ,rf_acc_temp[1],nb_acc_temp[1],svc_acc_temp[1]],'200':[lr_acc_temp[2],knn_acc_temp[2],dt_acc_temp[2]\n",
    "                                                                                                            ,rf_acc_temp[2],nb_acc_temp[2],svc_acc_temp[2]],'231':[lr_acc_temp[3],knn_acc_temp[3],dt_acc_temp[3]\n",
    "                                                                                                             ,rf_acc_temp[3],nb_acc_temp[3],svc_acc_temp[3]],'300':[lr_acc_temp[4],knn_acc_temp[4],dt_acc_temp[4]\n",
    "                                                                                                             ,rf_acc_temp[4],nb_acc_temp[4],svc_acc_temp[4]],'mean':[lr_xd,knn_xd,dt_xd,rf_xd,\n",
    "                                                                                                              nb_xd,svc_xd],'S.D.':[lr_sd,knn_sd,dt_sd,rf_sd,\n",
    "                                                                                                              nb_sd,svc_sd]})\n",
    "\n",
    "lr_xd=statistics.mean(lr_delta_temp)\n",
    "knn_xd=statistics.mean(knn_delta_temp)\n",
    "dt_xd=statistics.mean(dt_delta_temp)\n",
    "rf_xd=statistics.mean(rf_delta_temp)\n",
    "nb_xd=statistics.mean(nb_delta_temp)\n",
    "svc_xd=statistics.mean(svc_delta_temp)\n",
    "\n",
    "lr_sd=statistics.stdev(lr_delta_temp)\n",
    "knn_sd=statistics.stdev(knn_delta_temp)\n",
    "dt_sd=statistics.stdev(dt_delta_temp)\n",
    "rf_sd=statistics.stdev(rf_delta_temp)\n",
    "nb_sd=statistics.stdev(nb_delta_temp)\n",
    "svc_sd=statistics.stdev(svc_delta_temp)\n",
    "\n",
    "ex_delta_res=pd.DataFrame({'model':['Logistic regression','KNN','Decision tree','Random forest','NB','SVC'],'100':[lr_delta_temp[0],knn_delta_temp[0],dt_delta_temp[0],\n",
    "                                                                                                            rf_delta_temp[0],nb_delta_temp[0],svc_delta_temp[0]],'123':[lr_delta_temp[1],knn_delta_temp[1],dt_delta_temp[1]\n",
    "                                                                                                            ,rf_delta_temp[1],nb_delta_temp[1],svc_delta_temp[1]],'200':[lr_delta_temp[2],knn_delta_temp[2],dt_delta_temp[2]\n",
    "                                                                                                            ,rf_delta_temp[2],nb_delta_temp[2],svc_delta_temp[2]],'231':[lr_delta_temp[3],knn_delta_temp[3],dt_delta_temp[3]\n",
    "                                                                                                             ,rf_delta_temp[3],nb_delta_temp[3],svc_delta_temp[3]],'300':[lr_delta_temp[4],knn_delta_temp[4],dt_delta_temp[4]\n",
    "                                                                                                             ,rf_delta_temp[4],nb_delta_temp[4],svc_delta_temp[4]],'mean':[lr_xd,knn_xd,dt_xd,rf_xd,\n",
    "                                                                                                              nb_xd,svc_xd],'S.D.':[lr_sd,knn_sd,dt_sd,rf_sd,\n",
    "                                                                                                              nb_sd,svc_sd]})\n",
    "\n",
    "print('Compare recall score')\n",
    "ex_recall_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "9xRxzvHFiMDr",
    "outputId": "5ae1c3f9-242f-4df0-feed-780d3bd11eb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare F1 score\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-5f743fc8-0c30-467d-a699-761d279ecada\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>100</th>\n",
       "      <th>123</th>\n",
       "      <th>200</th>\n",
       "      <th>231</th>\n",
       "      <th>300</th>\n",
       "      <th>mean</th>\n",
       "      <th>S.D.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic regression</td>\n",
       "      <td>0.842809</td>\n",
       "      <td>0.893891</td>\n",
       "      <td>0.880503</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.880503</td>\n",
       "      <td>0.879028</td>\n",
       "      <td>0.021656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.888179</td>\n",
       "      <td>0.893204</td>\n",
       "      <td>0.891720</td>\n",
       "      <td>0.880171</td>\n",
       "      <td>0.020968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision tree</td>\n",
       "      <td>0.769737</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.830671</td>\n",
       "      <td>0.802721</td>\n",
       "      <td>0.814570</td>\n",
       "      <td>0.800402</td>\n",
       "      <td>0.024091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random forest</td>\n",
       "      <td>0.836120</td>\n",
       "      <td>0.877743</td>\n",
       "      <td>0.904025</td>\n",
       "      <td>0.882540</td>\n",
       "      <td>0.884735</td>\n",
       "      <td>0.877033</td>\n",
       "      <td>0.024963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NB</td>\n",
       "      <td>0.848684</td>\n",
       "      <td>0.880259</td>\n",
       "      <td>0.886792</td>\n",
       "      <td>0.913738</td>\n",
       "      <td>0.874214</td>\n",
       "      <td>0.880737</td>\n",
       "      <td>0.023430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.839590</td>\n",
       "      <td>0.898089</td>\n",
       "      <td>0.894410</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.880976</td>\n",
       "      <td>0.026511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5f743fc8-0c30-467d-a699-761d279ecada')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-5f743fc8-0c30-467d-a699-761d279ecada button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-5f743fc8-0c30-467d-a699-761d279ecada');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                 model       100       123       200       231       300  \\\n",
       "0  Logistic regression  0.842809  0.893891  0.880503  0.897436  0.880503   \n",
       "1                  KNN  0.843137  0.884615  0.888179  0.893204  0.891720   \n",
       "2        Decision tree  0.769737  0.784314  0.830671  0.802721  0.814570   \n",
       "3        Random forest  0.836120  0.877743  0.904025  0.882540  0.884735   \n",
       "4                   NB  0.848684  0.880259  0.886792  0.913738  0.874214   \n",
       "5                  SVC  0.839590  0.898089  0.894410  0.903226  0.869565   \n",
       "\n",
       "       mean      S.D.  \n",
       "0  0.879028  0.021656  \n",
       "1  0.880171  0.020968  \n",
       "2  0.800402  0.024091  \n",
       "3  0.877033  0.024963  \n",
       "4  0.880737  0.023430  \n",
       "5  0.880976  0.026511  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Compare F1 score')\n",
    "ex_F1_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "FDIVahsViQpM",
    "outputId": "567df903-1ed2-48a4-9b16-3a299313fedf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare Train-accuracy score\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-cb6100f9-a581-4cdf-8582-449030ea1860\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>100</th>\n",
       "      <th>123</th>\n",
       "      <th>200</th>\n",
       "      <th>231</th>\n",
       "      <th>300</th>\n",
       "      <th>mean</th>\n",
       "      <th>S.D.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic regression</td>\n",
       "      <td>0.878505</td>\n",
       "      <td>0.866044</td>\n",
       "      <td>0.869159</td>\n",
       "      <td>0.861371</td>\n",
       "      <td>0.870717</td>\n",
       "      <td>0.869159</td>\n",
       "      <td>0.006327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.873188</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.876812</td>\n",
       "      <td>0.865217</td>\n",
       "      <td>0.022246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision tree</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random forest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NB</td>\n",
       "      <td>0.884735</td>\n",
       "      <td>0.866044</td>\n",
       "      <td>0.862928</td>\n",
       "      <td>0.853583</td>\n",
       "      <td>0.872274</td>\n",
       "      <td>0.867913</td>\n",
       "      <td>0.011573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.908100</td>\n",
       "      <td>0.887850</td>\n",
       "      <td>0.883178</td>\n",
       "      <td>0.880062</td>\n",
       "      <td>0.898754</td>\n",
       "      <td>0.891589</td>\n",
       "      <td>0.011635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb6100f9-a581-4cdf-8582-449030ea1860')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-cb6100f9-a581-4cdf-8582-449030ea1860 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-cb6100f9-a581-4cdf-8582-449030ea1860');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                 model       100       123       200       231       300  \\\n",
       "0  Logistic regression  0.878505  0.866044  0.869159  0.861371  0.870717   \n",
       "1                  KNN  0.826087  0.869565  0.873188  0.880435  0.876812   \n",
       "2        Decision tree  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "3        Random forest  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "4                   NB  0.884735  0.866044  0.862928  0.853583  0.872274   \n",
       "5                  SVC  0.908100  0.887850  0.883178  0.880062  0.898754   \n",
       "\n",
       "       mean      S.D.  \n",
       "0  0.869159  0.006327  \n",
       "1  0.865217  0.022246  \n",
       "2  1.000000  0.000000  \n",
       "3  1.000000  0.000000  \n",
       "4  0.867913  0.011573  \n",
       "5  0.891589  0.011635  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Compare Train-accuracy score')\n",
    "ex_acctr_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "J_chnkp5iWPv",
    "outputId": "c1f6418d-ccea-4ad8-9f63-0f92fb234686"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare Test-accuracy score\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-cad643ed-8020-413a-a345-25f5d9272d06\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>100</th>\n",
       "      <th>123</th>\n",
       "      <th>200</th>\n",
       "      <th>231</th>\n",
       "      <th>300</th>\n",
       "      <th>mean</th>\n",
       "      <th>S.D.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic regression</td>\n",
       "      <td>0.829710</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.862319</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.862319</td>\n",
       "      <td>0.863768</td>\n",
       "      <td>0.021527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.894081</td>\n",
       "      <td>0.876947</td>\n",
       "      <td>0.875389</td>\n",
       "      <td>0.875389</td>\n",
       "      <td>0.880062</td>\n",
       "      <td>0.880374</td>\n",
       "      <td>0.007896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision tree</td>\n",
       "      <td>0.746377</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.807971</td>\n",
       "      <td>0.789855</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>0.780435</td>\n",
       "      <td>0.025824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random forest</td>\n",
       "      <td>0.822464</td>\n",
       "      <td>0.858696</td>\n",
       "      <td>0.887681</td>\n",
       "      <td>0.865942</td>\n",
       "      <td>0.865942</td>\n",
       "      <td>0.860145</td>\n",
       "      <td>0.023704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NB</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.865942</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.902174</td>\n",
       "      <td>0.855072</td>\n",
       "      <td>0.865217</td>\n",
       "      <td>0.025024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.829710</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.876812</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.865942</td>\n",
       "      <td>0.026127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cad643ed-8020-413a-a345-25f5d9272d06')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-cad643ed-8020-413a-a345-25f5d9272d06 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-cad643ed-8020-413a-a345-25f5d9272d06');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                 model       100       123       200       231       300  \\\n",
       "0  Logistic regression  0.829710  0.880435  0.862319  0.884058  0.862319   \n",
       "1                  KNN  0.894081  0.876947  0.875389  0.875389  0.880062   \n",
       "2        Decision tree  0.746377  0.760870  0.807971  0.789855  0.797101   \n",
       "3        Random forest  0.822464  0.858696  0.887681  0.865942  0.865942   \n",
       "4                   NB  0.833333  0.865942  0.869565  0.902174  0.855072   \n",
       "5                  SVC  0.829710  0.884058  0.876812  0.891304  0.847826   \n",
       "\n",
       "       mean      S.D.  \n",
       "0  0.863768  0.021527  \n",
       "1  0.880374  0.007896  \n",
       "2  0.780435  0.025824  \n",
       "3  0.860145  0.023704  \n",
       "4  0.865217  0.025024  \n",
       "5  0.865942  0.026127  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Compare Test-accuracy score')\n",
    "ex_acc_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "0lRnkh3Lia0a",
    "outputId": "5f63f381-9272-4da2-a0c8-c4af47e45f23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare different of train/test accuracy score score\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-3632e3af-7626-4c55-8896-b6096f33454e\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>100</th>\n",
       "      <th>123</th>\n",
       "      <th>200</th>\n",
       "      <th>231</th>\n",
       "      <th>300</th>\n",
       "      <th>mean</th>\n",
       "      <th>S.D.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic regression</td>\n",
       "      <td>0.048795</td>\n",
       "      <td>-0.014391</td>\n",
       "      <td>0.006840</td>\n",
       "      <td>-0.022687</td>\n",
       "      <td>0.008398</td>\n",
       "      <td>0.005391</td>\n",
       "      <td>0.027725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>-0.067994</td>\n",
       "      <td>-0.007382</td>\n",
       "      <td>-0.002201</td>\n",
       "      <td>0.005045</td>\n",
       "      <td>-0.003251</td>\n",
       "      <td>-0.015156</td>\n",
       "      <td>0.029875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision tree</td>\n",
       "      <td>0.253623</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.192029</td>\n",
       "      <td>0.210145</td>\n",
       "      <td>0.202899</td>\n",
       "      <td>0.219565</td>\n",
       "      <td>0.025824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random forest</td>\n",
       "      <td>0.177536</td>\n",
       "      <td>0.141304</td>\n",
       "      <td>0.112319</td>\n",
       "      <td>0.134058</td>\n",
       "      <td>0.134058</td>\n",
       "      <td>0.139855</td>\n",
       "      <td>0.023704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NB</td>\n",
       "      <td>0.051402</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>-0.006637</td>\n",
       "      <td>-0.048591</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>0.002695</td>\n",
       "      <td>0.036424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.003792</td>\n",
       "      <td>0.006366</td>\n",
       "      <td>-0.011242</td>\n",
       "      <td>0.050928</td>\n",
       "      <td>0.025647</td>\n",
       "      <td>0.037520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3632e3af-7626-4c55-8896-b6096f33454e')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-3632e3af-7626-4c55-8896-b6096f33454e button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-3632e3af-7626-4c55-8896-b6096f33454e');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                 model       100       123       200       231       300  \\\n",
       "0  Logistic regression  0.048795 -0.014391  0.006840 -0.022687  0.008398   \n",
       "1                  KNN -0.067994 -0.007382 -0.002201  0.005045 -0.003251   \n",
       "2        Decision tree  0.253623  0.239130  0.192029  0.210145  0.202899   \n",
       "3        Random forest  0.177536  0.141304  0.112319  0.134058  0.134058   \n",
       "4                   NB  0.051402  0.000102 -0.006637 -0.048591  0.017202   \n",
       "5                  SVC  0.078390  0.003792  0.006366 -0.011242  0.050928   \n",
       "\n",
       "       mean      S.D.  \n",
       "0  0.005391  0.027725  \n",
       "1 -0.015156  0.029875  \n",
       "2  0.219565  0.025824  \n",
       "3  0.139855  0.023704  \n",
       "4  0.002695  0.036424  \n",
       "5  0.025647  0.037520  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Compare different of train/test accuracy score score')\n",
    "ex_delta_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDQ-QJOGYdKp"
   },
   "source": [
    "#Compare Others model & MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYFqtRhOjGst"
   },
   "source": [
    "##Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbs6SVzAsFRh"
   },
   "source": [
    "###data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "HL4PYxkzYa7o",
    "outputId": "91aaca87-ed9b-4844-85b6-3a76725bd602"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-c1b0ee90-0e80-4b27-b8d1-2d1fc9b24dc1\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>100</th>\n",
       "      <th>123</th>\n",
       "      <th>200</th>\n",
       "      <th>231</th>\n",
       "      <th>300</th>\n",
       "      <th>mean</th>\n",
       "      <th>S.D.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic regression</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.908497</td>\n",
       "      <td>0.915033</td>\n",
       "      <td>0.915033</td>\n",
       "      <td>0.915033</td>\n",
       "      <td>0.895425</td>\n",
       "      <td>0.040290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.908497</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.915033</td>\n",
       "      <td>0.894118</td>\n",
       "      <td>0.029010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision tree</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.849673</td>\n",
       "      <td>0.771242</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.794771</td>\n",
       "      <td>0.034150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random forest</td>\n",
       "      <td>0.816993</td>\n",
       "      <td>0.915033</td>\n",
       "      <td>0.954248</td>\n",
       "      <td>0.908497</td>\n",
       "      <td>0.928105</td>\n",
       "      <td>0.904575</td>\n",
       "      <td>0.052001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NB</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.934641</td>\n",
       "      <td>0.908497</td>\n",
       "      <td>0.899346</td>\n",
       "      <td>0.035679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.915033</td>\n",
       "      <td>0.915033</td>\n",
       "      <td>0.899346</td>\n",
       "      <td>0.054410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c1b0ee90-0e80-4b27-b8d1-2d1fc9b24dc1')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-c1b0ee90-0e80-4b27-b8d1-2d1fc9b24dc1 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-c1b0ee90-0e80-4b27-b8d1-2d1fc9b24dc1');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                 model       100       123       200       231       300  \\\n",
       "0  Logistic regression  0.823529  0.908497  0.915033  0.915033  0.915033   \n",
       "1                  KNN  0.843137  0.901961  0.908497  0.901961  0.915033   \n",
       "2        Decision tree  0.764706  0.784314  0.849673  0.771242  0.803922   \n",
       "3        Random forest  0.816993  0.915033  0.954248  0.908497  0.928105   \n",
       "4                   NB  0.843137  0.888889  0.921569  0.934641  0.908497   \n",
       "5                  SVC  0.803922  0.921569  0.941176  0.915033  0.915033   \n",
       "\n",
       "       mean      S.D.  \n",
       "0  0.895425  0.040290  \n",
       "1  0.894118  0.029010  \n",
       "2  0.794771  0.034150  \n",
       "3  0.904575  0.052001  \n",
       "4  0.899346  0.035679  \n",
       "5  0.899346  0.054410  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_recall_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "vwYha1OrZJr8",
    "outputId": "9e9ce853-2928-4686-a7d9-32726005f5fd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-021dd994-3368-4f23-947b-0a9d4cabd59d\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLP method</th>\n",
       "      <th>Mean</th>\n",
       "      <th>S.D.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Only MLP</td>\n",
       "      <td>0.894118</td>\n",
       "      <td>0.052369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP with drop out</td>\n",
       "      <td>0.908497</td>\n",
       "      <td>0.042609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP with l2</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.075518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP with l2 &amp; drop out</td>\n",
       "      <td>0.920261</td>\n",
       "      <td>0.031140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-021dd994-3368-4f23-947b-0a9d4cabd59d')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-021dd994-3368-4f23-947b-0a9d4cabd59d button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-021dd994-3368-4f23-947b-0a9d4cabd59d');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "               MLP method      Mean      S.D.\n",
       "0                Only MLP  0.894118  0.052369\n",
       "1       MLP with drop out  0.908497  0.042609\n",
       "2             MLP with l2  0.882353  0.075518\n",
       "3  MLP with l2 & drop out  0.920261  0.031140"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_recall_xb_sd_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODe6WOOKlq3F"
   },
   "source": [
    "###Store mean&S.D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "id": "MxjQ6p74lZlR"
   },
   "outputs": [],
   "source": [
    "lr_xb=statistics.mean(lr_recall_temp)\n",
    "knn_xb=statistics.mean(knn_recall_temp)\n",
    "dt_xb=statistics.mean(dt_recall_temp)\n",
    "rf_xb=statistics.mean(rf_recall_temp)\n",
    "nb_xb=statistics.mean(nb_recall_temp)\n",
    "svc_xb=statistics.mean(svc_recall_temp)\n",
    "\n",
    "lr_sd=statistics.stdev(lr_recall_temp)\n",
    "knn_sd=statistics.stdev(knn_recall_temp)\n",
    "dt_sd=statistics.stdev(dt_recall_temp)\n",
    "rf_sd=statistics.stdev(rf_recall_temp)\n",
    "nb_sd=statistics.stdev(nb_recall_temp)\n",
    "svc_sd=statistics.stdev(svc_recall_temp)\n",
    "\n",
    "MLP0_xb=statistics.mean(MLP0_recall_temp)\n",
    "MLPd_xb=statistics.mean(MLPd_recall_temp)\n",
    "MLPl2_xb=statistics.mean(MLPl2_recall_temp)\n",
    "MLP_xb=statistics.mean(MLP_recall_temp)\n",
    "\n",
    "MLP0_sd=statistics.stdev(MLP0_recall_temp)\n",
    "MLPd_sd=statistics.stdev(MLPd_recall_temp)\n",
    "MLPl2_sd=statistics.stdev(MLPl2_recall_temp)\n",
    "MLP_sd=statistics.stdev(MLP_recall_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zzGXdlPmCbp"
   },
   "source": [
    "###Compare Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "cNqjeviCl3X2",
    "outputId": "fc037d39-14d0-4b60-ebc8-46542001f2da"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ['lr','knn','dt','rf','nb','svc','MLP','MLP&drop','MLP&l2','MLP l2&drop']\n",
    "mean = [lr_xb,knn_xb,dt_xb,rf_xb,nb_xb,svc_xb,MLP0_xb,MLPd_xb,MLPl2_xb,MLP_xb]\n",
    "\n",
    "plt.figure(figsize=(10,5.5))\n",
    "plt.bar(model, mean)\n",
    "plt.title('Mean of Recall in different model')\n",
    "plt.xlabel('model')\n",
    "plt.ylim(0.78, 1)\n",
    "plt.ylabel('mean')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Au8e48kVrLji"
   },
   "source": [
    "###Compare S.D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "_ZSalKaurK9E",
    "outputId": "2eed3218-0e18-47b9-e576-2e7d59a65455"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ['lr','knn','dt','rf','nb','svc','MLP','MLP&drop','MLP&l2','MLP l2&drop']\n",
    "mean = [lr_sd,knn_sd,dt_sd,rf_sd,nb_sd,svc_sd,MLP0_sd,MLPd_sd,MLPl2_sd,MLP_sd]\n",
    "\n",
    "plt.figure(figsize=(10,5.5))\n",
    "plt.bar(model, mean)\n",
    "plt.title('Mean of Recall in different model')\n",
    "plt.xlabel('model')\n",
    "plt.ylim(0.02, 0.08)\n",
    "plt.ylabel('mean')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Rty1tozjKQg"
   },
   "source": [
    "##F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uqs5yLvJsK_5"
   },
   "source": [
    "###data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "1onENsEcjLxb",
    "outputId": "71281ff5-8cae-4fb5-9b90-90652f0b6f93"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-4d4f1a32-b469-44a5-b48b-bf7fedc922d3\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>100</th>\n",
       "      <th>123</th>\n",
       "      <th>200</th>\n",
       "      <th>231</th>\n",
       "      <th>300</th>\n",
       "      <th>mean</th>\n",
       "      <th>S.D.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic regression</td>\n",
       "      <td>0.842809</td>\n",
       "      <td>0.893891</td>\n",
       "      <td>0.880503</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.880503</td>\n",
       "      <td>0.879028</td>\n",
       "      <td>0.021656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.888179</td>\n",
       "      <td>0.893204</td>\n",
       "      <td>0.891720</td>\n",
       "      <td>0.880171</td>\n",
       "      <td>0.020968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision tree</td>\n",
       "      <td>0.769737</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.830671</td>\n",
       "      <td>0.802721</td>\n",
       "      <td>0.814570</td>\n",
       "      <td>0.800402</td>\n",
       "      <td>0.024091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random forest</td>\n",
       "      <td>0.836120</td>\n",
       "      <td>0.877743</td>\n",
       "      <td>0.904025</td>\n",
       "      <td>0.882540</td>\n",
       "      <td>0.884735</td>\n",
       "      <td>0.877033</td>\n",
       "      <td>0.024963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NB</td>\n",
       "      <td>0.848684</td>\n",
       "      <td>0.880259</td>\n",
       "      <td>0.886792</td>\n",
       "      <td>0.913738</td>\n",
       "      <td>0.874214</td>\n",
       "      <td>0.880737</td>\n",
       "      <td>0.023430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.839590</td>\n",
       "      <td>0.898089</td>\n",
       "      <td>0.894410</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.880976</td>\n",
       "      <td>0.026511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d4f1a32-b469-44a5-b48b-bf7fedc922d3')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-4d4f1a32-b469-44a5-b48b-bf7fedc922d3 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-4d4f1a32-b469-44a5-b48b-bf7fedc922d3');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                 model       100       123       200       231       300  \\\n",
       "0  Logistic regression  0.842809  0.893891  0.880503  0.897436  0.880503   \n",
       "1                  KNN  0.843137  0.884615  0.888179  0.893204  0.891720   \n",
       "2        Decision tree  0.769737  0.784314  0.830671  0.802721  0.814570   \n",
       "3        Random forest  0.836120  0.877743  0.904025  0.882540  0.884735   \n",
       "4                   NB  0.848684  0.880259  0.886792  0.913738  0.874214   \n",
       "5                  SVC  0.839590  0.898089  0.894410  0.903226  0.869565   \n",
       "\n",
       "       mean      S.D.  \n",
       "0  0.879028  0.021656  \n",
       "1  0.880171  0.020968  \n",
       "2  0.800402  0.024091  \n",
       "3  0.877033  0.024963  \n",
       "4  0.880737  0.023430  \n",
       "5  0.880976  0.026511  "
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_F1_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "294mbYMvjOtv",
    "outputId": "c74ff557-2a99-4ba6-dff7-9500556b6bcf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-af266cce-0b1c-49e6-a1fe-555425549f05\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLP method</th>\n",
       "      <th>Mean</th>\n",
       "      <th>S.D.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Only MLP</td>\n",
       "      <td>0.879177</td>\n",
       "      <td>0.028635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP with drop out</td>\n",
       "      <td>0.886167</td>\n",
       "      <td>0.023284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP with l2</td>\n",
       "      <td>0.873178</td>\n",
       "      <td>0.033210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP with l2 &amp; drop out</td>\n",
       "      <td>0.884875</td>\n",
       "      <td>0.011695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-af266cce-0b1c-49e6-a1fe-555425549f05')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-af266cce-0b1c-49e6-a1fe-555425549f05 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-af266cce-0b1c-49e6-a1fe-555425549f05');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "               MLP method      Mean      S.D.\n",
       "0                Only MLP  0.879177  0.028635\n",
       "1       MLP with drop out  0.886167  0.023284\n",
       "2             MLP with l2  0.873178  0.033210\n",
       "3  MLP with l2 & drop out  0.884875  0.011695"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_F1_xb_sd_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDlNFio7sQue"
   },
   "source": [
    "###Get mean&S.D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "id": "ggY8CVO2sQBs"
   },
   "outputs": [],
   "source": [
    "lr_xb=statistics.mean(lr_F1_temp)\n",
    "knn_xb=statistics.mean(knn_F1_temp)\n",
    "dt_xb=statistics.mean(dt_F1_temp)\n",
    "rf_xb=statistics.mean(rf_F1_temp)\n",
    "nb_xb=statistics.mean(nb_F1_temp)\n",
    "svc_xb=statistics.mean(svc_F1_temp)\n",
    "\n",
    "lr_sd=statistics.stdev(lr_F1_temp)\n",
    "knn_sd=statistics.stdev(knn_F1_temp)\n",
    "dt_sd=statistics.stdev(dt_F1_temp)\n",
    "rf_sd=statistics.stdev(rf_F1_temp)\n",
    "nb_sd=statistics.stdev(nb_F1_temp)\n",
    "svc_sd=statistics.stdev(svc_F1_temp)\n",
    "\n",
    "MLP0_xb=statistics.mean(MLP0_F1_temp)\n",
    "MLPd_xb=statistics.mean(MLPd_F1_temp)\n",
    "MLPl2_xb=statistics.mean(MLPl2_F1_temp)\n",
    "MLP_xb=statistics.mean(MLP_F1_temp)\n",
    "\n",
    "MLP0_sd=statistics.stdev(MLP0_F1_temp)\n",
    "MLPd_sd=statistics.stdev(MLPd_F1_temp)\n",
    "MLPl2_sd=statistics.stdev(MLPl2_F1_temp)\n",
    "MLP_sd=statistics.stdev(MLP_F1_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpGHSRgNsvV6"
   },
   "source": [
    "###Compare mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "puDnT7NCsy_m",
    "outputId": "b3e808cb-d24f-441a-b432-1ca228508506"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ['lr','knn','dt','rf','nb','svc','MLP','MLP&drop','MLP&l2','MLP l2&drop']\n",
    "mean = [lr_xb,knn_xb,dt_xb,rf_xb,nb_xb,svc_xb,MLP0_xb,MLPd_xb,MLPl2_xb,MLP_xb]\n",
    "\n",
    "plt.figure(figsize=(10,5.5))\n",
    "plt.bar(model, mean)\n",
    "plt.title('Mean of F1 in different model')\n",
    "plt.xlabel('model')\n",
    "plt.ylim(0.78, 1)\n",
    "plt.ylabel('mean')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqNDOWJQtRao"
   },
   "source": [
    "###Compare S.D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "rw1uBl93tVZH",
    "outputId": "c714c328-1021-4c84-ae7d-b7a25cb39e3c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ['lr','knn','dt','rf','nb','svc','MLP','MLP&drop','MLP&l2','MLP l2&drop']\n",
    "mean = [lr_sd,knn_sd,dt_sd,rf_sd,nb_sd,svc_sd,MLP0_sd,MLPd_sd,MLPl2_sd,MLP_sd]\n",
    "\n",
    "plt.figure(figsize=(10,5.5))\n",
    "plt.bar(model, mean)\n",
    "plt.title('S.D. of Recall in different model')\n",
    "plt.xlabel('model')\n",
    "plt.ylim(0.01, 0.035)\n",
    "plt.ylabel('mean')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvnLPQm0jWjX"
   },
   "source": [
    "##Train accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXReeBsSulYk"
   },
   "source": [
    "###data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "wld19OasjZBP",
    "outputId": "ddb1ebfc-7ee2-4469-9a73-ebb75fc5b1e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-96c50d9f-4af8-4568-afc7-c77d0d518865\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>100</th>\n",
       "      <th>123</th>\n",
       "      <th>200</th>\n",
       "      <th>231</th>\n",
       "      <th>300</th>\n",
       "      <th>mean</th>\n",
       "      <th>S.D.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic regression</td>\n",
       "      <td>0.878505</td>\n",
       "      <td>0.866044</td>\n",
       "      <td>0.869159</td>\n",
       "      <td>0.861371</td>\n",
       "      <td>0.870717</td>\n",
       "      <td>0.869159</td>\n",
       "      <td>0.006327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.873188</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.876812</td>\n",
       "      <td>0.865217</td>\n",
       "      <td>0.022246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision tree</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random forest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NB</td>\n",
       "      <td>0.884735</td>\n",
       "      <td>0.866044</td>\n",
       "      <td>0.862928</td>\n",
       "      <td>0.853583</td>\n",
       "      <td>0.872274</td>\n",
       "      <td>0.867913</td>\n",
       "      <td>0.011573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.908100</td>\n",
       "      <td>0.887850</td>\n",
       "      <td>0.883178</td>\n",
       "      <td>0.880062</td>\n",
       "      <td>0.898754</td>\n",
       "      <td>0.891589</td>\n",
       "      <td>0.011635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-96c50d9f-4af8-4568-afc7-c77d0d518865')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-96c50d9f-4af8-4568-afc7-c77d0d518865 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-96c50d9f-4af8-4568-afc7-c77d0d518865');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                 model       100       123       200       231       300  \\\n",
       "0  Logistic regression  0.878505  0.866044  0.869159  0.861371  0.870717   \n",
       "1                  KNN  0.826087  0.869565  0.873188  0.880435  0.876812   \n",
       "2        Decision tree  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "3        Random forest  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "4                   NB  0.884735  0.866044  0.862928  0.853583  0.872274   \n",
       "5                  SVC  0.908100  0.887850  0.883178  0.880062  0.898754   \n",
       "\n",
       "       mean      S.D.  \n",
       "0  0.869159  0.006327  \n",
       "1  0.865217  0.022246  \n",
       "2  1.000000  0.000000  \n",
       "3  1.000000  0.000000  \n",
       "4  0.867913  0.011573  \n",
       "5  0.891589  0.011635  "
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_acctr_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "btPSGoBSjb9Y",
    "outputId": "e094a304-9ccb-4ee5-e4cc-d1ac19057e50"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-8acdee9b-377a-43cb-8a7d-da230799abf4\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLP method</th>\n",
       "      <th>Mean</th>\n",
       "      <th>S.D.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Only MLP</td>\n",
       "      <td>0.881931</td>\n",
       "      <td>0.009241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP with drop out</td>\n",
       "      <td>0.878816</td>\n",
       "      <td>0.006553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP with l2</td>\n",
       "      <td>0.871028</td>\n",
       "      <td>0.005441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP with l2 &amp; drop out</td>\n",
       "      <td>0.872274</td>\n",
       "      <td>0.006516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8acdee9b-377a-43cb-8a7d-da230799abf4')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-8acdee9b-377a-43cb-8a7d-da230799abf4 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-8acdee9b-377a-43cb-8a7d-da230799abf4');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "               MLP method      Mean      S.D.\n",
       "0                Only MLP  0.881931  0.009241\n",
       "1       MLP with drop out  0.878816  0.006553\n",
       "2             MLP with l2  0.871028  0.005441\n",
       "3  MLP with l2 & drop out  0.872274  0.006516"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_acctr_xb_sd_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UocgISTPuoR3"
   },
   "source": [
    "### Get mean&S.D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "id": "JcHzBSMwuut9"
   },
   "outputs": [],
   "source": [
    "lr_xb=statistics.mean(lr_acctr_temp)\n",
    "knn_xb=statistics.mean(knn_acctr_temp)\n",
    "dt_xb=statistics.mean(dt_acctr_temp)\n",
    "rf_xb=statistics.mean(rf_acctr_temp)\n",
    "nb_xb=statistics.mean(nb_acctr_temp)\n",
    "svc_xb=statistics.mean(svc_acctr_temp)\n",
    "\n",
    "lr_sd=statistics.stdev(lr_acctr_temp)\n",
    "knn_sd=statistics.stdev(knn_acctr_temp)\n",
    "dt_sd=statistics.stdev(dt_acctr_temp)\n",
    "rf_sd=statistics.stdev(rf_acctr_temp)\n",
    "nb_sd=statistics.stdev(nb_acctr_temp)\n",
    "svc_sd=statistics.stdev(svc_acctr_temp)\n",
    "\n",
    "MLP0_xb=statistics.mean(MLP0_acctr_temp)\n",
    "MLPd_xb=statistics.mean(MLPd_acctr_temp)\n",
    "MLPl2_xb=statistics.mean(MLPl2_acctr_temp)\n",
    "MLP_xb=statistics.mean(MLP_acctr_temp)\n",
    "\n",
    "MLP0_sd=statistics.stdev(MLP0_acctr_temp)\n",
    "MLPd_sd=statistics.stdev(MLPd_acctr_temp)\n",
    "MLPl2_sd=statistics.stdev(MLPl2_acctr_temp)\n",
    "MLP_sd=statistics.stdev(MLP_acctr_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XTQr34ivJ1v"
   },
   "source": [
    "###Compare mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "_TrLGqUmvMuO",
    "outputId": "50aaa791-9f53-4cc8-a69a-6d48ba4bc774"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ['lr','knn','dt','rf','nb','svc','MLP','MLP&drop','MLP&l2','MLP l2&drop']\n",
    "mean = [lr_xb,knn_xb,dt_xb,rf_xb,nb_xb,svc_xb,MLP0_xb,MLPd_xb,MLPl2_xb,MLP_xb]\n",
    "\n",
    "plt.figure(figsize=(10,5.5))\n",
    "plt.bar(model, mean)\n",
    "plt.title('Mean of Train-accuracy in different model')\n",
    "plt.xlabel('model')\n",
    "plt.ylim(0.78, 1)\n",
    "plt.ylabel('mean')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDfOmVr1vQ5y"
   },
   "source": [
    "###Compare S.D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "nh1GxdpLvQO8",
    "outputId": "336d7f15-3e03-4fae-fdd6-2ebb9235493a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ['lr','knn','dt','rf','nb','svc','MLP','MLP&drop','MLP&l2','MLP l2&drop']\n",
    "mean = [lr_sd,knn_sd,dt_sd,rf_sd,nb_sd,svc_sd,MLP0_sd,MLPd_sd,MLPl2_sd,MLP_sd]\n",
    "\n",
    "plt.figure(figsize=(10,5.5))\n",
    "plt.bar(model, mean)\n",
    "plt.title('S.D. of Train-accuracy in different model')\n",
    "plt.xlabel('model')\n",
    "#plt.ylim(0.01, 0.035)\n",
    "plt.ylabel('mean')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMOfcrH_jggt"
   },
   "source": [
    "##Test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxWf2Nk4vsT3"
   },
   "source": [
    "###data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "EjDmCyI4jjDI",
    "outputId": "a90b7cfa-4706-4d4c-e279-bc9169c9480b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-4ae367fe-12c6-4c1f-84a3-3af307e54719\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>100</th>\n",
       "      <th>123</th>\n",
       "      <th>200</th>\n",
       "      <th>231</th>\n",
       "      <th>300</th>\n",
       "      <th>mean</th>\n",
       "      <th>S.D.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic regression</td>\n",
       "      <td>0.829710</td>\n",
       "      <td>0.880435</td>\n",
       "      <td>0.862319</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.862319</td>\n",
       "      <td>0.863768</td>\n",
       "      <td>0.021527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.894081</td>\n",
       "      <td>0.876947</td>\n",
       "      <td>0.875389</td>\n",
       "      <td>0.875389</td>\n",
       "      <td>0.880062</td>\n",
       "      <td>0.880374</td>\n",
       "      <td>0.007896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision tree</td>\n",
       "      <td>0.746377</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.807971</td>\n",
       "      <td>0.789855</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>0.780435</td>\n",
       "      <td>0.025824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random forest</td>\n",
       "      <td>0.822464</td>\n",
       "      <td>0.858696</td>\n",
       "      <td>0.887681</td>\n",
       "      <td>0.865942</td>\n",
       "      <td>0.865942</td>\n",
       "      <td>0.860145</td>\n",
       "      <td>0.023704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NB</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.865942</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.902174</td>\n",
       "      <td>0.855072</td>\n",
       "      <td>0.865217</td>\n",
       "      <td>0.025024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.829710</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.876812</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.865942</td>\n",
       "      <td>0.026127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4ae367fe-12c6-4c1f-84a3-3af307e54719')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-4ae367fe-12c6-4c1f-84a3-3af307e54719 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-4ae367fe-12c6-4c1f-84a3-3af307e54719');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                 model       100       123       200       231       300  \\\n",
       "0  Logistic regression  0.829710  0.880435  0.862319  0.884058  0.862319   \n",
       "1                  KNN  0.894081  0.876947  0.875389  0.875389  0.880062   \n",
       "2        Decision tree  0.746377  0.760870  0.807971  0.789855  0.797101   \n",
       "3        Random forest  0.822464  0.858696  0.887681  0.865942  0.865942   \n",
       "4                   NB  0.833333  0.865942  0.869565  0.902174  0.855072   \n",
       "5                  SVC  0.829710  0.884058  0.876812  0.891304  0.847826   \n",
       "\n",
       "       mean      S.D.  \n",
       "0  0.863768  0.021527  \n",
       "1  0.880374  0.007896  \n",
       "2  0.780435  0.025824  \n",
       "3  0.860145  0.023704  \n",
       "4  0.865217  0.025024  \n",
       "5  0.865942  0.026127  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_acc_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "YbB9Ip5ej5yq",
    "outputId": "37d01c5c-f157-4de5-da93-555809a203e1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-f5c30ab6-8e7c-45f9-a7b3-9e82b09d3dbc\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLP method</th>\n",
       "      <th>Mean</th>\n",
       "      <th>S.D.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Only MLP</td>\n",
       "      <td>0.864493</td>\n",
       "      <td>0.027901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP with drop out</td>\n",
       "      <td>0.871014</td>\n",
       "      <td>0.023842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP with l2</td>\n",
       "      <td>0.859420</td>\n",
       "      <td>0.029368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP with l2 &amp; drop out</td>\n",
       "      <td>0.867391</td>\n",
       "      <td>0.012180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5c30ab6-8e7c-45f9-a7b3-9e82b09d3dbc')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-f5c30ab6-8e7c-45f9-a7b3-9e82b09d3dbc button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-f5c30ab6-8e7c-45f9-a7b3-9e82b09d3dbc');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "               MLP method      Mean      S.D.\n",
       "0                Only MLP  0.864493  0.027901\n",
       "1       MLP with drop out  0.871014  0.023842\n",
       "2             MLP with l2  0.859420  0.029368\n",
       "3  MLP with l2 & drop out  0.867391  0.012180"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_acc_xb_sd_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1y5P-BtvwK2"
   },
   "source": [
    "###Get mean&S.D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "id": "Cm_zWWpGv0Yy"
   },
   "outputs": [],
   "source": [
    "lr_xb=statistics.mean(lr_acc_temp)\n",
    "knn_xb=statistics.mean(knn_acc_temp)\n",
    "dt_xb=statistics.mean(dt_acc_temp)\n",
    "rf_xb=statistics.mean(rf_acc_temp)\n",
    "nb_xb=statistics.mean(nb_acc_temp)\n",
    "svc_xb=statistics.mean(svc_acc_temp)\n",
    "\n",
    "lr_sd=statistics.stdev(lr_acc_temp)\n",
    "knn_sd=statistics.stdev(knn_acc_temp)\n",
    "dt_sd=statistics.stdev(dt_acc_temp)\n",
    "rf_sd=statistics.stdev(rf_acc_temp)\n",
    "nb_sd=statistics.stdev(nb_acc_temp)\n",
    "svc_sd=statistics.stdev(svc_acc_temp)\n",
    "\n",
    "MLP0_xb=statistics.mean(MLP0_acc_temp)\n",
    "MLPd_xb=statistics.mean(MLPd_acc_temp)\n",
    "MLPl2_xb=statistics.mean(MLPl2_acc_temp)\n",
    "MLP_xb=statistics.mean(MLP_acc_temp)\n",
    "\n",
    "MLP0_sd=statistics.stdev(MLP0_acc_temp)\n",
    "MLPd_sd=statistics.stdev(MLPd_acc_temp)\n",
    "MLPl2_sd=statistics.stdev(MLPl2_acc_temp)\n",
    "MLP_sd=statistics.stdev(MLP_acc_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLg3WEczwBFF"
   },
   "source": [
    "###Compare mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "pfYWuCebwDtF",
    "outputId": "1b74c3c6-eb56-426c-efe8-13cd6191e7b7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ['lr','knn','dt','rf','nb','svc','MLP','MLP&drop','MLP&l2','MLP l2&drop']\n",
    "mean = [lr_xb,knn_xb,dt_xb,rf_xb,nb_xb,svc_xb,MLP0_xb,MLPd_xb,MLPl2_xb,MLP_xb]\n",
    "\n",
    "plt.figure(figsize=(10,5.5))\n",
    "plt.bar(model, mean)\n",
    "plt.title('Mean of Test-accuracy in different model')\n",
    "plt.xlabel('model')\n",
    "plt.ylim(0.75, 1)\n",
    "plt.ylabel('mean')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHMbpbh2wGLy"
   },
   "source": [
    "###Compare S.D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "mlLpFffuwOFt",
    "outputId": "51862eb7-b1dd-4bed-daa7-921913ebe990"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ['lr','knn','dt','rf','nb','svc','MLP','MLP&drop','MLP&l2','MLP l2&drop']\n",
    "mean = [lr_sd,knn_sd,dt_sd,rf_sd,nb_sd,svc_sd,MLP0_sd,MLPd_sd,MLPl2_sd,MLP_sd]\n",
    "\n",
    "plt.figure(figsize=(10,5.5))\n",
    "plt.bar(model, mean)\n",
    "plt.title('S.D. of Test-accuracy in different model')\n",
    "plt.xlabel('model')\n",
    "#plt.ylim(0.01, 0.035)\n",
    "plt.ylabel('mean')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9nek5u1wYjC"
   },
   "source": [
    "##Different of Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G68IiUO5weym"
   },
   "source": [
    "###data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "a16CNYeIwg49",
    "outputId": "e8a5b165-ed37-4579-fe42-2c97536b392e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-3798d306-ea8e-4488-bfb4-d1f5b7a3d0d0\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>100</th>\n",
       "      <th>123</th>\n",
       "      <th>200</th>\n",
       "      <th>231</th>\n",
       "      <th>300</th>\n",
       "      <th>mean</th>\n",
       "      <th>S.D.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic regression</td>\n",
       "      <td>0.048795</td>\n",
       "      <td>-0.014391</td>\n",
       "      <td>0.006840</td>\n",
       "      <td>-0.022687</td>\n",
       "      <td>0.008398</td>\n",
       "      <td>0.005391</td>\n",
       "      <td>0.027725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>-0.067994</td>\n",
       "      <td>-0.007382</td>\n",
       "      <td>-0.002201</td>\n",
       "      <td>0.005045</td>\n",
       "      <td>-0.003251</td>\n",
       "      <td>-0.015156</td>\n",
       "      <td>0.029875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision tree</td>\n",
       "      <td>0.253623</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.192029</td>\n",
       "      <td>0.210145</td>\n",
       "      <td>0.202899</td>\n",
       "      <td>0.219565</td>\n",
       "      <td>0.025824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random forest</td>\n",
       "      <td>0.177536</td>\n",
       "      <td>0.141304</td>\n",
       "      <td>0.112319</td>\n",
       "      <td>0.134058</td>\n",
       "      <td>0.134058</td>\n",
       "      <td>0.139855</td>\n",
       "      <td>0.023704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NB</td>\n",
       "      <td>0.051402</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>-0.006637</td>\n",
       "      <td>-0.048591</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>0.002695</td>\n",
       "      <td>0.036424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.003792</td>\n",
       "      <td>0.006366</td>\n",
       "      <td>-0.011242</td>\n",
       "      <td>0.050928</td>\n",
       "      <td>0.025647</td>\n",
       "      <td>0.037520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3798d306-ea8e-4488-bfb4-d1f5b7a3d0d0')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-3798d306-ea8e-4488-bfb4-d1f5b7a3d0d0 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-3798d306-ea8e-4488-bfb4-d1f5b7a3d0d0');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                 model       100       123       200       231       300  \\\n",
       "0  Logistic regression  0.048795 -0.014391  0.006840 -0.022687  0.008398   \n",
       "1                  KNN -0.067994 -0.007382 -0.002201  0.005045 -0.003251   \n",
       "2        Decision tree  0.253623  0.239130  0.192029  0.210145  0.202899   \n",
       "3        Random forest  0.177536  0.141304  0.112319  0.134058  0.134058   \n",
       "4                   NB  0.051402  0.000102 -0.006637 -0.048591  0.017202   \n",
       "5                  SVC  0.078390  0.003792  0.006366 -0.011242  0.050928   \n",
       "\n",
       "       mean      S.D.  \n",
       "0  0.005391  0.027725  \n",
       "1 -0.015156  0.029875  \n",
       "2  0.219565  0.025824  \n",
       "3  0.139855  0.023704  \n",
       "4  0.002695  0.036424  \n",
       "5  0.025647  0.037520  "
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_delta_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "PanYT0v-wjcv",
    "outputId": "e4812dc0-25bf-4d67-bea6-e5d0fe703b68"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-1fe79c64-47ec-4715-8eae-2479171f97d2\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MLP method</th>\n",
       "      <th>Mean</th>\n",
       "      <th>S.D.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Only MLP</td>\n",
       "      <td>0.017439</td>\n",
       "      <td>0.033120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP with drop out</td>\n",
       "      <td>0.007802</td>\n",
       "      <td>0.029579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP with l2</td>\n",
       "      <td>0.011608</td>\n",
       "      <td>0.032239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP with l2 &amp; drop out</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.017984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1fe79c64-47ec-4715-8eae-2479171f97d2')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-1fe79c64-47ec-4715-8eae-2479171f97d2 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-1fe79c64-47ec-4715-8eae-2479171f97d2');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "               MLP method      Mean      S.D.\n",
       "0                Only MLP  0.017439  0.033120\n",
       "1       MLP with drop out  0.007802  0.029579\n",
       "2             MLP with l2  0.011608  0.032239\n",
       "3  MLP with l2 & drop out  0.004883  0.017984"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_delta_xb_sd_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAF5JPv4wu_n"
   },
   "source": [
    "###Get mean&S.D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "id": "p7g0poDkw0Lq"
   },
   "outputs": [],
   "source": [
    "lr_xb=statistics.mean(lr_delta_temp)\n",
    "knn_xb=statistics.mean(knn_delta_temp)\n",
    "dt_xb=statistics.mean(dt_delta_temp)\n",
    "rf_xb=statistics.mean(rf_delta_temp)\n",
    "nb_xb=statistics.mean(nb_delta_temp)\n",
    "svc_xb=statistics.mean(svc_delta_temp)\n",
    "\n",
    "lr_sd=statistics.stdev(lr_delta_temp)\n",
    "knn_sd=statistics.stdev(knn_delta_temp)\n",
    "dt_sd=statistics.stdev(dt_delta_temp)\n",
    "rf_sd=statistics.stdev(rf_delta_temp)\n",
    "nb_sd=statistics.stdev(nb_delta_temp)\n",
    "svc_sd=statistics.stdev(svc_delta_temp)\n",
    "\n",
    "MLP0_xb=statistics.mean(MLP0_delta_temp)\n",
    "MLPd_xb=statistics.mean(MLPd_delta_temp)\n",
    "MLPl2_xb=statistics.mean(MLPl2_delta_temp)\n",
    "MLP_xb=statistics.mean(MLP_delta_temp)\n",
    "\n",
    "MLP0_sd=statistics.stdev(MLP0_delta_temp)\n",
    "MLPd_sd=statistics.stdev(MLPd_delta_temp)\n",
    "MLPl2_sd=statistics.stdev(MLPl2_delta_temp)\n",
    "MLP_sd=statistics.stdev(MLP_delta_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPdyxLb3w02R"
   },
   "source": [
    "###Compare mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "OcYC82RMw5JD",
    "outputId": "8bf933c0-a1c1-4b2a-a203-31127caf3f4d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ['lr','knn','dt','rf','nb','svc','MLP','MLP&drop','MLP&l2','MLP l2&drop']\n",
    "mean = [lr_xb,knn_xb,dt_xb,rf_xb,nb_xb,svc_xb,MLP0_xb,MLPd_xb,MLPl2_xb,MLP_xb]\n",
    "\n",
    "plt.figure(figsize=(10,5.5))\n",
    "plt.bar(model, mean)\n",
    "plt.title('Mean of Different of Accuracy in different model')\n",
    "plt.xlabel('model')\n",
    "#plt.ylim(0.75, 1)\n",
    "plt.ylabel('mean')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2NPmsPFw5gf"
   },
   "source": [
    "###Compare S.D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "Yu8N3ONdw8sf",
    "outputId": "ab5b8334-49ac-449f-90da-1edc23544e62"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ['lr','knn','dt','rf','nb','svc','MLP','MLP&drop','MLP&l2','MLP l2&drop']\n",
    "mean = [lr_sd,knn_sd,dt_sd,rf_sd,nb_sd,svc_sd,MLP0_sd,MLPd_sd,MLPl2_sd,MLP_sd]\n",
    "\n",
    "plt.figure(figsize=(10,5.5))\n",
    "plt.bar(model, mean)\n",
    "plt.title('S.D.of Different of Accuracy in different model')\n",
    "plt.xlabel('model')\n",
    "#plt.ylim(0.01, 0.035)\n",
    "plt.ylabel('mean')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "observe how random seed effect to model.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
